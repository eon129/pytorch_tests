{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "896654c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import time\n",
    "from torchvision.models import resnet50, ResNet50_Weights, ResNet152_Weights, EfficientNet_B7_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe203b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09fd3c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9318ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset location\n",
    "TRAIN_DATA_PATH = \"../Data/CATS_DOGS/train\"\n",
    "TEST_DATA_PATH = \"../Data/CATS_DOGS/test\"\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = {\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'validation':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    'train': \n",
    "    torchvision.datasets.ImageFolder(TRAIN_DATA_PATH, data_transforms['train']),\n",
    "    'validation': \n",
    "    torchvision.datasets.ImageFolder(TEST_DATA_PATH, data_transforms['validation'])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train':\n",
    "    torch.utils.data.DataLoader(image_datasets['train'],\n",
    "                                batch_size=32,\n",
    "                                shuffle=True,\n",
    "                                num_workers=16),  # for Kaggle\n",
    "    'validation':\n",
    "    torch.utils.data.DataLoader(image_datasets['validation'],\n",
    "                                batch_size=32,\n",
    "                                shuffle=False,\n",
    "                                num_workers=16)  # for Kaggle\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07091c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ba64fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader):\n",
    "    #Sets the module in evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    #correct.to(device)\n",
    "    \n",
    "    #dont update dynamic computation graph\n",
    "    with torch.no_grad():\n",
    "        #for every example in test\n",
    "        for data, target in data_loader:\n",
    "            \n",
    "            target = target.view(-1, 1).float()\n",
    "            target.to(device)\n",
    "            \n",
    "            #evaluate the model\n",
    "            output = model(data.to(device))\n",
    "            \n",
    "            #acumulate the loss\n",
    "            test_loss += F.binary_cross_entropy(output, target.to(device)).item()\n",
    "            \n",
    "            pred_cls = output.round()\n",
    "            correct += pred_cls.eq(target.view(-1, 1).to(device)).sum() \n",
    "        \n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(data_loader.dataset), 100. * correct / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3664797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, test_on_every_epoch = False):\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "        \n",
    "        if test_on_every_epoch:\n",
    "            test()\n",
    "        \n",
    "        start = time.time()\n",
    "  \n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        \n",
    "            model.train()\n",
    "        \n",
    "            #forward pass\n",
    "            out = model(data.to(device))\n",
    "        \n",
    "            #Use negative log likelihood loss.\n",
    "            loss = criterion(out, target.view(-1, 1).float().to(device))\n",
    "        \n",
    "        \n",
    "            #with this gradients are calculated\n",
    "            loss.backward()\n",
    "    \n",
    "            #update gradients\n",
    "            optimizer.step()\n",
    "        \n",
    "            #Set gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #Display iteration statistics\n",
    "            if batch_idx % log_interval == 0:\n",
    "            \n",
    "                #print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(data_loader.dataset),100. * batch_idx / len(data_loader), loss.item()))\n",
    "    \n",
    "        end = time.time()\n",
    "        print('Time: {} '.format(end - start))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9175b4a5",
   "metadata": {},
   "source": [
    "### Test ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0898075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 4\n",
    "#batchsize = 32\n",
    "#batchsize_test = 32\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 60\n",
    "\n",
    "#Network model\n",
    "model = resnet50(weights=\"IMAGENET1K_V1\").to(device)\n",
    "#Disable training for all parameters in pretrained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False   \n",
    "\n",
    "#Change last fully conected layer to a one with 1 outputs. This layer is trainable.\n",
    "model.fc = torch.nn.Sequential(\n",
    "               torch.nn.Linear(2048, 128),\n",
    "               torch.nn.ReLU(inplace=True),\n",
    "               torch.nn.Linear(128, 1),\n",
    "               torch.nn.Sigmoid()).to(device)\n",
    "\n",
    "\n",
    "#Stochastic gradient decent\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=momentum)\n",
    "#Binary cross entropy loss\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12602e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just for testing\n",
    "it = iter(dataloaders['train'])\n",
    "data, target = next(it)\n",
    "data.shape\n",
    "\n",
    "out = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "682b9b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffd1b09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/18743 (0%)]\tLoss: 2.174237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1920/18743 (10%)]\tLoss: 0.141219\n",
      "Train Epoch: 1 [3840/18743 (20%)]\tLoss: 0.130228\n",
      "Train Epoch: 1 [5760/18743 (31%)]\tLoss: 0.098233\n",
      "Train Epoch: 1 [7680/18743 (41%)]\tLoss: 0.031510\n",
      "Train Epoch: 1 [9600/18743 (51%)]\tLoss: 0.111414\n",
      "Train Epoch: 1 [11520/18743 (61%)]\tLoss: 0.051351\n",
      "Train Epoch: 1 [13440/18743 (72%)]\tLoss: 0.027579\n",
      "Train Epoch: 1 [15360/18743 (82%)]\tLoss: 0.114412\n",
      "Train Epoch: 1 [17280/18743 (92%)]\tLoss: 0.070859\n",
      "Time: 27.14565110206604 \n",
      "Train Epoch: 2 [0/18743 (0%)]\tLoss: 0.028978\n",
      "Train Epoch: 2 [1920/18743 (10%)]\tLoss: 0.022781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [3840/18743 (20%)]\tLoss: 0.046417\n",
      "Train Epoch: 2 [5760/18743 (31%)]\tLoss: 0.243914\n",
      "Train Epoch: 2 [7680/18743 (41%)]\tLoss: 0.009209\n",
      "Train Epoch: 2 [9600/18743 (51%)]\tLoss: 0.015112\n",
      "Train Epoch: 2 [11520/18743 (61%)]\tLoss: 0.002013\n",
      "Train Epoch: 2 [13440/18743 (72%)]\tLoss: 0.152536\n",
      "Train Epoch: 2 [15360/18743 (82%)]\tLoss: 0.135802\n",
      "Train Epoch: 2 [17280/18743 (92%)]\tLoss: 0.167618\n",
      "Time: 27.12320303916931 \n",
      "Train Epoch: 3 [0/18743 (0%)]\tLoss: 0.007859\n",
      "Train Epoch: 3 [1920/18743 (10%)]\tLoss: 0.045653\n",
      "Train Epoch: 3 [3840/18743 (20%)]\tLoss: 0.065216\n",
      "Train Epoch: 3 [5760/18743 (31%)]\tLoss: 0.013973\n",
      "Train Epoch: 3 [7680/18743 (41%)]\tLoss: 0.020603\n",
      "Train Epoch: 3 [9600/18743 (51%)]\tLoss: 0.242618\n",
      "Train Epoch: 3 [11520/18743 (61%)]\tLoss: 0.018440\n",
      "Train Epoch: 3 [13440/18743 (72%)]\tLoss: 0.022802\n",
      "Train Epoch: 3 [15360/18743 (82%)]\tLoss: 0.016636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [17280/18743 (92%)]\tLoss: 0.053994\n",
      "Time: 27.060869216918945 \n",
      "Train Epoch: 4 [0/18743 (0%)]\tLoss: 0.060914\n",
      "Train Epoch: 4 [1920/18743 (10%)]\tLoss: 0.015744\n",
      "Train Epoch: 4 [3840/18743 (20%)]\tLoss: 0.060671\n",
      "Train Epoch: 4 [5760/18743 (31%)]\tLoss: 0.010037\n",
      "Train Epoch: 4 [7680/18743 (41%)]\tLoss: 0.053991\n",
      "Train Epoch: 4 [9600/18743 (51%)]\tLoss: 0.025629\n",
      "Train Epoch: 4 [11520/18743 (61%)]\tLoss: 0.034511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [13440/18743 (72%)]\tLoss: 0.037606\n",
      "Train Epoch: 4 [15360/18743 (82%)]\tLoss: 0.049939\n",
      "Train Epoch: 4 [17280/18743 (92%)]\tLoss: 0.259232\n",
      "Time: 26.845443964004517 \n"
     ]
    }
   ],
   "source": [
    "train(model, dataloaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a14438e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0018, Accuracy: 6124/6251 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, dataloaders['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec394790",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a22f6",
   "metadata": {},
   "source": [
    "In notebook 16, I used a simpler model, same training data and validation set, same hyperparameters and same optimizer and criterion function, ran for 4 epochs and got a 76% of acuracy.\n",
    "Using pretrained ResNet50 model, just changing a little the last layers, which are the only trainable by the way, I was able to get 98% of acuracy in just 4 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2306be",
   "metadata": {},
   "source": [
    "### Test Resnet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38699056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 4\n",
    "#batchsize = 32\n",
    "#batchsize_test = 32\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 60\n",
    "\n",
    "#Network model\n",
    "model = torchvision.models.resnet152(weights=ResNet152_Weights.IMAGENET1K_V2).to(device)\n",
    "#Disable training for all parameters in pretrained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False   \n",
    "\n",
    "#Change last fully conected layer to a one with 1 outputs. This layer is trainable.\n",
    "model.fc = torch.nn.Sequential(\n",
    "               torch.nn.Linear(2048, 128),\n",
    "               torch.nn.ReLU(inplace=True),\n",
    "               torch.nn.Linear(128, 1),\n",
    "               torch.nn.Sigmoid()).to(device)\n",
    "\n",
    "\n",
    "#Stochastic gradient decent\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=momentum)\n",
    "#Binary cross entropy loss\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7901b891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/18743 (0%)]\tLoss: 0.687897\n",
      "Train Epoch: 1 [1920/18743 (10%)]\tLoss: 0.379074\n",
      "Train Epoch: 1 [3840/18743 (20%)]\tLoss: 0.135129\n",
      "Train Epoch: 1 [5760/18743 (31%)]\tLoss: 0.135480\n",
      "Train Epoch: 1 [7680/18743 (41%)]\tLoss: 0.064524\n",
      "Train Epoch: 1 [9600/18743 (51%)]\tLoss: 0.043436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [11520/18743 (61%)]\tLoss: 0.072983\n",
      "Train Epoch: 1 [13440/18743 (72%)]\tLoss: 0.042779\n",
      "Train Epoch: 1 [15360/18743 (82%)]\tLoss: 0.076443\n",
      "Train Epoch: 1 [17280/18743 (92%)]\tLoss: 0.060013\n",
      "Time: 58.462249517440796 \n",
      "Train Epoch: 2 [0/18743 (0%)]\tLoss: 0.050965\n",
      "Train Epoch: 2 [1920/18743 (10%)]\tLoss: 0.044757\n",
      "Train Epoch: 2 [3840/18743 (20%)]\tLoss: 0.107943\n",
      "Train Epoch: 2 [5760/18743 (31%)]\tLoss: 0.036360\n",
      "Train Epoch: 2 [7680/18743 (41%)]\tLoss: 0.023734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [9600/18743 (51%)]\tLoss: 0.065608\n",
      "Train Epoch: 2 [11520/18743 (61%)]\tLoss: 0.036655\n",
      "Train Epoch: 2 [13440/18743 (72%)]\tLoss: 0.112597\n",
      "Train Epoch: 2 [15360/18743 (82%)]\tLoss: 0.030062\n",
      "Train Epoch: 2 [17280/18743 (92%)]\tLoss: 0.024547\n",
      "Time: 58.34886598587036 \n",
      "Train Epoch: 3 [0/18743 (0%)]\tLoss: 0.012428\n",
      "Train Epoch: 3 [1920/18743 (10%)]\tLoss: 0.249961\n",
      "Train Epoch: 3 [3840/18743 (20%)]\tLoss: 0.010485\n",
      "Train Epoch: 3 [5760/18743 (31%)]\tLoss: 0.036099\n",
      "Train Epoch: 3 [7680/18743 (41%)]\tLoss: 0.039868\n",
      "Train Epoch: 3 [9600/18743 (51%)]\tLoss: 0.030681\n",
      "Train Epoch: 3 [11520/18743 (61%)]\tLoss: 0.055848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [13440/18743 (72%)]\tLoss: 0.049777\n",
      "Train Epoch: 3 [15360/18743 (82%)]\tLoss: 0.004573\n",
      "Train Epoch: 3 [17280/18743 (92%)]\tLoss: 0.030309\n",
      "Time: 58.48272967338562 \n",
      "Train Epoch: 4 [0/18743 (0%)]\tLoss: 0.002478\n",
      "Train Epoch: 4 [1920/18743 (10%)]\tLoss: 0.039159\n",
      "Train Epoch: 4 [3840/18743 (20%)]\tLoss: 0.007415\n",
      "Train Epoch: 4 [5760/18743 (31%)]\tLoss: 0.028232\n",
      "Train Epoch: 4 [7680/18743 (41%)]\tLoss: 0.034050\n",
      "Train Epoch: 4 [9600/18743 (51%)]\tLoss: 0.067180\n",
      "Train Epoch: 4 [11520/18743 (61%)]\tLoss: 0.216953\n",
      "Train Epoch: 4 [13440/18743 (72%)]\tLoss: 0.010488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [15360/18743 (82%)]\tLoss: 0.027229\n",
      "Train Epoch: 4 [17280/18743 (92%)]\tLoss: 0.153607\n",
      "Time: 58.76094079017639 \n"
     ]
    }
   ],
   "source": [
    "train(model, dataloaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e26d9a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011, Accuracy: 6191/6251 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, dataloaders['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e73a75",
   "metadata": {},
   "source": [
    "For ResNet152, using the same approach as in the last example, we got a 99% of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccb2bf3",
   "metadata": {},
   "source": [
    "### Test EfficientNet b7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "758ab2cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EfficientNet_B7_Weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4552/1544411325.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEfficientNet_B7_Weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGENET1K_V1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'EfficientNet_B7_Weights' is not defined"
     ]
    }
   ],
   "source": [
    "EfficientNet_B7_Weights.IMAGENET1K_V1.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa1307b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = {\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((600,600)),\n",
    "        transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'validation':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((600,600)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    'train': \n",
    "    torchvision.datasets.ImageFolder(TRAIN_DATA_PATH, data_transforms['train']),\n",
    "    'validation': \n",
    "    torchvision.datasets.ImageFolder(TEST_DATA_PATH, data_transforms['validation'])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train':\n",
    "    torch.utils.data.DataLoader(image_datasets['train'],\n",
    "                                batch_size=32,\n",
    "                                shuffle=True,\n",
    "                                num_workers=16),  # for Kaggle\n",
    "    'validation':\n",
    "    torch.utils.data.DataLoader(image_datasets['validation'],\n",
    "                                batch_size=32,\n",
    "                                shuffle=False,\n",
    "                                num_workers=16)  # for Kaggle\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab2a2bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17/3614716404.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 443\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    444\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n",
      "    event_list = self._selector.select(timeout)\n",
      "  File \"/opt/conda/lib/python3.8/selectors.py\", line 468, in select\n",
      "    fd_event_list = self._selector.poll(timeout, max_ev)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 4351) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.\n"
     ]
    }
   ],
   "source": [
    "it = iter(dataloaders['train'])\n",
    "data, target = next(it)\n",
    "data.shape\n",
    "\n",
    "out = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8ccc955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 4\n",
    "#batchsize = 32\n",
    "#batchsize_test = 32\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 60\n",
    "\n",
    "#Network model\n",
    "model = torchvision.models.efficientnet_b7(weights=EfficientNet_B7_Weights.IMAGENET1K_V1).to(device)\n",
    "#Disable training for all parameters in pretrained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False   \n",
    "\n",
    "#Change last fully conected layer to a one with 1 outputs. This layer is trainable.\n",
    "model.fc = torch.nn.Sequential(\n",
    "               torch.nn.Linear(2048, 128),\n",
    "               torch.nn.ReLU(inplace=True),\n",
    "               torch.nn.Linear(128, 1),\n",
    "               torch.nn.Sigmoid()).to(device)\n",
    "\n",
    "\n",
    "#Stochastic gradient decent\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=momentum)\n",
    "#Binary cross entropy loss\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de76f481",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torchvision/datasets/folder.py\", line 232, in __getitem__\n    sample = self.transform(sample)\nTypeError: __init__() takes 1 positional argument but 2 positional arguments (and 3 keyword-only arguments) were given\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17/2505030926.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_17/2318275479.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, test_on_every_epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torchvision/datasets/folder.py\", line 232, in __getitem__\n    sample = self.transform(sample)\nTypeError: __init__() takes 1 positional argument but 2 positional arguments (and 3 keyword-only arguments) were given\n"
     ]
    }
   ],
   "source": [
    "train(model, dataloaders['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd6834",
   "metadata": {},
   "source": [
    "### ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f25440",
   "metadata": {},
   "source": [
    "- Analyze transformation from efficientnet weights\n",
    "- Increase shared memory in case is necessary and train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59b6ee",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931d2f2",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/code/pmigdal/transfer-learning-with-resnet-50-in-pytorch\n",
    "- https://paperswithcode.com/sota/image-classification-on-imagenet\n",
    "- https://pytorch.org/vision/stable/models.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
