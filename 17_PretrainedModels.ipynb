{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "896654c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import time\n",
    "from torchvision.models import resnet50, ResNet50_Weights, ResNet152_Weights, EfficientNet_B7_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe203b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09fd3c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9318ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset location\n",
    "TRAIN_DATA_PATH = \"../Data/CATS_DOGS/train\"\n",
    "TEST_DATA_PATH = \"../Data/CATS_DOGS/test\"\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = {\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'validation':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    'train': \n",
    "    torchvision.datasets.ImageFolder(TRAIN_DATA_PATH, data_transforms['train']),\n",
    "    'validation': \n",
    "    torchvision.datasets.ImageFolder(TEST_DATA_PATH, data_transforms['validation'])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train':\n",
    "    torch.utils.data.DataLoader(image_datasets['train'],\n",
    "                                batch_size=32,\n",
    "                                shuffle=True,\n",
    "                                num_workers=16),  # for Kaggle\n",
    "    'validation':\n",
    "    torch.utils.data.DataLoader(image_datasets['validation'],\n",
    "                                batch_size=32,\n",
    "                                shuffle=False,\n",
    "                                num_workers=16)  # for Kaggle\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ba64fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader):\n",
    "    #Sets the module in evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    #correct.to(device)\n",
    "    \n",
    "    #dont update dynamic computation graph\n",
    "    with torch.no_grad():\n",
    "        #for every example in test\n",
    "        for data, target in data_loader:\n",
    "            \n",
    "            target = target.view(-1, 1).float()\n",
    "            target.to(device)\n",
    "            \n",
    "            #evaluate the model\n",
    "            output = model(data.to(device))\n",
    "            \n",
    "            #acumulate the loss\n",
    "            test_loss += F.binary_cross_entropy(output, target.to(device)).item()\n",
    "            \n",
    "            pred_cls = output.round()\n",
    "            correct += pred_cls.eq(target.view(-1, 1).to(device)).sum() \n",
    "        \n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(data_loader.dataset), 100. * correct / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3664797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, test_on_every_epoch = False):\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "        \n",
    "        if test_on_every_epoch:\n",
    "            test()\n",
    "        \n",
    "        start = time.time()\n",
    "  \n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        \n",
    "            model.train()\n",
    "        \n",
    "            #forward pass\n",
    "            out = model(data.to(device))\n",
    "        \n",
    "            #Use negative log likelihood loss.\n",
    "            loss = criterion(out, target.view(-1, 1).float().to(device))\n",
    "        \n",
    "        \n",
    "            #with this gradients are calculated\n",
    "            loss.backward()\n",
    "    \n",
    "            #update gradients\n",
    "            optimizer.step()\n",
    "        \n",
    "            #Set gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #Display iteration statistics\n",
    "            if batch_idx % log_interval == 0:\n",
    "            \n",
    "                #print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(data_loader.dataset),100. * batch_idx / len(data_loader), loss.item()))\n",
    "    \n",
    "        end = time.time()\n",
    "        print('Time: {} '.format(end - start))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9175b4a5",
   "metadata": {},
   "source": [
    "### Test ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0898075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 4\n",
    "#batchsize = 32\n",
    "#batchsize_test = 32\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 60\n",
    "\n",
    "#Network model\n",
    "model = resnet50(weights=\"IMAGENET1K_V1\").to(device)\n",
    "#Disable training for all parameters in pretrained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False   \n",
    "\n",
    "#Change last fully conected layer to a one with 1 outputs. This layer is trainable.\n",
    "model.fc = torch.nn.Sequential(\n",
    "               torch.nn.Linear(2048, 128),\n",
    "               torch.nn.ReLU(inplace=True),\n",
    "               torch.nn.Linear(128, 1),\n",
    "               torch.nn.Sigmoid()).to(device)\n",
    "\n",
    "\n",
    "#Stochastic gradient decent\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=momentum)\n",
    "#Binary cross entropy loss\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12602e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just for testing\n",
    "it = iter(dataloaders['train'])\n",
    "data, target = next(it)\n",
    "data.shape\n",
    "\n",
    "out = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "682b9b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffd1b09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/18743 (0%)]\tLoss: 2.174237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1920/18743 (10%)]\tLoss: 0.141219\n",
      "Train Epoch: 1 [3840/18743 (20%)]\tLoss: 0.130228\n",
      "Train Epoch: 1 [5760/18743 (31%)]\tLoss: 0.098233\n",
      "Train Epoch: 1 [7680/18743 (41%)]\tLoss: 0.031510\n",
      "Train Epoch: 1 [9600/18743 (51%)]\tLoss: 0.111414\n",
      "Train Epoch: 1 [11520/18743 (61%)]\tLoss: 0.051351\n",
      "Train Epoch: 1 [13440/18743 (72%)]\tLoss: 0.027579\n",
      "Train Epoch: 1 [15360/18743 (82%)]\tLoss: 0.114412\n",
      "Train Epoch: 1 [17280/18743 (92%)]\tLoss: 0.070859\n",
      "Time: 27.14565110206604 \n",
      "Train Epoch: 2 [0/18743 (0%)]\tLoss: 0.028978\n",
      "Train Epoch: 2 [1920/18743 (10%)]\tLoss: 0.022781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [3840/18743 (20%)]\tLoss: 0.046417\n",
      "Train Epoch: 2 [5760/18743 (31%)]\tLoss: 0.243914\n",
      "Train Epoch: 2 [7680/18743 (41%)]\tLoss: 0.009209\n",
      "Train Epoch: 2 [9600/18743 (51%)]\tLoss: 0.015112\n",
      "Train Epoch: 2 [11520/18743 (61%)]\tLoss: 0.002013\n",
      "Train Epoch: 2 [13440/18743 (72%)]\tLoss: 0.152536\n",
      "Train Epoch: 2 [15360/18743 (82%)]\tLoss: 0.135802\n",
      "Train Epoch: 2 [17280/18743 (92%)]\tLoss: 0.167618\n",
      "Time: 27.12320303916931 \n",
      "Train Epoch: 3 [0/18743 (0%)]\tLoss: 0.007859\n",
      "Train Epoch: 3 [1920/18743 (10%)]\tLoss: 0.045653\n",
      "Train Epoch: 3 [3840/18743 (20%)]\tLoss: 0.065216\n",
      "Train Epoch: 3 [5760/18743 (31%)]\tLoss: 0.013973\n",
      "Train Epoch: 3 [7680/18743 (41%)]\tLoss: 0.020603\n",
      "Train Epoch: 3 [9600/18743 (51%)]\tLoss: 0.242618\n",
      "Train Epoch: 3 [11520/18743 (61%)]\tLoss: 0.018440\n",
      "Train Epoch: 3 [13440/18743 (72%)]\tLoss: 0.022802\n",
      "Train Epoch: 3 [15360/18743 (82%)]\tLoss: 0.016636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [17280/18743 (92%)]\tLoss: 0.053994\n",
      "Time: 27.060869216918945 \n",
      "Train Epoch: 4 [0/18743 (0%)]\tLoss: 0.060914\n",
      "Train Epoch: 4 [1920/18743 (10%)]\tLoss: 0.015744\n",
      "Train Epoch: 4 [3840/18743 (20%)]\tLoss: 0.060671\n",
      "Train Epoch: 4 [5760/18743 (31%)]\tLoss: 0.010037\n",
      "Train Epoch: 4 [7680/18743 (41%)]\tLoss: 0.053991\n",
      "Train Epoch: 4 [9600/18743 (51%)]\tLoss: 0.025629\n",
      "Train Epoch: 4 [11520/18743 (61%)]\tLoss: 0.034511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [13440/18743 (72%)]\tLoss: 0.037606\n",
      "Train Epoch: 4 [15360/18743 (82%)]\tLoss: 0.049939\n",
      "Train Epoch: 4 [17280/18743 (92%)]\tLoss: 0.259232\n",
      "Time: 26.845443964004517 \n"
     ]
    }
   ],
   "source": [
    "train(model, dataloaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a14438e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0018, Accuracy: 6124/6251 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, dataloaders['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec394790",
   "metadata": {},
   "source": [
    "### So?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a22f6",
   "metadata": {},
   "source": [
    "In notebook 16, I used a simpler model, same training data and validation set, same hyperparameters and same optimizer and criterion function, ran for 4 epochs and got a 76% of acuracy.\n",
    "Using pretrained ResNet50 model, just changing a little the last layers, which are the only trainable by the way, I was able to get 98% of acuracy in just 4 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2306be",
   "metadata": {},
   "source": [
    "### Test Resnet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38699056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 4\n",
    "#batchsize = 32\n",
    "#batchsize_test = 32\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 60\n",
    "\n",
    "#Network model\n",
    "model = torchvision.models.resnet152(weights=ResNet152_Weights.IMAGENET1K_V2).to(device)\n",
    "#Disable training for all parameters in pretrained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False   \n",
    "\n",
    "#Change last fully conected layer to a one with 1 outputs. This layer is trainable.\n",
    "model.fc = torch.nn.Sequential(\n",
    "               torch.nn.Linear(2048, 128),\n",
    "               torch.nn.ReLU(inplace=True),\n",
    "               torch.nn.Linear(128, 1),\n",
    "               torch.nn.Sigmoid()).to(device)\n",
    "\n",
    "\n",
    "#Stochastic gradient decent\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=momentum)\n",
    "#Binary cross entropy loss\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7901b891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/18743 (0%)]\tLoss: 0.687897\n",
      "Train Epoch: 1 [1920/18743 (10%)]\tLoss: 0.379074\n",
      "Train Epoch: 1 [3840/18743 (20%)]\tLoss: 0.135129\n",
      "Train Epoch: 1 [5760/18743 (31%)]\tLoss: 0.135480\n",
      "Train Epoch: 1 [7680/18743 (41%)]\tLoss: 0.064524\n",
      "Train Epoch: 1 [9600/18743 (51%)]\tLoss: 0.043436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [11520/18743 (61%)]\tLoss: 0.072983\n",
      "Train Epoch: 1 [13440/18743 (72%)]\tLoss: 0.042779\n",
      "Train Epoch: 1 [15360/18743 (82%)]\tLoss: 0.076443\n",
      "Train Epoch: 1 [17280/18743 (92%)]\tLoss: 0.060013\n",
      "Time: 58.462249517440796 \n",
      "Train Epoch: 2 [0/18743 (0%)]\tLoss: 0.050965\n",
      "Train Epoch: 2 [1920/18743 (10%)]\tLoss: 0.044757\n",
      "Train Epoch: 2 [3840/18743 (20%)]\tLoss: 0.107943\n",
      "Train Epoch: 2 [5760/18743 (31%)]\tLoss: 0.036360\n",
      "Train Epoch: 2 [7680/18743 (41%)]\tLoss: 0.023734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [9600/18743 (51%)]\tLoss: 0.065608\n",
      "Train Epoch: 2 [11520/18743 (61%)]\tLoss: 0.036655\n",
      "Train Epoch: 2 [13440/18743 (72%)]\tLoss: 0.112597\n",
      "Train Epoch: 2 [15360/18743 (82%)]\tLoss: 0.030062\n",
      "Train Epoch: 2 [17280/18743 (92%)]\tLoss: 0.024547\n",
      "Time: 58.34886598587036 \n",
      "Train Epoch: 3 [0/18743 (0%)]\tLoss: 0.012428\n",
      "Train Epoch: 3 [1920/18743 (10%)]\tLoss: 0.249961\n",
      "Train Epoch: 3 [3840/18743 (20%)]\tLoss: 0.010485\n",
      "Train Epoch: 3 [5760/18743 (31%)]\tLoss: 0.036099\n",
      "Train Epoch: 3 [7680/18743 (41%)]\tLoss: 0.039868\n",
      "Train Epoch: 3 [9600/18743 (51%)]\tLoss: 0.030681\n",
      "Train Epoch: 3 [11520/18743 (61%)]\tLoss: 0.055848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [13440/18743 (72%)]\tLoss: 0.049777\n",
      "Train Epoch: 3 [15360/18743 (82%)]\tLoss: 0.004573\n",
      "Train Epoch: 3 [17280/18743 (92%)]\tLoss: 0.030309\n",
      "Time: 58.48272967338562 \n",
      "Train Epoch: 4 [0/18743 (0%)]\tLoss: 0.002478\n",
      "Train Epoch: 4 [1920/18743 (10%)]\tLoss: 0.039159\n",
      "Train Epoch: 4 [3840/18743 (20%)]\tLoss: 0.007415\n",
      "Train Epoch: 4 [5760/18743 (31%)]\tLoss: 0.028232\n",
      "Train Epoch: 4 [7680/18743 (41%)]\tLoss: 0.034050\n",
      "Train Epoch: 4 [9600/18743 (51%)]\tLoss: 0.067180\n",
      "Train Epoch: 4 [11520/18743 (61%)]\tLoss: 0.216953\n",
      "Train Epoch: 4 [13440/18743 (72%)]\tLoss: 0.010488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [15360/18743 (82%)]\tLoss: 0.027229\n",
      "Train Epoch: 4 [17280/18743 (92%)]\tLoss: 0.153607\n",
      "Time: 58.76094079017639 \n"
     ]
    }
   ],
   "source": [
    "train(model, dataloaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e26d9a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0011, Accuracy: 6191/6251 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, dataloaders['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b5bc7",
   "metadata": {},
   "source": [
    "### So?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e73a75",
   "metadata": {},
   "source": [
    "For ResNet152, using the same approach as in the last example, we got a 99% of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccb2bf3",
   "metadata": {},
   "source": [
    "### Test EfficientNet b7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758ab2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<class 'torchvision.transforms._presets.ImageClassification'>, crop_size=600, resize_size=600, interpolation=<InterpolationMode.BICUBIC: 'bicubic'>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EfficientNet_B7_Weights.IMAGENET1K_V1.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa1307b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = {\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((600,600)),\n",
    "        transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'validation':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((600,600)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    'train': \n",
    "    torchvision.datasets.ImageFolder(TRAIN_DATA_PATH, data_transforms['train']),\n",
    "    'validation': \n",
    "    torchvision.datasets.ImageFolder(TEST_DATA_PATH, data_transforms['validation'])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train':\n",
    "    torch.utils.data.DataLoader(image_datasets['train'],\n",
    "                                batch_size=16,\n",
    "                                shuffle=True,\n",
    "                                num_workers=16),  # for Kaggle\n",
    "    'validation':\n",
    "    torch.utils.data.DataLoader(image_datasets['validation'],\n",
    "                                batch_size=16,\n",
    "                                shuffle=False,\n",
    "                                num_workers=16)  # for Kaggle\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab2a2bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(dataloaders['train'])\n",
    "data, target = next(it)\n",
    "data.shape\n",
    "\n",
    "out = model(data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c47d7568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.5, inplace=True)\n",
       "  (1): Linear(in_features=2560, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8ccc955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 4\n",
    "#batchsize = 32\n",
    "#batchsize_test = 32\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 60\n",
    "\n",
    "#Network model\n",
    "model = torchvision.models.efficientnet_b7(weights=EfficientNet_B7_Weights.IMAGENET1K_V1).to(device)\n",
    "#Disable training for all parameters in pretrained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False   \n",
    "\n",
    "#Change last fully conected layer to a one with 1 outputs. This layer is trainable.\n",
    "model.classifier = torch.nn.Sequential(\n",
    "               torch.nn.Linear(2560, 128),\n",
    "               torch.nn.ReLU(inplace=True),\n",
    "               torch.nn.Linear(128, 1),\n",
    "               torch.nn.Sigmoid()).to(device)\n",
    "\n",
    "\n",
    "#Stochastic gradient decent\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=momentum)\n",
    "#Binary cross entropy loss\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de76f481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/18743 (0%)]\tLoss: 0.695592\n",
      "Train Epoch: 1 [960/18743 (5%)]\tLoss: 0.639343\n",
      "Train Epoch: 1 [1920/18743 (10%)]\tLoss: 0.458206\n",
      "Train Epoch: 1 [2880/18743 (15%)]\tLoss: 0.341754\n",
      "Train Epoch: 1 [3840/18743 (20%)]\tLoss: 0.203342\n",
      "Train Epoch: 1 [4800/18743 (26%)]\tLoss: 0.158357\n",
      "Train Epoch: 1 [5760/18743 (31%)]\tLoss: 0.196246\n",
      "Train Epoch: 1 [6720/18743 (36%)]\tLoss: 0.120954\n",
      "Train Epoch: 1 [7680/18743 (41%)]\tLoss: 0.168544\n",
      "Train Epoch: 1 [8640/18743 (46%)]\tLoss: 0.046757\n",
      "Train Epoch: 1 [9600/18743 (51%)]\tLoss: 0.060458\n",
      "Train Epoch: 1 [10560/18743 (56%)]\tLoss: 0.065150\n",
      "Train Epoch: 1 [11520/18743 (61%)]\tLoss: 0.142871\n",
      "Train Epoch: 1 [12480/18743 (67%)]\tLoss: 0.080331\n",
      "Train Epoch: 1 [13440/18743 (72%)]\tLoss: 0.036240\n",
      "Train Epoch: 1 [14400/18743 (77%)]\tLoss: 0.047595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [15360/18743 (82%)]\tLoss: 0.093751\n",
      "Train Epoch: 1 [16320/18743 (87%)]\tLoss: 0.031336\n",
      "Train Epoch: 1 [17280/18743 (92%)]\tLoss: 0.041792\n",
      "Train Epoch: 1 [18240/18743 (97%)]\tLoss: 0.031712\n",
      "Time: 541.9774088859558 \n",
      "Train Epoch: 2 [0/18743 (0%)]\tLoss: 0.006529\n",
      "Train Epoch: 2 [960/18743 (5%)]\tLoss: 0.039117\n",
      "Train Epoch: 2 [1920/18743 (10%)]\tLoss: 0.016267\n",
      "Train Epoch: 2 [2880/18743 (15%)]\tLoss: 0.277881\n",
      "Train Epoch: 2 [3840/18743 (20%)]\tLoss: 0.137085\n",
      "Train Epoch: 2 [4800/18743 (26%)]\tLoss: 0.267798\n",
      "Train Epoch: 2 [5760/18743 (31%)]\tLoss: 0.037081\n",
      "Train Epoch: 2 [6720/18743 (36%)]\tLoss: 0.054386\n",
      "Train Epoch: 2 [7680/18743 (41%)]\tLoss: 0.048425\n",
      "Train Epoch: 2 [8640/18743 (46%)]\tLoss: 0.059025\n",
      "Train Epoch: 2 [9600/18743 (51%)]\tLoss: 0.009134\n",
      "Train Epoch: 2 [10560/18743 (56%)]\tLoss: 0.023747\n",
      "Train Epoch: 2 [11520/18743 (61%)]\tLoss: 0.013351\n",
      "Train Epoch: 2 [12480/18743 (67%)]\tLoss: 0.017892\n",
      "Train Epoch: 2 [13440/18743 (72%)]\tLoss: 0.011756\n",
      "Train Epoch: 2 [14400/18743 (77%)]\tLoss: 0.035944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [15360/18743 (82%)]\tLoss: 0.125342\n",
      "Train Epoch: 2 [16320/18743 (87%)]\tLoss: 0.289530\n",
      "Train Epoch: 2 [17280/18743 (92%)]\tLoss: 0.061905\n",
      "Train Epoch: 2 [18240/18743 (97%)]\tLoss: 0.004642\n",
      "Time: 543.5254535675049 \n",
      "Train Epoch: 3 [0/18743 (0%)]\tLoss: 0.079853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [960/18743 (5%)]\tLoss: 0.037287\n",
      "Train Epoch: 3 [1920/18743 (10%)]\tLoss: 0.006270\n",
      "Train Epoch: 3 [2880/18743 (15%)]\tLoss: 0.007050\n",
      "Train Epoch: 3 [3840/18743 (20%)]\tLoss: 0.072400\n",
      "Train Epoch: 3 [4800/18743 (26%)]\tLoss: 0.071890\n",
      "Train Epoch: 3 [5760/18743 (31%)]\tLoss: 0.012660\n",
      "Train Epoch: 3 [6720/18743 (36%)]\tLoss: 0.079342\n",
      "Train Epoch: 3 [7680/18743 (41%)]\tLoss: 0.031960\n",
      "Train Epoch: 3 [8640/18743 (46%)]\tLoss: 0.041830\n",
      "Train Epoch: 3 [9600/18743 (51%)]\tLoss: 0.036380\n",
      "Train Epoch: 3 [10560/18743 (56%)]\tLoss: 0.182501\n",
      "Train Epoch: 3 [11520/18743 (61%)]\tLoss: 0.042463\n",
      "Train Epoch: 3 [12480/18743 (67%)]\tLoss: 0.005602\n",
      "Train Epoch: 3 [13440/18743 (72%)]\tLoss: 0.023093\n",
      "Train Epoch: 3 [14400/18743 (77%)]\tLoss: 0.009413\n",
      "Train Epoch: 3 [15360/18743 (82%)]\tLoss: 0.098556\n",
      "Train Epoch: 3 [16320/18743 (87%)]\tLoss: 0.041606\n",
      "Train Epoch: 3 [17280/18743 (92%)]\tLoss: 0.062724\n",
      "Train Epoch: 3 [18240/18743 (97%)]\tLoss: 0.052399\n",
      "Time: 570.105890750885 \n",
      "Train Epoch: 4 [0/18743 (0%)]\tLoss: 0.019403\n",
      "Train Epoch: 4 [960/18743 (5%)]\tLoss: 0.016966\n",
      "Train Epoch: 4 [1920/18743 (10%)]\tLoss: 0.005583\n",
      "Train Epoch: 4 [2880/18743 (15%)]\tLoss: 0.318524\n",
      "Train Epoch: 4 [3840/18743 (20%)]\tLoss: 0.077989\n",
      "Train Epoch: 4 [4800/18743 (26%)]\tLoss: 0.019075\n",
      "Train Epoch: 4 [5760/18743 (31%)]\tLoss: 0.048390\n",
      "Train Epoch: 4 [6720/18743 (36%)]\tLoss: 0.005449\n",
      "Train Epoch: 4 [7680/18743 (41%)]\tLoss: 0.003815\n",
      "Train Epoch: 4 [8640/18743 (46%)]\tLoss: 0.008045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:850: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [9600/18743 (51%)]\tLoss: 0.216792\n",
      "Train Epoch: 4 [10560/18743 (56%)]\tLoss: 0.006671\n",
      "Train Epoch: 4 [11520/18743 (61%)]\tLoss: 0.007289\n",
      "Train Epoch: 4 [12480/18743 (67%)]\tLoss: 0.004812\n",
      "Train Epoch: 4 [13440/18743 (72%)]\tLoss: 0.034706\n",
      "Train Epoch: 4 [14400/18743 (77%)]\tLoss: 0.016296\n",
      "Train Epoch: 4 [15360/18743 (82%)]\tLoss: 0.042554\n",
      "Train Epoch: 4 [16320/18743 (87%)]\tLoss: 0.044532\n",
      "Train Epoch: 4 [17280/18743 (92%)]\tLoss: 0.063566\n",
      "Train Epoch: 4 [18240/18743 (97%)]\tLoss: 0.002083\n",
      "Time: 550.1512305736542 \n"
     ]
    }
   ],
   "source": [
    "train(model, dataloaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc801c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0018, Accuracy: 6208/6251 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, dataloaders['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd6834",
   "metadata": {},
   "source": [
    "### ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f25440",
   "metadata": {},
   "source": [
    "- NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59b6ee",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931d2f2",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/code/pmigdal/transfer-learning-with-resnet-50-in-pytorch\n",
    "- https://paperswithcode.com/sota/image-classification-on-imagenet\n",
    "- https://pytorch.org/vision/stable/models.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
