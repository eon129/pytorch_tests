{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "traditional-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "authentic-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset = datasets.load_wine()\n",
    "\n",
    "X, Y = wine_dataset.data, wine_dataset.target \n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "#normalize inputs\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "\n",
    "#to tensor\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.int64))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "earlier-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleClassification(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(MultipleClassification, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(n_input, n_hidden)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "julian-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rate = 0.01\n",
    "n_epoch = 600\n",
    "\n",
    "#model\n",
    "model = MultipleClassification(n_features, 3, 3)\n",
    "\n",
    "#criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#omptimizer\n",
    "optim = torch.optim.SGD(model.parameters(), lr=l_rate)\n",
    "#optim = torch.optim.Adam(model.parameters(), lr=l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "narrative-pillow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1724226474761963\n",
      "Loss: 1.1703236103057861\n",
      "Loss: 1.1682567596435547\n",
      "Loss: 1.1662046909332275\n",
      "Loss: 1.1641550064086914\n",
      "Loss: 1.1621360778808594\n",
      "Loss: 1.1601452827453613\n",
      "Loss: 1.158182144165039\n",
      "Loss: 1.1562482118606567\n",
      "Loss: 1.1543042659759521\n",
      "Loss: 1.152330756187439\n",
      "Loss: 1.1503831148147583\n",
      "Loss: 1.1484489440917969\n",
      "Loss: 1.1464972496032715\n",
      "Loss: 1.144569754600525\n",
      "Loss: 1.1426434516906738\n",
      "Loss: 1.1406382322311401\n",
      "Loss: 1.138657808303833\n",
      "Loss: 1.1366974115371704\n",
      "Loss: 1.1347225904464722\n",
      "Loss: 1.1327630281448364\n",
      "Loss: 1.13067626953125\n",
      "Loss: 1.128499150276184\n",
      "Loss: 1.1263480186462402\n",
      "Loss: 1.1241976022720337\n",
      "Loss: 1.121995449066162\n",
      "Loss: 1.1197350025177002\n",
      "Loss: 1.1174509525299072\n",
      "Loss: 1.1149330139160156\n",
      "Loss: 1.112345576286316\n",
      "Loss: 1.1097229719161987\n",
      "Loss: 1.1071289777755737\n",
      "Loss: 1.104555606842041\n",
      "Loss: 1.101912498474121\n",
      "Loss: 1.0992299318313599\n",
      "Loss: 1.096553921699524\n",
      "Loss: 1.0938705205917358\n",
      "Loss: 1.09114670753479\n",
      "Loss: 1.0883325338363647\n",
      "Loss: 1.0854876041412354\n",
      "Loss: 1.0826681852340698\n",
      "Loss: 1.0798723697662354\n",
      "Loss: 1.0771002769470215\n",
      "Loss: 1.0743508338928223\n",
      "Loss: 1.0716230869293213\n",
      "Loss: 1.0689009428024292\n",
      "Loss: 1.0661237239837646\n",
      "Loss: 1.0632891654968262\n",
      "Loss: 1.0604792833328247\n",
      "Loss: 1.05769944190979\n",
      "Loss: 1.0549397468566895\n",
      "Loss: 1.0521985292434692\n",
      "Loss: 1.0494753122329712\n",
      "Loss: 1.046769142150879\n",
      "Loss: 1.0440796613693237\n",
      "Loss: 1.0414074659347534\n",
      "Loss: 1.0387499332427979\n",
      "Loss: 1.0361071825027466\n",
      "Loss: 1.0334796905517578\n",
      "Loss: 1.030866265296936\n",
      "Loss: 1.0282707214355469\n",
      "Loss: 1.0257225036621094\n",
      "Loss: 1.0231857299804688\n",
      "Loss: 1.0206820964813232\n",
      "Loss: 1.0182461738586426\n",
      "Loss: 1.015821933746338\n",
      "Loss: 1.013406753540039\n",
      "Loss: 1.0110023021697998\n",
      "Loss: 1.0086067914962769\n",
      "Loss: 1.0062211751937866\n",
      "Loss: 1.003841519355774\n",
      "Loss: 1.0014299154281616\n",
      "Loss: 0.9990266561508179\n",
      "Loss: 0.9966319799423218\n",
      "Loss: 0.9942741990089417\n",
      "Loss: 0.9919606447219849\n",
      "Loss: 0.9896751642227173\n",
      "Loss: 0.9873867630958557\n",
      "Loss: 0.9850471615791321\n",
      "Loss: 0.9827193021774292\n",
      "Loss: 0.9804260730743408\n",
      "Loss: 0.9781392216682434\n",
      "Loss: 0.9758826494216919\n",
      "Loss: 0.9736728072166443\n",
      "Loss: 0.9714687466621399\n",
      "Loss: 0.9692875742912292\n",
      "Loss: 0.967159628868103\n",
      "Loss: 0.9650367498397827\n",
      "Loss: 0.962918758392334\n",
      "Loss: 0.9608064889907837\n",
      "Loss: 0.9587083458900452\n",
      "Loss: 0.9566267728805542\n",
      "Loss: 0.9545636177062988\n",
      "Loss: 0.9525103569030762\n",
      "Loss: 0.9504622220993042\n",
      "Loss: 0.9484201073646545\n",
      "Loss: 0.9464130401611328\n",
      "Loss: 0.9444107413291931\n",
      "Loss: 0.9424130320549011\n",
      "Loss: 0.9404208660125732\n",
      "Loss: 0.9384390711784363\n",
      "Loss: 0.936475932598114\n",
      "Loss: 0.9345405697822571\n",
      "Loss: 0.9326215982437134\n",
      "Loss: 0.9307215809822083\n",
      "Loss: 0.9288268685340881\n",
      "Loss: 0.9269355535507202\n",
      "Loss: 0.9250715970993042\n",
      "Loss: 0.9232413172721863\n",
      "Loss: 0.9214165806770325\n",
      "Loss: 0.919596254825592\n",
      "Loss: 0.9177818894386292\n",
      "Loss: 0.9159722328186035\n",
      "Loss: 0.914171576499939\n",
      "Loss: 0.9123895168304443\n",
      "Loss: 0.9106225967407227\n",
      "Loss: 0.9088620543479919\n",
      "Loss: 0.9071061015129089\n",
      "Loss: 0.9053562879562378\n",
      "Loss: 0.9036110043525696\n",
      "Loss: 0.901870846748352\n",
      "Loss: 0.9001363515853882\n",
      "Loss: 0.8984072208404541\n",
      "Loss: 0.8966839909553528\n",
      "Loss: 0.8949676752090454\n",
      "Loss: 0.8932667970657349\n",
      "Loss: 0.8915724754333496\n",
      "Loss: 0.889892041683197\n",
      "Loss: 0.8882173895835876\n",
      "Loss: 0.8865551352500916\n",
      "Loss: 0.884919285774231\n",
      "Loss: 0.8833087086677551\n",
      "Loss: 0.881701648235321\n",
      "Loss: 0.8801009058952332\n",
      "Loss: 0.8785057663917542\n",
      "Loss: 0.8769170641899109\n",
      "Loss: 0.8753337860107422\n",
      "Loss: 0.873757004737854\n",
      "Loss: 0.8721863627433777\n",
      "Loss: 0.8706226348876953\n",
      "Loss: 0.8690747022628784\n",
      "Loss: 0.8675329089164734\n",
      "Loss: 0.8659976720809937\n",
      "Loss: 0.8644680976867676\n",
      "Loss: 0.8629447221755981\n",
      "Loss: 0.8614276051521301\n",
      "Loss: 0.8599159717559814\n",
      "Loss: 0.8584108948707581\n",
      "Loss: 0.8569117188453674\n",
      "Loss: 0.855418860912323\n",
      "Loss: 0.8539322018623352\n",
      "Loss: 0.8524510860443115\n",
      "Loss: 0.8509764671325684\n",
      "Loss: 0.849507749080658\n",
      "Loss: 0.8480448722839355\n",
      "Loss: 0.8465886116027832\n",
      "Loss: 0.8451377153396606\n",
      "Loss: 0.8436933159828186\n",
      "Loss: 0.8422557711601257\n",
      "Loss: 0.8408293128013611\n",
      "Loss: 0.839409589767456\n",
      "Loss: 0.8379955887794495\n",
      "Loss: 0.8365836143493652\n",
      "Loss: 0.835178792476654\n",
      "Loss: 0.8337871432304382\n",
      "Loss: 0.8324010968208313\n",
      "Loss: 0.8310223817825317\n",
      "Loss: 0.8296537399291992\n",
      "Loss: 0.8282909393310547\n",
      "Loss: 0.8269334435462952\n",
      "Loss: 0.8255823850631714\n",
      "Loss: 0.8242362141609192\n",
      "Loss: 0.8228976130485535\n",
      "Loss: 0.8215703368186951\n",
      "Loss: 0.8202486038208008\n",
      "Loss: 0.8189324140548706\n",
      "Loss: 0.8176229000091553\n",
      "Loss: 0.8163213133811951\n",
      "Loss: 0.8150255680084229\n",
      "Loss: 0.8137351274490356\n",
      "Loss: 0.8124496340751648\n",
      "Loss: 0.8111661076545715\n",
      "Loss: 0.809887707233429\n",
      "Loss: 0.8086145520210266\n",
      "Loss: 0.8073467016220093\n",
      "Loss: 0.8060838580131531\n",
      "Loss: 0.8048256635665894\n",
      "Loss: 0.8035730719566345\n",
      "Loss: 0.8023250699043274\n",
      "Loss: 0.8010822534561157\n",
      "Loss: 0.799843966960907\n",
      "Loss: 0.7986107468605042\n",
      "Loss: 0.7973816394805908\n",
      "Loss: 0.7961562871932983\n",
      "Loss: 0.7949311137199402\n",
      "Loss: 0.7937083840370178\n",
      "Loss: 0.792490541934967\n",
      "Loss: 0.7912771701812744\n",
      "Loss: 0.7900680303573608\n",
      "Loss: 0.7888637185096741\n",
      "Loss: 0.7876636981964111\n",
      "Loss: 0.786467432975769\n",
      "Loss: 0.7852753400802612\n",
      "Loss: 0.7840873599052429\n",
      "Loss: 0.7829033136367798\n",
      "Loss: 0.7817232012748718\n",
      "Loss: 0.7805467247962952\n",
      "Loss: 0.7793745398521423\n",
      "Loss: 0.7782055735588074\n",
      "Loss: 0.7770400643348694\n",
      "Loss: 0.7758785486221313\n",
      "Loss: 0.7747208476066589\n",
      "Loss: 0.773566484451294\n",
      "Loss: 0.7724151015281677\n",
      "Loss: 0.7712676525115967\n",
      "Loss: 0.7701232433319092\n",
      "Loss: 0.7689818143844604\n",
      "Loss: 0.7678437829017639\n",
      "Loss: 0.7667090892791748\n",
      "Loss: 0.7655774354934692\n",
      "Loss: 0.7644485831260681\n",
      "Loss: 0.7633227705955505\n",
      "Loss: 0.762199342250824\n",
      "Loss: 0.7610785961151123\n",
      "Loss: 0.7599604725837708\n",
      "Loss: 0.7588450312614441\n",
      "Loss: 0.7577319145202637\n",
      "Loss: 0.7566216588020325\n",
      "Loss: 0.7555137276649475\n",
      "Loss: 0.7544083595275879\n",
      "Loss: 0.7533050775527954\n",
      "Loss: 0.7522045969963074\n",
      "Loss: 0.7511061429977417\n",
      "Loss: 0.7500095963478088\n",
      "Loss: 0.7489154934883118\n",
      "Loss: 0.7478232979774475\n",
      "Loss: 0.7467327117919922\n",
      "Loss: 0.7456444501876831\n",
      "Loss: 0.7445585131645203\n",
      "Loss: 0.7434738278388977\n",
      "Loss: 0.742391049861908\n",
      "Loss: 0.7413104176521301\n",
      "Loss: 0.740230917930603\n",
      "Loss: 0.7391529083251953\n",
      "Loss: 0.7380764484405518\n",
      "Loss: 0.7370015382766724\n",
      "Loss: 0.7359278798103333\n",
      "Loss: 0.7348549962043762\n",
      "Loss: 0.7337843179702759\n",
      "Loss: 0.7327146530151367\n",
      "Loss: 0.731645941734314\n",
      "Loss: 0.7305785417556763\n",
      "Loss: 0.7295121550559998\n",
      "Loss: 0.728446364402771\n",
      "Loss: 0.7273818254470825\n",
      "Loss: 0.7263174653053284\n",
      "Loss: 0.7252544164657593\n",
      "Loss: 0.7241914868354797\n",
      "Loss: 0.7231295704841614\n",
      "Loss: 0.722065269947052\n",
      "Loss: 0.7209969758987427\n",
      "Loss: 0.7199280261993408\n",
      "Loss: 0.718858540058136\n",
      "Loss: 0.717788577079773\n",
      "Loss: 0.7167187929153442\n",
      "Loss: 0.7156490087509155\n",
      "Loss: 0.7145799994468689\n",
      "Loss: 0.7135104537010193\n",
      "Loss: 0.712440550327301\n",
      "Loss: 0.7113704681396484\n",
      "Loss: 0.710300087928772\n",
      "Loss: 0.7092294692993164\n",
      "Loss: 0.7081584334373474\n",
      "Loss: 0.7070854306221008\n",
      "Loss: 0.706004798412323\n",
      "Loss: 0.7049245834350586\n",
      "Loss: 0.7038461565971375\n",
      "Loss: 0.7027670741081238\n",
      "Loss: 0.7016865611076355\n",
      "Loss: 0.7006058096885681\n",
      "Loss: 0.6995231509208679\n",
      "Loss: 0.6984399557113647\n",
      "Loss: 0.6973559260368347\n",
      "Loss: 0.6962715983390808\n",
      "Loss: 0.6951863169670105\n",
      "Loss: 0.6940991878509521\n",
      "Loss: 0.693003237247467\n",
      "Loss: 0.6919063925743103\n",
      "Loss: 0.6908069252967834\n",
      "Loss: 0.6897062063217163\n",
      "Loss: 0.6886035799980164\n",
      "Loss: 0.6874991059303284\n",
      "Loss: 0.6863927841186523\n",
      "Loss: 0.6852841377258301\n",
      "Loss: 0.6841678023338318\n",
      "Loss: 0.6830489635467529\n",
      "Loss: 0.6819276213645935\n",
      "Loss: 0.6808043718338013\n",
      "Loss: 0.6796784996986389\n",
      "Loss: 0.6785498857498169\n",
      "Loss: 0.6774189472198486\n",
      "Loss: 0.6762850880622864\n",
      "Loss: 0.6751485466957092\n",
      "Loss: 0.6740097999572754\n",
      "Loss: 0.6728678345680237\n",
      "Loss: 0.6717230677604675\n",
      "Loss: 0.6705756783485413\n",
      "Loss: 0.6694248914718628\n",
      "Loss: 0.6682713031768799\n",
      "Loss: 0.6671143770217896\n",
      "Loss: 0.6659546494483948\n",
      "Loss: 0.6647914052009583\n",
      "Loss: 0.6636251211166382\n",
      "Loss: 0.6624550819396973\n",
      "Loss: 0.6612704992294312\n",
      "Loss: 0.6600845456123352\n",
      "Loss: 0.6588951349258423\n",
      "Loss: 0.6577039957046509\n",
      "Loss: 0.6565093994140625\n",
      "Loss: 0.6553110480308533\n",
      "Loss: 0.6541091799736023\n",
      "Loss: 0.65290367603302\n",
      "Loss: 0.6516954302787781\n",
      "Loss: 0.6504842638969421\n",
      "Loss: 0.6492722630500793\n",
      "Loss: 0.6480563282966614\n",
      "Loss: 0.6468366384506226\n",
      "Loss: 0.6456127166748047\n",
      "Loss: 0.6443849802017212\n",
      "Loss: 0.643153727054596\n",
      "Loss: 0.6419203877449036\n",
      "Loss: 0.6406829953193665\n",
      "Loss: 0.6394413113594055\n",
      "Loss: 0.6381957530975342\n",
      "Loss: 0.6369457244873047\n",
      "Loss: 0.635691225528717\n",
      "Loss: 0.6344326138496399\n",
      "Loss: 0.6331692337989807\n",
      "Loss: 0.6319027543067932\n",
      "Loss: 0.630635142326355\n",
      "Loss: 0.6293631792068481\n",
      "Loss: 0.6280866861343384\n",
      "Loss: 0.6268059015274048\n",
      "Loss: 0.6255205869674683\n",
      "Loss: 0.6242311596870422\n",
      "Loss: 0.6229369044303894\n",
      "Loss: 0.6216378808021545\n",
      "Loss: 0.6203349232673645\n",
      "Loss: 0.6190266013145447\n",
      "Loss: 0.6177142262458801\n",
      "Loss: 0.6163970232009888\n",
      "Loss: 0.6150755286216736\n",
      "Loss: 0.6137489676475525\n",
      "Loss: 0.612417995929718\n",
      "Loss: 0.6110822558403015\n",
      "Loss: 0.6097419857978821\n",
      "Loss: 0.6083968281745911\n",
      "Loss: 0.6070466637611389\n",
      "Loss: 0.6056923866271973\n",
      "Loss: 0.6043328642845154\n",
      "Loss: 0.6029689311981201\n",
      "Loss: 0.6016018390655518\n",
      "Loss: 0.600232720375061\n",
      "Loss: 0.5988591909408569\n",
      "Loss: 0.5974807739257812\n",
      "Loss: 0.5960978269577026\n",
      "Loss: 0.5947083234786987\n",
      "Loss: 0.593310534954071\n",
      "Loss: 0.5919081568717957\n",
      "Loss: 0.590501070022583\n",
      "Loss: 0.5890890955924988\n",
      "Loss: 0.5876723527908325\n",
      "Loss: 0.586250901222229\n",
      "Loss: 0.584823489189148\n",
      "Loss: 0.5833753347396851\n",
      "Loss: 0.5819222927093506\n",
      "Loss: 0.5804643034934998\n",
      "Loss: 0.5790013670921326\n",
      "Loss: 0.5775334239006042\n",
      "Loss: 0.576060950756073\n",
      "Loss: 0.5745835900306702\n",
      "Loss: 0.5731009840965271\n",
      "Loss: 0.5716139674186707\n",
      "Loss: 0.5701259970664978\n",
      "Loss: 0.5686354041099548\n",
      "Loss: 0.5671361684799194\n",
      "Loss: 0.5656207203865051\n",
      "Loss: 0.5641005039215088\n",
      "Loss: 0.5625754594802856\n",
      "Loss: 0.56104576587677\n",
      "Loss: 0.5595117807388306\n",
      "Loss: 0.5579733848571777\n",
      "Loss: 0.556438148021698\n",
      "Loss: 0.554898738861084\n",
      "Loss: 0.5533542633056641\n",
      "Loss: 0.5518059730529785\n",
      "Loss: 0.5502534508705139\n",
      "Loss: 0.5486965179443359\n",
      "Loss: 0.5471355319023132\n",
      "Loss: 0.5455694794654846\n",
      "Loss: 0.5439914464950562\n",
      "Loss: 0.542409360408783\n",
      "Loss: 0.5408229827880859\n",
      "Loss: 0.5392323136329651\n",
      "Loss: 0.5376375317573547\n",
      "Loss: 0.5360391139984131\n",
      "Loss: 0.5344366431236267\n",
      "Loss: 0.5328302979469299\n",
      "Loss: 0.5312203168869019\n",
      "Loss: 0.5296063423156738\n",
      "Loss: 0.5279898047447205\n",
      "Loss: 0.5263699889183044\n",
      "Loss: 0.5247462391853333\n",
      "Loss: 0.5231190323829651\n",
      "Loss: 0.5214883685112\n",
      "Loss: 0.5198543667793274\n",
      "Loss: 0.5182170867919922\n",
      "Loss: 0.5165764689445496\n",
      "Loss: 0.5149325728416443\n",
      "Loss: 0.5132855176925659\n",
      "Loss: 0.5116356015205383\n",
      "Loss: 0.5099833011627197\n",
      "Loss: 0.5083296298980713\n",
      "Loss: 0.5066731572151184\n",
      "Loss: 0.5050138235092163\n",
      "Loss: 0.5033529996871948\n",
      "Loss: 0.5016902685165405\n",
      "Loss: 0.5000255703926086\n",
      "Loss: 0.49835821986198425\n",
      "Loss: 0.49668851494789124\n",
      "Loss: 0.4950163960456848\n",
      "Loss: 0.4933421313762665\n",
      "Loss: 0.49166637659072876\n",
      "Loss: 0.489990234375\n",
      "Loss: 0.4883129596710205\n",
      "Loss: 0.4866344928741455\n",
      "Loss: 0.4849543571472168\n",
      "Loss: 0.48327213525772095\n",
      "Loss: 0.48158833384513855\n",
      "Loss: 0.47990283370018005\n",
      "Loss: 0.47821658849716187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4765288233757019\n",
      "Loss: 0.4748397469520569\n",
      "Loss: 0.47314929962158203\n",
      "Loss: 0.47145789861679077\n",
      "Loss: 0.4697655141353607\n",
      "Loss: 0.4680721163749695\n",
      "Loss: 0.4663776755332947\n",
      "Loss: 0.46468231081962585\n",
      "Loss: 0.4629863202571869\n",
      "Loss: 0.46128952503204346\n",
      "Loss: 0.4595921039581299\n",
      "Loss: 0.4578940272331238\n",
      "Loss: 0.45619598031044006\n",
      "Loss: 0.454497754573822\n",
      "Loss: 0.4527992010116577\n",
      "Loss: 0.451100617647171\n",
      "Loss: 0.44940367341041565\n",
      "Loss: 0.4477129280567169\n",
      "Loss: 0.4460218548774719\n",
      "Loss: 0.44433143734931946\n",
      "Loss: 0.44264087080955505\n",
      "Loss: 0.4409506320953369\n",
      "Loss: 0.4392610788345337\n",
      "Loss: 0.43757230043411255\n",
      "Loss: 0.43588489294052124\n",
      "Loss: 0.4341980814933777\n",
      "Loss: 0.4325118958950043\n",
      "Loss: 0.43082675337791443\n",
      "Loss: 0.42914241552352905\n",
      "Loss: 0.4274608790874481\n",
      "Loss: 0.4257825016975403\n",
      "Loss: 0.4241052269935608\n",
      "Loss: 0.42242932319641113\n",
      "Loss: 0.42075467109680176\n",
      "Loss: 0.41908127069473267\n",
      "Loss: 0.4174095094203949\n",
      "Loss: 0.4157392084598541\n",
      "Loss: 0.4140705466270447\n",
      "Loss: 0.4124036133289337\n",
      "Loss: 0.4107385575771332\n",
      "Loss: 0.4090752601623535\n",
      "Loss: 0.4074140191078186\n",
      "Loss: 0.40575456619262695\n",
      "Loss: 0.4040974974632263\n",
      "Loss: 0.40244245529174805\n",
      "Loss: 0.40078967809677124\n",
      "Loss: 0.3991389870643616\n",
      "Loss: 0.39749112725257874\n",
      "Loss: 0.3958457112312317\n",
      "Loss: 0.3942025303840637\n",
      "Loss: 0.3925621807575226\n",
      "Loss: 0.390924334526062\n",
      "Loss: 0.3892894387245178\n",
      "Loss: 0.38765740394592285\n",
      "Loss: 0.3860282599925995\n",
      "Loss: 0.38440218567848206\n",
      "Loss: 0.38277965784072876\n",
      "Loss: 0.38116103410720825\n",
      "Loss: 0.37954553961753845\n",
      "Loss: 0.37793341279029846\n",
      "Loss: 0.3763245940208435\n",
      "Loss: 0.37471920251846313\n",
      "Loss: 0.3731173872947693\n",
      "Loss: 0.37151914834976196\n",
      "Loss: 0.3699241280555725\n",
      "Loss: 0.3683333694934845\n",
      "Loss: 0.3667459487915039\n",
      "Loss: 0.3651626706123352\n",
      "Loss: 0.3635830879211426\n",
      "Loss: 0.36200761795043945\n",
      "Loss: 0.3604360520839691\n",
      "Loss: 0.3588687777519226\n",
      "Loss: 0.3573053479194641\n",
      "Loss: 0.355746328830719\n",
      "Loss: 0.35419151186943054\n",
      "Loss: 0.35264086723327637\n",
      "Loss: 0.3510947823524475\n",
      "Loss: 0.34955334663391113\n",
      "Loss: 0.34801626205444336\n",
      "Loss: 0.3464837670326233\n",
      "Loss: 0.34495583176612854\n",
      "Loss: 0.34343254566192627\n",
      "Loss: 0.34191417694091797\n",
      "Loss: 0.3404005169868469\n",
      "Loss: 0.3388916850090027\n",
      "Loss: 0.3373877704143524\n",
      "Loss: 0.33588868379592896\n",
      "Loss: 0.3343948423862457\n",
      "Loss: 0.3329058885574341\n",
      "Loss: 0.33142226934432983\n",
      "Loss: 0.3299436569213867\n",
      "Loss: 0.328470379114151\n",
      "Loss: 0.3269995152950287\n",
      "Loss: 0.3255205452442169\n",
      "Loss: 0.32404664158821106\n",
      "Loss: 0.3225780129432678\n",
      "Loss: 0.3211148977279663\n",
      "Loss: 0.31965726613998413\n",
      "Loss: 0.3182048201560974\n",
      "Loss: 0.3167579472064972\n",
      "Loss: 0.31531664729118347\n",
      "Loss: 0.313880980014801\n",
      "Loss: 0.31245094537734985\n",
      "Loss: 0.3110264837741852\n",
      "Loss: 0.30960771441459656\n",
      "Loss: 0.3081948161125183\n",
      "Loss: 0.3067874312400818\n",
      "Loss: 0.3053858280181885\n",
      "Loss: 0.3039901554584503\n",
      "Loss: 0.3026018738746643\n",
      "Loss: 0.3012235462665558\n",
      "Loss: 0.29985105991363525\n",
      "Loss: 0.2984844744205475\n",
      "Loss: 0.29712390899658203\n",
      "Loss: 0.29576927423477173\n",
      "Loss: 0.29442092776298523\n",
      "Loss: 0.29307839274406433\n",
      "Loss: 0.29174190759658813\n",
      "Loss: 0.29041174054145813\n",
      "Loss: 0.28908759355545044\n",
      "Loss: 0.2877696454524994\n",
      "Loss: 0.28645792603492737\n",
      "Loss: 0.2851523458957672\n",
      "Loss: 0.28385305404663086\n",
      "Loss: 0.28255993127822876\n",
      "Loss: 0.28127315640449524\n",
      "Loss: 0.2799924910068512\n",
      "Loss: 0.2787182629108429\n",
      "Loss: 0.2774503231048584\n",
      "Loss: 0.27618858218193054\n",
      "Loss: 0.27493345737457275\n",
      "Loss: 0.2736843526363373\n",
      "Loss: 0.2724418342113495\n",
      "Loss: 0.2712055444717407\n",
      "Loss: 0.2699756324291229\n",
      "Loss: 0.26875221729278564\n",
      "Loss: 0.2675349712371826\n",
      "Loss: 0.2663242816925049\n",
      "Loss: 0.2651200294494629\n",
      "Loss: 0.2639221251010895\n",
      "Loss: 0.26273053884506226\n",
      "Loss: 0.2615455389022827\n",
      "Loss: 0.2603667378425598\n",
      "Loss: 0.2591944932937622\n",
      "Loss: 0.25802865624427795\n",
      "Loss: 0.256869375705719\n",
      "Loss: 0.2557169497013092\n",
      "Loss: 0.25457102060317993\n",
      "Loss: 0.2534314692020416\n",
      "Loss: 0.2522982656955719\n",
      "Loss: 0.2511714994907379\n",
      "Loss: 0.25005096197128296\n",
      "Loss: 0.24893687665462494\n",
      "Loss: 0.2478291392326355\n",
      "Loss: 0.24672771990299225\n",
      "Loss: 0.245632603764534\n",
      "Loss: 0.24454385042190552\n",
      "Loss: 0.24346140027046204\n",
      "Loss: 0.24238529801368713\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    #model prediction\n",
    "    y_pred = model(X_train)\n",
    "    \n",
    "    #calculate loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    \n",
    "    #Calculate gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #update gradients\n",
    "    optim.step()\n",
    "    \n",
    "    #zero grad\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    print(f'Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sacred-inside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.8055555820465088\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    #y_pred = torch.softmax(model(X_test), dim=0)\n",
    "    y_pred = torch.softmax(model(X_test), dim=0)\n",
    "    \n",
    "    accuracy = y_test.eq(torch.argmax(y_pred, dim=1)).sum() / float(y_test.shape[0])\n",
    "    \n",
    "    print(f'Accuracy:{accuracy}')\n",
    "    #print(torch.argmax(y_pred, dim=1))\n",
    "    #print(y_test.eq(torch.argmax(y_pred, dim=1)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dutch-reproduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g:0.25238659977912903 b:1.2459741830825806\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([0, 1, 2])\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "y_pred_g = torch.tensor([[2.0, 0.4, 0.2], [0.1, 2.4, 0.2], [0.1, 0.4, 2.2]])\n",
    "y_pred_b = torch.tensor([[0.8, 0.1, 2.2], [2.0, 2.4, 2.2], [2.0, 2.4, 2.2]])\n",
    "\n",
    "lg = loss(y_pred_g, y)\n",
    "lb = loss(y_pred_b, y)\n",
    "\n",
    "print(f'g:{lg.item()} b:{lb.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-sitting",
   "metadata": {},
   "source": [
    "### Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-curtis",
   "metadata": {},
   "source": [
    "- NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-shock",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-garlic",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
