{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "traditional-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "authentic-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset = datasets.load_wine()\n",
    "\n",
    "X, Y = wine_dataset.data, wine_dataset.target \n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "#normalize inputs\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "\n",
    "#to tensor\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.int64))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "earlier-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleClassification(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(MultipleClassification, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(n_input, n_hidden)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "julian-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rate = 0.01\n",
    "n_epoch = 500\n",
    "\n",
    "#model\n",
    "model = MultipleClassification(n_features, 3, 3)\n",
    "\n",
    "#criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#omptimizer\n",
    "#optim = torch.optim.SGD(model.parameters(), lr=l_rate)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sporting-feature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([142])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "narrative-pillow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.166039228439331\n",
      "Loss: 1.1333742141723633\n",
      "Loss: 1.1025639772415161\n",
      "Loss: 1.0744627714157104\n",
      "Loss: 1.0475364923477173\n",
      "Loss: 1.0213408470153809\n",
      "Loss: 0.9957389235496521\n",
      "Loss: 0.9712271690368652\n",
      "Loss: 0.9484230279922485\n",
      "Loss: 0.9259997010231018\n",
      "Loss: 0.9037935733795166\n",
      "Loss: 0.8815076351165771\n",
      "Loss: 0.8591243028640747\n",
      "Loss: 0.836688756942749\n",
      "Loss: 0.8143588900566101\n",
      "Loss: 0.791954755783081\n",
      "Loss: 0.7695958018302917\n",
      "Loss: 0.7475085854530334\n",
      "Loss: 0.7256489992141724\n",
      "Loss: 0.7041077017784119\n",
      "Loss: 0.6829541325569153\n",
      "Loss: 0.6621244549751282\n",
      "Loss: 0.6416674852371216\n",
      "Loss: 0.6218101382255554\n",
      "Loss: 0.6025839447975159\n",
      "Loss: 0.5841512084007263\n",
      "Loss: 0.5664086937904358\n",
      "Loss: 0.5493499040603638\n",
      "Loss: 0.5329856276512146\n",
      "Loss: 0.5172358751296997\n",
      "Loss: 0.5021737217903137\n",
      "Loss: 0.48780006170272827\n",
      "Loss: 0.4738319218158722\n",
      "Loss: 0.46046876907348633\n",
      "Loss: 0.44768109917640686\n",
      "Loss: 0.43544355034828186\n",
      "Loss: 0.4237394630908966\n",
      "Loss: 0.41254928708076477\n",
      "Loss: 0.40184906125068665\n",
      "Loss: 0.3916163146495819\n",
      "Loss: 0.38172000646591187\n",
      "Loss: 0.3721148669719696\n",
      "Loss: 0.3628585934638977\n",
      "Loss: 0.3539484441280365\n",
      "Loss: 0.3454429805278778\n",
      "Loss: 0.3372650742530823\n",
      "Loss: 0.32959672808647156\n",
      "Loss: 0.32205814123153687\n",
      "Loss: 0.3146877884864807\n",
      "Loss: 0.3074915111064911\n",
      "Loss: 0.3005222678184509\n",
      "Loss: 0.29374992847442627\n",
      "Loss: 0.2871215045452118\n",
      "Loss: 0.28059664368629456\n",
      "Loss: 0.27416276931762695\n",
      "Loss: 0.2678261697292328\n",
      "Loss: 0.26166486740112305\n",
      "Loss: 0.2556852102279663\n",
      "Loss: 0.24983860552310944\n",
      "Loss: 0.2440892904996872\n",
      "Loss: 0.23829999566078186\n",
      "Loss: 0.23232941329479218\n",
      "Loss: 0.22629226744174957\n",
      "Loss: 0.22033794224262238\n",
      "Loss: 0.2142317295074463\n",
      "Loss: 0.20827066898345947\n",
      "Loss: 0.20241297781467438\n",
      "Loss: 0.1966804713010788\n",
      "Loss: 0.19102835655212402\n",
      "Loss: 0.18545743823051453\n",
      "Loss: 0.18006959557533264\n",
      "Loss: 0.1747826784849167\n",
      "Loss: 0.1693921983242035\n",
      "Loss: 0.16415420174598694\n",
      "Loss: 0.15859973430633545\n",
      "Loss: 0.15290357172489166\n",
      "Loss: 0.14691035449504852\n",
      "Loss: 0.14077094197273254\n",
      "Loss: 0.1348145604133606\n",
      "Loss: 0.12887565791606903\n",
      "Loss: 0.12299231439828873\n",
      "Loss: 0.11663247644901276\n",
      "Loss: 0.11041578650474548\n",
      "Loss: 0.10440229624509811\n",
      "Loss: 0.09867963939905167\n",
      "Loss: 0.09339286386966705\n",
      "Loss: 0.08857030421495438\n",
      "Loss: 0.08411967754364014\n",
      "Loss: 0.08000516891479492\n",
      "Loss: 0.07620933651924133\n",
      "Loss: 0.0727064236998558\n",
      "Loss: 0.06947311758995056\n",
      "Loss: 0.06648164242506027\n",
      "Loss: 0.06371698528528214\n",
      "Loss: 0.061144568026065826\n",
      "Loss: 0.058740124106407166\n",
      "Loss: 0.05648324266076088\n",
      "Loss: 0.05435635522007942\n",
      "Loss: 0.05234500765800476\n",
      "Loss: 0.05043746903538704\n",
      "Loss: 0.048623520880937576\n",
      "Loss: 0.04689538851380348\n",
      "Loss: 0.04524683579802513\n",
      "Loss: 0.04378507286310196\n",
      "Loss: 0.04249504208564758\n",
      "Loss: 0.0412440225481987\n",
      "Loss: 0.04003482684493065\n",
      "Loss: 0.03887023404240608\n",
      "Loss: 0.03775337338447571\n",
      "Loss: 0.03668547421693802\n",
      "Loss: 0.03566742315888405\n",
      "Loss: 0.03469936177134514\n",
      "Loss: 0.033780697733163834\n",
      "Loss: 0.0329100675880909\n",
      "Loss: 0.032103974372148514\n",
      "Loss: 0.03140702098608017\n",
      "Loss: 0.03071434423327446\n",
      "Loss: 0.030020613223314285\n",
      "Loss: 0.029359471052885056\n",
      "Loss: 0.02877555415034294\n",
      "Loss: 0.028213849291205406\n",
      "Loss: 0.027673620730638504\n",
      "Loss: 0.02715410850942135\n",
      "Loss: 0.02665119059383869\n",
      "Loss: 0.026157177984714508\n",
      "Loss: 0.02568160369992256\n",
      "Loss: 0.025227293372154236\n",
      "Loss: 0.02483329363167286\n",
      "Loss: 0.02441774494946003\n",
      "Loss: 0.02399589493870735\n",
      "Loss: 0.023622680455446243\n",
      "Loss: 0.02326137013733387\n",
      "Loss: 0.022911565378308296\n",
      "Loss: 0.022572869434952736\n",
      "Loss: 0.0222448892891407\n",
      "Loss: 0.02192729152739048\n",
      "Loss: 0.02161966823041439\n",
      "Loss: 0.021321704611182213\n",
      "Loss: 0.02103303372859955\n",
      "Loss: 0.02076801471412182\n",
      "Loss: 0.020490778610110283\n",
      "Loss: 0.02023513987660408\n",
      "Loss: 0.019986340776085854\n",
      "Loss: 0.0197441466152668\n",
      "Loss: 0.019508399069309235\n",
      "Loss: 0.01928792893886566\n",
      "Loss: 0.01906333863735199\n",
      "Loss: 0.01885264553129673\n",
      "Loss: 0.018646635115146637\n",
      "Loss: 0.01844516210258007\n",
      "Loss: 0.0182481799274683\n",
      "Loss: 0.018055567517876625\n",
      "Loss: 0.017867248505353928\n",
      "Loss: 0.01768312230706215\n",
      "Loss: 0.017503123730421066\n",
      "Loss: 0.017326490953564644\n",
      "Loss: 0.017172446474432945\n",
      "Loss: 0.01699029840528965\n",
      "Loss: 0.016830457374453545\n",
      "Loss: 0.016673335805535316\n",
      "Loss: 0.016518838703632355\n",
      "Loss: 0.01636699214577675\n",
      "Loss: 0.016217801719903946\n",
      "Loss: 0.01607106253504753\n",
      "Loss: 0.015926958993077278\n",
      "Loss: 0.01581035554409027\n",
      "Loss: 0.015653559938073158\n",
      "Loss: 0.015523344278335571\n",
      "Loss: 0.015394865535199642\n",
      "Loss: 0.015268181450664997\n",
      "Loss: 0.015143295750021935\n",
      "Loss: 0.015020257793366909\n",
      "Loss: 0.014899086207151413\n",
      "Loss: 0.014779799617826939\n",
      "Loss: 0.014662393368780613\n",
      "Loss: 0.01454690471291542\n",
      "Loss: 0.01443952601402998\n",
      "Loss: 0.014327522367238998\n",
      "Loss: 0.014222905971109867\n",
      "Loss: 0.01411943044513464\n",
      "Loss: 0.014017147943377495\n",
      "Loss: 0.01391607616096735\n",
      "Loss: 0.013816256076097488\n",
      "Loss: 0.013717688620090485\n",
      "Loss: 0.013620388694107533\n",
      "Loss: 0.013524385169148445\n",
      "Loss: 0.013429690152406693\n",
      "Loss: 0.013336317613720894\n",
      "Loss: 0.013244261033833027\n",
      "Loss: 0.013153516687452793\n",
      "Loss: 0.013086333870887756\n",
      "Loss: 0.012981095351278782\n",
      "Loss: 0.012898784130811691\n",
      "Loss: 0.012817191891372204\n",
      "Loss: 0.012736334465444088\n",
      "Loss: 0.012656242586672306\n",
      "Loss: 0.012576911598443985\n",
      "Loss: 0.012498400174081326\n",
      "Loss: 0.012420709244906902\n",
      "Loss: 0.012343847192823887\n",
      "Loss: 0.01226783450692892\n",
      "Loss: 0.012192698195576668\n",
      "Loss: 0.012118428945541382\n",
      "Loss: 0.012045041657984257\n",
      "Loss: 0.011972538195550442\n",
      "Loss: 0.011918991804122925\n",
      "Loss: 0.011834664270281792\n",
      "Loss: 0.011768775060772896\n",
      "Loss: 0.011703284457325935\n",
      "Loss: 0.011638211086392403\n",
      "Loss: 0.011573568917810917\n",
      "Loss: 0.011509399861097336\n",
      "Loss: 0.011445699259638786\n",
      "Loss: 0.011382505297660828\n",
      "Loss: 0.011319844052195549\n",
      "Loss: 0.011257719248533249\n",
      "Loss: 0.011196157895028591\n",
      "Loss: 0.011135165579617023\n",
      "Loss: 0.011074759997427464\n",
      "Loss: 0.01101494301110506\n",
      "Loss: 0.01095572393387556\n",
      "Loss: 0.010897113010287285\n",
      "Loss: 0.010839109309017658\n",
      "Loss: 0.01080273650586605\n",
      "Loss: 0.010728862136602402\n",
      "Loss: 0.010676192119717598\n",
      "Loss: 0.01062371488660574\n",
      "Loss: 0.01057144533842802\n",
      "Loss: 0.010519420728087425\n",
      "Loss: 0.010467642918229103\n",
      "Loss: 0.010416142642498016\n",
      "Loss: 0.01036528404802084\n",
      "Loss: 0.010314817540347576\n",
      "Loss: 0.01026469748467207\n",
      "Loss: 0.010214942507445812\n",
      "Loss: 0.010165550746023655\n",
      "Loss: 0.010116550140082836\n",
      "Loss: 0.010068240575492382\n",
      "Loss: 0.010020382702350616\n",
      "Loss: 0.009972965344786644\n",
      "Loss: 0.009925991296768188\n",
      "Loss: 0.009879461489617825\n",
      "Loss: 0.00983335543423891\n",
      "Loss: 0.009796278551220894\n",
      "Loss: 0.009745405055582523\n",
      "Loss: 0.00970323197543621\n",
      "Loss: 0.00966121070086956\n",
      "Loss: 0.009619321674108505\n",
      "Loss: 0.009577577002346516\n",
      "Loss: 0.00953600276261568\n",
      "Loss: 0.009494604542851448\n",
      "Loss: 0.009453385137021542\n",
      "Loss: 0.009412367828190327\n",
      "Loss: 0.009371560998260975\n",
      "Loss: 0.009330972097814083\n",
      "Loss: 0.009290620684623718\n",
      "Loss: 0.009250491857528687\n",
      "Loss: 0.009210596792399883\n",
      "Loss: 0.00917096808552742\n",
      "Loss: 0.009131541475653648\n",
      "Loss: 0.009092266671359539\n",
      "Loss: 0.009053248912096024\n",
      "Loss: 0.009014497511088848\n",
      "Loss: 0.008976018987596035\n",
      "Loss: 0.00893781241029501\n",
      "Loss: 0.008899886161088943\n",
      "Loss: 0.008862231858074665\n",
      "Loss: 0.008824863471090794\n",
      "Loss: 0.008787788450717926\n",
      "Loss: 0.008750979788601398\n",
      "Loss: 0.008730495348572731\n",
      "Loss: 0.008680730126798153\n",
      "Loss: 0.00864702370017767\n",
      "Loss: 0.008613369427621365\n",
      "Loss: 0.008579757995903492\n",
      "Loss: 0.008546200580894947\n",
      "Loss: 0.008512712083756924\n",
      "Loss: 0.00847929809242487\n",
      "Loss: 0.008445973508059978\n",
      "Loss: 0.008412751369178295\n",
      "Loss: 0.008379649370908737\n",
      "Loss: 0.008346649818122387\n",
      "Loss: 0.008313789963722229\n",
      "Loss: 0.008281085640192032\n",
      "Loss: 0.008248521015048027\n",
      "Loss: 0.00821611937135458\n",
      "Loss: 0.008183863945305347\n",
      "Loss: 0.008151774294674397\n",
      "Loss: 0.008119863457977772\n",
      "Loss: 0.008088129572570324\n",
      "Loss: 0.008056577295064926\n",
      "Loss: 0.0080252168700099\n",
      "Loss: 0.007994047366082668\n",
      "Loss: 0.007963070645928383\n",
      "Loss: 0.007932275533676147\n",
      "Loss: 0.007901698350906372\n",
      "Loss: 0.007871300913393497\n",
      "Loss: 0.007841110229492188\n",
      "Loss: 0.007811111398041248\n",
      "Loss: 0.007781303487718105\n",
      "Loss: 0.007761259097605944\n",
      "Loss: 0.007724445313215256\n",
      "Loss: 0.007697185967117548\n",
      "Loss: 0.007669920567423105\n",
      "Loss: 0.007642654236406088\n",
      "Loss: 0.007615398149937391\n",
      "Loss: 0.007588159758597612\n",
      "Loss: 0.007560933008790016\n",
      "Loss: 0.007533760275691748\n",
      "Loss: 0.007506630849093199\n",
      "Loss: 0.007479547522962093\n",
      "Loss: 0.007452534046024084\n",
      "Loss: 0.007425589486956596\n",
      "Loss: 0.007398728746920824\n",
      "Loss: 0.007371950428932905\n",
      "Loss: 0.007345273625105619\n",
      "Loss: 0.007318687159568071\n",
      "Loss: 0.0072922175750136375\n",
      "Loss: 0.007265860680490732\n",
      "Loss: 0.007239621132612228\n",
      "Loss: 0.007213500794023275\n",
      "Loss: 0.007187504321336746\n",
      "Loss: 0.007161649409681559\n",
      "Loss: 0.007135920226573944\n",
      "Loss: 0.00711031211540103\n",
      "Loss: 0.007084846496582031\n",
      "Loss: 0.007059512194246054\n",
      "Loss: 0.007034337613731623\n",
      "Loss: 0.007009276654571295\n",
      "Loss: 0.006984364241361618\n",
      "Loss: 0.006959589663892984\n",
      "Loss: 0.006934951525181532\n",
      "Loss: 0.006910457741469145\n",
      "Loss: 0.006886111106723547\n",
      "Loss: 0.006866502109915018\n",
      "Loss: 0.0068396758288145065\n",
      "Loss: 0.0068174200132489204\n",
      "Loss: 0.006795137654989958\n",
      "Loss: 0.00677283201366663\n",
      "Loss: 0.006750517524778843\n",
      "Loss: 0.006728178821504116\n",
      "Loss: 0.006705844774842262\n",
      "Loss: 0.006683520972728729\n",
      "Loss: 0.006661206018179655\n",
      "Loss: 0.006638907361775637\n",
      "Loss: 0.006616642232984304\n",
      "Loss: 0.006594417616724968\n",
      "Loss: 0.006572226993739605\n",
      "Loss: 0.0065500992350280285\n",
      "Loss: 0.006528017111122608\n",
      "Loss: 0.00650600902736187\n",
      "Loss: 0.006484063807874918\n",
      "Loss: 0.006462194956839085\n",
      "Loss: 0.006440403405576944\n",
      "Loss: 0.0064186942763626575\n",
      "Loss: 0.0063970740884542465\n",
      "Loss: 0.006375543307512999\n",
      "Loss: 0.006354048848152161\n",
      "Loss: 0.0063325525261461735\n",
      "Loss: 0.006311120931059122\n",
      "Loss: 0.0062897768802940845\n",
      "Loss: 0.0062685078009963036\n",
      "Loss: 0.006247346289455891\n",
      "Loss: 0.006226257421076298\n",
      "Loss: 0.006205268204212189\n",
      "Loss: 0.006184381432831287\n",
      "Loss: 0.006163586862385273\n",
      "Loss: 0.006142898462712765\n",
      "Loss: 0.006122302729636431\n",
      "Loss: 0.006101800128817558\n",
      "Loss: 0.006081410218030214\n",
      "Loss: 0.00606111902743578\n",
      "Loss: 0.006056110840290785\n",
      "Loss: 0.006022571120411158\n",
      "Loss: 0.0060041616670787334\n",
      "Loss: 0.005985692609101534\n",
      "Loss: 0.005967174656689167\n",
      "Loss: 0.005948606878519058\n",
      "Loss: 0.005930009298026562\n",
      "Loss: 0.00591139355674386\n",
      "Loss: 0.005892748944461346\n",
      "Loss: 0.0058741020038723946\n",
      "Loss: 0.0058554490096867085\n",
      "Loss: 0.005836804397404194\n",
      "Loss: 0.005818184465169907\n",
      "Loss: 0.005799568723887205\n",
      "Loss: 0.005780989304184914\n",
      "Loss: 0.005762449000030756\n",
      "Loss: 0.00574394129216671\n",
      "Loss: 0.005725479684770107\n",
      "Loss: 0.005707069765776396\n",
      "Loss: 0.005688722711056471\n",
      "Loss: 0.005670415703207254\n",
      "Loss: 0.005652180872857571\n",
      "Loss: 0.005634005181491375\n",
      "Loss: 0.005615902133285999\n",
      "Loss: 0.0055978624150156975\n",
      "Loss: 0.005579897668212652\n",
      "Loss: 0.005562000907957554\n",
      "Loss: 0.005544182378798723\n",
      "Loss: 0.00552642997354269\n",
      "Loss: 0.0055087595246732235\n",
      "Loss: 0.005491156596690416\n",
      "Loss: 0.005473650526255369\n",
      "Loss: 0.005456201732158661\n",
      "Loss: 0.005438840948045254\n",
      "Loss: 0.005421556532382965\n",
      "Loss: 0.005404351279139519\n",
      "Loss: 0.00538722425699234\n",
      "Loss: 0.005370167549699545\n",
      "Loss: 0.005353188142180443\n",
      "Loss: 0.005349922459572554\n",
      "Loss: 0.0053209057077765465\n",
      "Loss: 0.005305483005940914\n",
      "Loss: 0.0052900067530572414\n",
      "Loss: 0.005274484865367413\n",
      "Loss: 0.005258927587419748\n",
      "Loss: 0.005243334919214249\n",
      "Loss: 0.0052277203649282455\n",
      "Loss: 0.005212085787206888\n",
      "Loss: 0.005196446552872658\n",
      "Loss: 0.0051807826384902\n",
      "Loss: 0.005165127571672201\n",
      "Loss: 0.005149479024112225\n",
      "Loss: 0.0051338342018425465\n",
      "Loss: 0.00511822197586298\n",
      "Loss: 0.0051026176661252975\n",
      "Loss: 0.005087044555693865\n",
      "Loss: 0.005071505904197693\n",
      "Loss: 0.005055991932749748\n",
      "Loss: 0.005040526390075684\n",
      "Loss: 0.005025092978030443\n",
      "Loss: 0.005009713117033243\n",
      "Loss: 0.0049943746998906136\n",
      "Loss: 0.004979082848876715\n",
      "Loss: 0.004963841754943132\n",
      "Loss: 0.0049486542120575905\n",
      "Loss: 0.004933521151542664\n",
      "Loss: 0.004918449558317661\n",
      "Loss: 0.004903424996882677\n",
      "Loss: 0.004888459108769894\n",
      "Loss: 0.004873550496995449\n",
      "Loss: 0.004858703818172216\n",
      "Loss: 0.0048439172096550465\n",
      "Loss: 0.004829189740121365\n",
      "Loss: 0.00481451628729701\n",
      "Loss: 0.004799907095730305\n",
      "Loss: 0.004785357974469662\n",
      "Loss: 0.004770871717482805\n",
      "Loss: 0.00475644413381815\n",
      "Loss: 0.004742075689136982\n",
      "Loss: 0.004727763123810291\n",
      "Loss: 0.004713508300483227\n",
      "Loss: 0.004699322395026684\n",
      "Loss: 0.004691082984209061\n",
      "Loss: 0.004672337789088488\n",
      "Loss: 0.004659430589526892\n",
      "Loss: 0.004646483808755875\n",
      "Loss: 0.004633501172065735\n",
      "Loss: 0.004620478488504887\n",
      "Loss: 0.0046074278652668\n",
      "Loss: 0.004594349302351475\n",
      "Loss: 0.004581250250339508\n",
      "Loss: 0.004568140953779221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.004555019084364176\n",
      "Loss: 0.004541896749287844\n",
      "Loss: 0.004528771620243788\n",
      "Loss: 0.004515654407441616\n",
      "Loss: 0.004502554889768362\n",
      "Loss: 0.004489460028707981\n",
      "Loss: 0.004476379137486219\n",
      "Loss: 0.004463321063667536\n",
      "Loss: 0.0044502937234938145\n",
      "Loss: 0.004437289200723171\n",
      "Loss: 0.004424321930855513\n",
      "Loss: 0.004411377478390932\n",
      "Loss: 0.004398471675813198\n",
      "Loss: 0.004385601729154587\n",
      "Loss: 0.0043727741576731205\n",
      "Loss: 0.004359981045126915\n",
      "Loss: 0.004347233567386866\n",
      "Loss: 0.004334529396146536\n",
      "Loss: 0.004321865737438202\n",
      "Loss: 0.004309241659939289\n",
      "Loss: 0.004296666942536831\n",
      "Loss: 0.004284142982214689\n",
      "Loss: 0.004271654412150383\n",
      "Loss: 0.004259225446730852\n",
      "Loss: 0.004246836993843317\n",
      "Loss: 0.004234489519149065\n",
      "Loss: 0.0042222049087285995\n",
      "Loss: 0.004209962673485279\n",
      "Loss: 0.00419776514172554\n",
      "Loss: 0.004185613710433245\n",
      "Loss: 0.004173506051301956\n",
      "Loss: 0.004161452408879995\n",
      "Loss: 0.004149451851844788\n",
      "Loss: 0.004137491341680288\n",
      "Loss: 0.004125583916902542\n",
      "Loss: 0.0041137137450277805\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    #model prediction\n",
    "    y_pred = model(X_train)\n",
    "    \n",
    "    #calculate loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    \n",
    "    #Calculate gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #update gradients\n",
    "    optim.step()\n",
    "    \n",
    "    #zero grad\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    print(f'Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "sacred-inside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24)\n",
      "tensor([1, 1, 2, 1, 0, 1, 2, 2, 0, 2, 1, 1, 0, 1, 1, 2, 2, 0, 1, 0, 0, 2, 1, 1,\n",
      "        2, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    #y_pred = torch.softmax(model(X_test), dim=0)\n",
    "    y_pred = torch.softmax(model(X_test), dim=0)\n",
    "    \n",
    "    accuracy = y_test.eq(torch.argmax(y_pred)).sum() / float(y_test.shape[0])\n",
    "    \n",
    "    #print(f'Accuracy:{accuracy}')\n",
    "    print(torch.argmax(y_pred))\n",
    "    print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dutch-reproduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g:0.25238659977912903 b:1.2459741830825806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([0, 1, 2])\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "y_pred_g = torch.tensor([[2.0, 0.4, 0.2], [0.1, 2.4, 0.2], [0.1, 0.4, 2.2]])\n",
    "y_pred_b = torch.tensor([[0.8, 0.1, 2.2], [2.0, 2.4, 2.2], [2.0, 2.4, 2.2]])\n",
    "\n",
    "lg = loss(y_pred_g, y)\n",
    "lb = loss(y_pred_b, y)\n",
    "\n",
    "print(f'g:{lg.item()} b:{lb.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-sitting",
   "metadata": {},
   "source": [
    "### Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-curtis",
   "metadata": {},
   "source": [
    "- There is a problem when evaluating the model. Softmax function returns a scalar value instead of a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-shock",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-garlic",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
