{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "floating-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chief-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset = datasets.load_wine()\n",
    "\n",
    "X, Y = wine_dataset.data, wine_dataset.target \n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "#normalize inputs\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "\n",
    "#to tensor\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.int64))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "right-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleClassification(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(MultipleClassification, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(n_input, n_hidden)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "laden-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rate = 0.01\n",
    "n_epoch = 600\n",
    "\n",
    "#model\n",
    "model = MultipleClassification(n_features, 3, 3)\n",
    "\n",
    "#criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#omptimizer\n",
    "optim = torch.optim.SGD(model.parameters(), lr=l_rate)\n",
    "#optim = torch.optim.Adam(model.parameters(), lr=l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "arbitrary-civilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0916117429733276\n",
      "Loss: 1.0885515213012695\n",
      "Loss: 1.0855119228363037\n",
      "Loss: 1.0825209617614746\n",
      "Loss: 1.0796340703964233\n",
      "Loss: 1.076765775680542\n",
      "Loss: 1.0739060640335083\n",
      "Loss: 1.0710725784301758\n",
      "Loss: 1.0683457851409912\n",
      "Loss: 1.0656229257583618\n",
      "Loss: 1.0628942251205444\n",
      "Loss: 1.0601820945739746\n",
      "Loss: 1.0574870109558105\n",
      "Loss: 1.0548261404037476\n",
      "Loss: 1.0522574186325073\n",
      "Loss: 1.0497642755508423\n",
      "Loss: 1.0473706722259521\n",
      "Loss: 1.0450186729431152\n",
      "Loss: 1.0426896810531616\n",
      "Loss: 1.040493369102478\n",
      "Loss: 1.038313865661621\n",
      "Loss: 1.0361908674240112\n",
      "Loss: 1.034127950668335\n",
      "Loss: 1.0321921110153198\n",
      "Loss: 1.030281901359558\n",
      "Loss: 1.0283433198928833\n",
      "Loss: 1.026389479637146\n",
      "Loss: 1.0244457721710205\n",
      "Loss: 1.0225087404251099\n",
      "Loss: 1.0206005573272705\n",
      "Loss: 1.0187338590621948\n",
      "Loss: 1.0168577432632446\n",
      "Loss: 1.0149948596954346\n",
      "Loss: 1.013214349746704\n",
      "Loss: 1.011464238166809\n",
      "Loss: 1.0097116231918335\n",
      "Loss: 1.0079656839370728\n",
      "Loss: 1.0062264204025269\n",
      "Loss: 1.004494309425354\n",
      "Loss: 1.002768635749817\n",
      "Loss: 1.0010485649108887\n",
      "Loss: 0.9993355870246887\n",
      "Loss: 0.9976274967193604\n",
      "Loss: 0.9959254860877991\n",
      "Loss: 0.994233250617981\n",
      "Loss: 0.9925597310066223\n",
      "Loss: 0.9908909201622009\n",
      "Loss: 0.9892496466636658\n",
      "Loss: 0.987642228603363\n",
      "Loss: 0.9860441088676453\n",
      "Loss: 0.9844493865966797\n",
      "Loss: 0.9828488230705261\n",
      "Loss: 0.9812358617782593\n",
      "Loss: 0.9796295166015625\n",
      "Loss: 0.9780650734901428\n",
      "Loss: 0.9765164256095886\n",
      "Loss: 0.9749898910522461\n",
      "Loss: 0.9734581112861633\n",
      "Loss: 0.9719033241271973\n",
      "Loss: 0.9703106880187988\n",
      "Loss: 0.9687213897705078\n",
      "Loss: 0.9671350121498108\n",
      "Loss: 0.9655448198318481\n",
      "Loss: 0.9639652967453003\n",
      "Loss: 0.9623903036117554\n",
      "Loss: 0.9608132839202881\n",
      "Loss: 0.9592311978340149\n",
      "Loss: 0.957671046257019\n",
      "Loss: 0.9561061859130859\n",
      "Loss: 0.9545280337333679\n",
      "Loss: 0.9529348015785217\n",
      "Loss: 0.9513459801673889\n",
      "Loss: 0.9497509598731995\n",
      "Loss: 0.9481734037399292\n",
      "Loss: 0.9466128349304199\n",
      "Loss: 0.9450538754463196\n",
      "Loss: 0.9434786438941956\n",
      "Loss: 0.9419010877609253\n",
      "Loss: 0.940326988697052\n",
      "Loss: 0.9387588500976562\n",
      "Loss: 0.9371929168701172\n",
      "Loss: 0.9356301426887512\n",
      "Loss: 0.934068500995636\n",
      "Loss: 0.9325090050697327\n",
      "Loss: 0.9309458136558533\n",
      "Loss: 0.9293627738952637\n",
      "Loss: 0.9277658462524414\n",
      "Loss: 0.9261713027954102\n",
      "Loss: 0.9245877265930176\n",
      "Loss: 0.923001229763031\n",
      "Loss: 0.9214047789573669\n",
      "Loss: 0.9197953939437866\n",
      "Loss: 0.9181910753250122\n",
      "Loss: 0.9165891408920288\n",
      "Loss: 0.9149866104125977\n",
      "Loss: 0.9133850932121277\n",
      "Loss: 0.9117841124534607\n",
      "Loss: 0.910182774066925\n",
      "Loss: 0.9085785746574402\n",
      "Loss: 0.9069743752479553\n",
      "Loss: 0.9053707718849182\n",
      "Loss: 0.9037675261497498\n",
      "Loss: 0.90216064453125\n",
      "Loss: 0.9005306959152222\n",
      "Loss: 0.8989019393920898\n",
      "Loss: 0.8972820043563843\n",
      "Loss: 0.8956732153892517\n",
      "Loss: 0.8940642476081848\n",
      "Loss: 0.8924587965011597\n",
      "Loss: 0.8908321261405945\n",
      "Loss: 0.8892049193382263\n",
      "Loss: 0.8875774145126343\n",
      "Loss: 0.8859493732452393\n",
      "Loss: 0.8843198418617249\n",
      "Loss: 0.882688581943512\n",
      "Loss: 0.8810575008392334\n",
      "Loss: 0.879425048828125\n",
      "Loss: 0.8777851462364197\n",
      "Loss: 0.8761325478553772\n",
      "Loss: 0.8744626045227051\n",
      "Loss: 0.8727623224258423\n",
      "Loss: 0.8710595965385437\n",
      "Loss: 0.8693536520004272\n",
      "Loss: 0.8676370978355408\n",
      "Loss: 0.8659203052520752\n",
      "Loss: 0.8642006516456604\n",
      "Loss: 0.8624783754348755\n",
      "Loss: 0.860751211643219\n",
      "Loss: 0.8590137958526611\n",
      "Loss: 0.8572498559951782\n",
      "Loss: 0.8554821014404297\n",
      "Loss: 0.8537088632583618\n",
      "Loss: 0.8519342541694641\n",
      "Loss: 0.8501578569412231\n",
      "Loss: 0.8483791351318359\n",
      "Loss: 0.8465971350669861\n",
      "Loss: 0.8448179364204407\n",
      "Loss: 0.8430390954017639\n",
      "Loss: 0.8412476778030396\n",
      "Loss: 0.8394387364387512\n",
      "Loss: 0.8376277089118958\n",
      "Loss: 0.8358145356178284\n",
      "Loss: 0.8339982628822327\n",
      "Loss: 0.832179605960846\n",
      "Loss: 0.8303589224815369\n",
      "Loss: 0.828535258769989\n",
      "Loss: 0.8267085552215576\n",
      "Loss: 0.8248798847198486\n",
      "Loss: 0.8230490684509277\n",
      "Loss: 0.8212145566940308\n",
      "Loss: 0.8193768262863159\n",
      "Loss: 0.8175307512283325\n",
      "Loss: 0.8156731128692627\n",
      "Loss: 0.8138130307197571\n",
      "Loss: 0.8119494915008545\n",
      "Loss: 0.8100837469100952\n",
      "Loss: 0.8082159161567688\n",
      "Loss: 0.8063454627990723\n",
      "Loss: 0.8044714331626892\n",
      "Loss: 0.8025949001312256\n",
      "Loss: 0.8007150888442993\n",
      "Loss: 0.7988345623016357\n",
      "Loss: 0.7969529032707214\n",
      "Loss: 0.7950684428215027\n",
      "Loss: 0.7931836247444153\n",
      "Loss: 0.7912989854812622\n",
      "Loss: 0.7894110083580017\n",
      "Loss: 0.7875233888626099\n",
      "Loss: 0.7856327891349792\n",
      "Loss: 0.7837396860122681\n",
      "Loss: 0.7818435430526733\n",
      "Loss: 0.7799451947212219\n",
      "Loss: 0.7780464887619019\n",
      "Loss: 0.7761455178260803\n",
      "Loss: 0.7742413878440857\n",
      "Loss: 0.7723385691642761\n",
      "Loss: 0.7704391479492188\n",
      "Loss: 0.7685368061065674\n",
      "Loss: 0.7666332125663757\n",
      "Loss: 0.7647264003753662\n",
      "Loss: 0.7628209590911865\n",
      "Loss: 0.7609134316444397\n",
      "Loss: 0.7590038180351257\n",
      "Loss: 0.7570924758911133\n",
      "Loss: 0.7551792860031128\n",
      "Loss: 0.7532642483711243\n",
      "Loss: 0.7513495683670044\n",
      "Loss: 0.7494329810142517\n",
      "Loss: 0.7475160360336304\n",
      "Loss: 0.7455963492393494\n",
      "Loss: 0.7436752915382385\n",
      "Loss: 0.7417523264884949\n",
      "Loss: 0.7398288249969482\n",
      "Loss: 0.7379032373428345\n",
      "Loss: 0.7359759211540222\n",
      "Loss: 0.73404860496521\n",
      "Loss: 0.7321215867996216\n",
      "Loss: 0.7301995158195496\n",
      "Loss: 0.7282809615135193\n",
      "Loss: 0.7263643741607666\n",
      "Loss: 0.7244463562965393\n",
      "Loss: 0.7225274443626404\n",
      "Loss: 0.7206082344055176\n",
      "Loss: 0.7186874151229858\n",
      "Loss: 0.7167666554450989\n",
      "Loss: 0.7148452997207642\n",
      "Loss: 0.7129233479499817\n",
      "Loss: 0.711001992225647\n",
      "Loss: 0.7090789675712585\n",
      "Loss: 0.7071557641029358\n",
      "Loss: 0.7052320241928101\n",
      "Loss: 0.7033082842826843\n",
      "Loss: 0.7013842463493347\n",
      "Loss: 0.6994599103927612\n",
      "Loss: 0.6975367665290833\n",
      "Loss: 0.6956135034561157\n",
      "Loss: 0.6936904788017273\n",
      "Loss: 0.6917680501937866\n",
      "Loss: 0.6898462176322937\n",
      "Loss: 0.687926709651947\n",
      "Loss: 0.6860111951828003\n",
      "Loss: 0.6840960383415222\n",
      "Loss: 0.6821820139884949\n",
      "Loss: 0.6802690029144287\n",
      "Loss: 0.6783574819564819\n",
      "Loss: 0.6764475107192993\n",
      "Loss: 0.6745420098304749\n",
      "Loss: 0.6726365685462952\n",
      "Loss: 0.6707326173782349\n",
      "Loss: 0.6688309907913208\n",
      "Loss: 0.6669304966926575\n",
      "Loss: 0.665031909942627\n",
      "Loss: 0.6631348133087158\n",
      "Loss: 0.6612398624420166\n",
      "Loss: 0.6593459844589233\n",
      "Loss: 0.6574550271034241\n",
      "Loss: 0.655567467212677\n",
      "Loss: 0.6536858677864075\n",
      "Loss: 0.6518062353134155\n",
      "Loss: 0.6499330401420593\n",
      "Loss: 0.6480613946914673\n",
      "Loss: 0.6461983919143677\n",
      "Loss: 0.64433753490448\n",
      "Loss: 0.6424794793128967\n",
      "Loss: 0.6406241655349731\n",
      "Loss: 0.6387715339660645\n",
      "Loss: 0.6369225382804871\n",
      "Loss: 0.6350764632225037\n",
      "Loss: 0.6332330107688904\n",
      "Loss: 0.6313931941986084\n",
      "Loss: 0.6295563578605652\n",
      "Loss: 0.6277230978012085\n",
      "Loss: 0.6258866190910339\n",
      "Loss: 0.6240373849868774\n",
      "Loss: 0.62217777967453\n",
      "Loss: 0.6203216910362244\n",
      "Loss: 0.6184689402580261\n",
      "Loss: 0.6166206002235413\n",
      "Loss: 0.6147753000259399\n",
      "Loss: 0.6129341721534729\n",
      "Loss: 0.6110967993736267\n",
      "Loss: 0.6092634797096252\n",
      "Loss: 0.6074349880218506\n",
      "Loss: 0.6056095957756042\n",
      "Loss: 0.6037890911102295\n",
      "Loss: 0.6019731163978577\n",
      "Loss: 0.6001608371734619\n",
      "Loss: 0.598353385925293\n",
      "Loss: 0.5965501070022583\n",
      "Loss: 0.5947521328926086\n",
      "Loss: 0.5929582715034485\n",
      "Loss: 0.5911726355552673\n",
      "Loss: 0.5894017815589905\n",
      "Loss: 0.5876362919807434\n",
      "Loss: 0.5858750939369202\n",
      "Loss: 0.5841194987297058\n",
      "Loss: 0.5823710560798645\n",
      "Loss: 0.5806308388710022\n",
      "Loss: 0.5788955688476562\n",
      "Loss: 0.5771653056144714\n",
      "Loss: 0.5754415988922119\n",
      "Loss: 0.5737283825874329\n",
      "Loss: 0.5720198154449463\n",
      "Loss: 0.5703181624412537\n",
      "Loss: 0.56862473487854\n",
      "Loss: 0.5669369101524353\n",
      "Loss: 0.5652547478675842\n",
      "Loss: 0.5635782480239868\n",
      "Loss: 0.561907172203064\n",
      "Loss: 0.5602418184280396\n",
      "Loss: 0.5585842132568359\n",
      "Loss: 0.5569359660148621\n",
      "Loss: 0.5552927255630493\n",
      "Loss: 0.5536554455757141\n",
      "Loss: 0.5520243644714355\n",
      "Loss: 0.5503990650177002\n",
      "Loss: 0.5487796664237976\n",
      "Loss: 0.5471663475036621\n",
      "Loss: 0.5455586910247803\n",
      "Loss: 0.5439572930335999\n",
      "Loss: 0.5423614978790283\n",
      "Loss: 0.5407724976539612\n",
      "Loss: 0.5391891002655029\n",
      "Loss: 0.5376117825508118\n",
      "Loss: 0.5360404253005981\n",
      "Loss: 0.5344756245613098\n",
      "Loss: 0.532918393611908\n",
      "Loss: 0.5313698053359985\n",
      "Loss: 0.529826819896698\n",
      "Loss: 0.5282902121543884\n",
      "Loss: 0.526760458946228\n",
      "Loss: 0.5252378582954407\n",
      "Loss: 0.5237216949462891\n",
      "Loss: 0.5222113132476807\n",
      "Loss: 0.5207076072692871\n",
      "Loss: 0.5192103981971741\n",
      "Loss: 0.5177189707756042\n",
      "Loss: 0.5162339806556702\n",
      "Loss: 0.5147555470466614\n",
      "Loss: 0.513283371925354\n",
      "Loss: 0.511817216873169\n",
      "Loss: 0.5103577971458435\n",
      "Loss: 0.508904755115509\n",
      "Loss: 0.507457435131073\n",
      "Loss: 0.5060186386108398\n",
      "Loss: 0.50458824634552\n",
      "Loss: 0.5031648874282837\n",
      "Loss: 0.5017474293708801\n",
      "Loss: 0.5003359913825989\n",
      "Loss: 0.4989314079284668\n",
      "Loss: 0.49753308296203613\n",
      "Loss: 0.49614080786705017\n",
      "Loss: 0.4947550594806671\n",
      "Loss: 0.493375688791275\n",
      "Loss: 0.49200206995010376\n",
      "Loss: 0.4906352460384369\n",
      "Loss: 0.48927468061447144\n",
      "Loss: 0.487920343875885\n",
      "Loss: 0.4865719974040985\n",
      "Loss: 0.48523104190826416\n",
      "Loss: 0.4838990867137909\n",
      "Loss: 0.48257333040237427\n",
      "Loss: 0.4812537729740143\n",
      "Loss: 0.47994011640548706\n",
      "Loss: 0.4786328673362732\n",
      "Loss: 0.47733166813850403\n",
      "Loss: 0.4760368764400482\n",
      "Loss: 0.47474798560142517\n",
      "Loss: 0.4734645485877991\n",
      "Loss: 0.47218790650367737\n",
      "Loss: 0.47091713547706604\n",
      "Loss: 0.4696524739265442\n",
      "Loss: 0.4683939516544342\n",
      "Loss: 0.4671408534049988\n",
      "Loss: 0.46589428186416626\n",
      "Loss: 0.4646536707878113\n",
      "Loss: 0.46341851353645325\n",
      "Loss: 0.4621897339820862\n",
      "Loss: 0.46096694469451904\n",
      "Loss: 0.4597495198249817\n",
      "Loss: 0.4585385322570801\n",
      "Loss: 0.4573327600955963\n",
      "Loss: 0.4561336040496826\n",
      "Loss: 0.45493945479393005\n",
      "Loss: 0.4537516236305237\n",
      "Loss: 0.45256906747817993\n",
      "Loss: 0.4513927400112152\n",
      "Loss: 0.45022159814834595\n",
      "Loss: 0.4490566849708557\n",
      "Loss: 0.4478970468044281\n",
      "Loss: 0.446743369102478\n",
      "Loss: 0.44559478759765625\n",
      "Loss: 0.4444524347782135\n",
      "Loss: 0.4433150887489319\n",
      "Loss: 0.44218358397483826\n",
      "Loss: 0.44105762243270874\n",
      "Loss: 0.4399367570877075\n",
      "Loss: 0.43882185220718384\n",
      "Loss: 0.4377118945121765\n",
      "Loss: 0.43660762906074524\n",
      "Loss: 0.43550893664360046\n",
      "Loss: 0.434415340423584\n",
      "Loss: 0.43332722783088684\n",
      "Loss: 0.43224388360977173\n",
      "Loss: 0.431166410446167\n",
      "Loss: 0.43009647727012634\n",
      "Loss: 0.42903491854667664\n",
      "Loss: 0.4279785752296448\n",
      "Loss: 0.4269275367259979\n",
      "Loss: 0.4258819818496704\n",
      "Loss: 0.4248412549495697\n",
      "Loss: 0.4238054156303406\n",
      "Loss: 0.4227747619152069\n",
      "Loss: 0.4217495024204254\n",
      "Loss: 0.42072901129722595\n",
      "Loss: 0.4197136461734772\n",
      "Loss: 0.4187028408050537\n",
      "Loss: 0.4176974296569824\n",
      "Loss: 0.4166966676712036\n",
      "Loss: 0.4157009720802307\n",
      "Loss: 0.414710134267807\n",
      "Loss: 0.413723886013031\n",
      "Loss: 0.41274258494377136\n",
      "Loss: 0.41176605224609375\n",
      "Loss: 0.4107942581176758\n",
      "Loss: 0.4098266661167145\n",
      "Loss: 0.4088636040687561\n",
      "Loss: 0.40790557861328125\n",
      "Loss: 0.4069518446922302\n",
      "Loss: 0.40600302815437317\n",
      "Loss: 0.405058354139328\n",
      "Loss: 0.40411853790283203\n",
      "Loss: 0.4031834304332733\n",
      "Loss: 0.4022523760795593\n",
      "Loss: 0.40132609009742737\n",
      "Loss: 0.4004042148590088\n",
      "Loss: 0.3994864821434021\n",
      "Loss: 0.3985733091831207\n",
      "Loss: 0.3976646363735199\n",
      "Loss: 0.39676034450531006\n",
      "Loss: 0.3958607316017151\n",
      "Loss: 0.3949654698371887\n",
      "Loss: 0.3940741717815399\n",
      "Loss: 0.39318710565567017\n",
      "Loss: 0.3923041522502899\n",
      "Loss: 0.39142540097236633\n",
      "Loss: 0.39055076241493225\n",
      "Loss: 0.3896803855895996\n",
      "Loss: 0.3888137936592102\n",
      "Loss: 0.3879530131816864\n",
      "Loss: 0.38709619641304016\n",
      "Loss: 0.3862433433532715\n",
      "Loss: 0.3853958249092102\n",
      "Loss: 0.38455191254615784\n",
      "Loss: 0.38371214270591736\n",
      "Loss: 0.3828757107257843\n",
      "Loss: 0.3820439875125885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3812156617641449\n",
      "Loss: 0.3803931772708893\n",
      "Loss: 0.37957465648651123\n",
      "Loss: 0.37875956296920776\n",
      "Loss: 0.3779486119747162\n",
      "Loss: 0.3771407902240753\n",
      "Loss: 0.3763371706008911\n",
      "Loss: 0.3755369186401367\n",
      "Loss: 0.3747404217720032\n",
      "Loss: 0.3739473521709442\n",
      "Loss: 0.37315788865089417\n",
      "Loss: 0.3723720610141754\n",
      "Loss: 0.37158945202827454\n",
      "Loss: 0.37081050872802734\n",
      "Loss: 0.37003520131111145\n",
      "Loss: 0.36926329135894775\n",
      "Loss: 0.3684948980808258\n",
      "Loss: 0.3677296042442322\n",
      "Loss: 0.3669681251049042\n",
      "Loss: 0.3662109673023224\n",
      "Loss: 0.36545702815055847\n",
      "Loss: 0.3647063970565796\n",
      "Loss: 0.3639589846134186\n",
      "Loss: 0.3632146120071411\n",
      "Loss: 0.36247387528419495\n",
      "Loss: 0.3617362082004547\n",
      "Loss: 0.3610017001628876\n",
      "Loss: 0.36027011275291443\n",
      "Loss: 0.35954099893569946\n",
      "Loss: 0.35881465673446655\n",
      "Loss: 0.3580915629863739\n",
      "Loss: 0.3573714792728424\n",
      "Loss: 0.35665467381477356\n",
      "Loss: 0.3559420108795166\n",
      "Loss: 0.3552328050136566\n",
      "Loss: 0.354526162147522\n",
      "Loss: 0.3538227379322052\n",
      "Loss: 0.35312211513519287\n",
      "Loss: 0.3524245023727417\n",
      "Loss: 0.351730078458786\n",
      "Loss: 0.3510381281375885\n",
      "Loss: 0.3503493070602417\n",
      "Loss: 0.34966322779655457\n",
      "Loss: 0.3489799499511719\n",
      "Loss: 0.3482998013496399\n",
      "Loss: 0.3476220369338989\n",
      "Loss: 0.34694740176200867\n",
      "Loss: 0.3462754786014557\n",
      "Loss: 0.3456060290336609\n",
      "Loss: 0.3449394702911377\n",
      "Loss: 0.34427574276924133\n",
      "Loss: 0.3436146676540375\n",
      "Loss: 0.3429561257362366\n",
      "Loss: 0.3423003554344177\n",
      "Loss: 0.34164702892303467\n",
      "Loss: 0.34099677205085754\n",
      "Loss: 0.3403488099575043\n",
      "Loss: 0.3397035598754883\n",
      "Loss: 0.3390607237815857\n",
      "Loss: 0.3384203314781189\n",
      "Loss: 0.33778274059295654\n",
      "Loss: 0.3371475636959076\n",
      "Loss: 0.33651497960090637\n",
      "Loss: 0.3358847200870514\n",
      "Loss: 0.3352568447589874\n",
      "Loss: 0.33463168144226074\n",
      "Loss: 0.33400896191596985\n",
      "Loss: 0.3333885967731476\n",
      "Loss: 0.33277058601379395\n",
      "Loss: 0.3321550488471985\n",
      "Loss: 0.3315419852733612\n",
      "Loss: 0.3309311270713806\n",
      "Loss: 0.3303225636482239\n",
      "Loss: 0.3297163248062134\n",
      "Loss: 0.3291127383708954\n",
      "Loss: 0.32851117849349976\n",
      "Loss: 0.3279118537902832\n",
      "Loss: 0.3273148238658905\n",
      "Loss: 0.3267199397087097\n",
      "Loss: 0.3261273503303528\n",
      "Loss: 0.32553720474243164\n",
      "Loss: 0.32494914531707764\n",
      "Loss: 0.32436344027519226\n",
      "Loss: 0.3237795829772949\n",
      "Loss: 0.32319802045822144\n",
      "Loss: 0.3226188123226166\n",
      "Loss: 0.322041779756546\n",
      "Loss: 0.3214665949344635\n",
      "Loss: 0.320893794298172\n",
      "Loss: 0.32032302021980286\n",
      "Loss: 0.3197533190250397\n",
      "Loss: 0.31918469071388245\n",
      "Loss: 0.3186176121234894\n",
      "Loss: 0.31805285811424255\n",
      "Loss: 0.31749001145362854\n",
      "Loss: 0.31692957878112793\n",
      "Loss: 0.3163708448410034\n",
      "Loss: 0.3158143162727356\n",
      "Loss: 0.3152596056461334\n",
      "Loss: 0.31470686197280884\n",
      "Loss: 0.31415635347366333\n",
      "Loss: 0.31360751390457153\n",
      "Loss: 0.3130609095096588\n",
      "Loss: 0.31251636147499084\n",
      "Loss: 0.3119734525680542\n",
      "Loss: 0.3114326000213623\n",
      "Loss: 0.310893714427948\n",
      "Loss: 0.3103564977645874\n",
      "Loss: 0.30982115864753723\n",
      "Loss: 0.3092879354953766\n",
      "Loss: 0.3087563216686249\n",
      "Loss: 0.3082268536090851\n",
      "Loss: 0.3076992928981781\n",
      "Loss: 0.3071731925010681\n",
      "Loss: 0.30664902925491333\n",
      "Loss: 0.30612683296203613\n",
      "Loss: 0.3056064248085022\n",
      "Loss: 0.3050878643989563\n",
      "Loss: 0.30457088351249695\n",
      "Loss: 0.30405566096305847\n",
      "Loss: 0.3035424053668976\n",
      "Loss: 0.30303066968917847\n",
      "Loss: 0.3025209605693817\n",
      "Loss: 0.30201274156570435\n",
      "Loss: 0.3015064299106598\n",
      "Loss: 0.3010014593601227\n",
      "Loss: 0.3004985749721527\n",
      "Loss: 0.29999735951423645\n",
      "Loss: 0.2994977533817291\n",
      "Loss: 0.2989996373653412\n",
      "Loss: 0.29850345849990845\n",
      "Loss: 0.2980085015296936\n",
      "Loss: 0.29751548171043396\n",
      "Loss: 0.2970244586467743\n",
      "Loss: 0.29653486609458923\n",
      "Loss: 0.2960469722747803\n",
      "Loss: 0.2955606281757355\n",
      "Loss: 0.2950758934020996\n",
      "Loss: 0.29459285736083984\n",
      "Loss: 0.2941112518310547\n",
      "Loss: 0.29363128542900085\n",
      "Loss: 0.2931528687477112\n",
      "Loss: 0.29267576336860657\n",
      "Loss: 0.2922006845474243\n",
      "Loss: 0.2917269170284271\n",
      "Loss: 0.2912544906139374\n",
      "Loss: 0.2907838523387909\n",
      "Loss: 0.2903144061565399\n",
      "Loss: 0.28984665870666504\n",
      "Loss: 0.28938043117523193\n",
      "Loss: 0.2889156937599182\n",
      "Loss: 0.2884523570537567\n",
      "Loss: 0.2879904508590698\n",
      "Loss: 0.28753000497817993\n",
      "Loss: 0.2870713174343109\n",
      "Loss: 0.2866136133670807\n",
      "Loss: 0.28615760803222656\n",
      "Loss: 0.28570300340652466\n",
      "Loss: 0.2852497696876526\n",
      "Loss: 0.2847978174686432\n",
      "Loss: 0.28434741497039795\n",
      "Loss: 0.28389859199523926\n",
      "Loss: 0.28345099091529846\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    #model prediction\n",
    "    y_pred = model(X_train)\n",
    "    \n",
    "    #calculate loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    \n",
    "    #Calculate gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #update gradients\n",
    "    optim.step()\n",
    "    \n",
    "    #zero grad\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    print(f'Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mechanical-neutral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.8055555820465088\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    #y_pred = torch.softmax(model(X_test), dim=0)\n",
    "    y_pred = torch.softmax(model(X_test), dim=0)\n",
    "    \n",
    "    accuracy = y_test.eq(torch.argmax(y_pred, dim=1)).sum() / float(y_test.shape[0])\n",
    "    \n",
    "    print(f'Accuracy:{accuracy}')\n",
    "    #print(torch.argmax(y_pred, dim=1))\n",
    "    #print(y_test.eq(torch.argmax(y_pred, dim=1)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "objective-islam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g:0.25238659977912903 b:1.2459741830825806\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([0, 1, 2])\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "y_pred_g = torch.tensor([[2.0, 0.4, 0.2], [0.1, 2.4, 0.2], [0.1, 0.4, 2.2]])\n",
    "y_pred_b = torch.tensor([[0.8, 0.1, 2.2], [2.0, 2.4, 2.2], [2.0, 2.4, 2.2]])\n",
    "\n",
    "lg = loss(y_pred_g, y)\n",
    "lb = loss(y_pred_b, y)\n",
    "\n",
    "print(f'g:{lg.item()} b:{lb.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-tension",
   "metadata": {},
   "source": [
    "### Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-plate",
   "metadata": {},
   "source": [
    "- NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-melbourne",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-tours",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
