{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "sought-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "#from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "statutory-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dAuto(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.padding =  (self.kernel_size[0] // 2, self.kernel_size[1] // 2) # dynamic add padding based on the kernel_size\n",
    " \n",
    "#partial function, sets parameters, kernel_size and bias by default. When used conv3x3 those parameters will be ued\n",
    "conv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "valuable-following",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2dAuto(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
     ]
    }
   ],
   "source": [
    "#in this example, even that kernel_size and bias are not used, those values are taken from the definition in partial function\n",
    "conv = conv3x3(in_channels=32, out_channels=64)\n",
    "print(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "equivalent-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_func(activation):\n",
    "    return  nn.ModuleDict([\n",
    "        ['relu', nn.ReLU(inplace=True)],\n",
    "        ['leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True)],\n",
    "        ['selu', nn.SELU(inplace=True)],\n",
    "        ['none', nn.Identity()]\n",
    "    ])[activation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "immune-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is intented to be extended by other class\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.in_channels, self.out_channels, self.activation = in_channels, out_channels, activation\n",
    "        self.blocks = nn.Identity()\n",
    "        self.activate = activation_func(activation)\n",
    "        self.shortcut = nn.Identity()   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.should_apply_shortcut: residual = self.shortcut(x)\n",
    "        x = self.blocks(x)\n",
    "        x += residual\n",
    "        x = self.activate(x)\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "continuing-eagle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Just test Residualblock, the output should be 2\n",
    "dummy = torch.ones(2,2)\n",
    "block = ResidualBlock(64, 64)\n",
    "block(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "musical-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shortcut applies a convolution of 1 kernel to transform in_channels to out_channels\n",
    "\n",
    "class ResNetResidualBlock(ResidualBlock):\n",
    "    def __init__(self, in_channels, out_channels, expansion=1, downsampling=1, conv=conv3x3, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.expansion, self.downsampling, self.conv = expansion, downsampling, conv\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, self.expanded_channels, kernel_size=1,\n",
    "                      stride=self.downsampling, bias=False),\n",
    "            nn.BatchNorm2d(self.expanded_channels)) if self.should_apply_shortcut else None\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def expanded_channels(self):\n",
    "        return self.out_channels * self.expansion\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.expanded_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aggressive-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(in_channels, out_channels, conv, *args, **kwargs):\n",
    "    return nn.Sequential(conv(in_channels, out_channels, *args, **kwargs), nn.BatchNorm2d(out_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "chronic-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBasicBlock(ResNetResidualBlock):\n",
    "    \"\"\"\n",
    "    Basic ResNet block composed by two layers of 3x3conv/batchnorm/activation\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "            conv_bn(self.in_channels, self.out_channels, conv=self.conv, bias=False, stride=self.downsampling),\n",
    "            activation_func(self.activation),\n",
    "            conv_bn(self.out_channels, self.expanded_channels, conv=self.conv, bias=False),\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "labeled-latin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNetBasicBlock(\n",
      "  (blocks): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2dAuto(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Sequential(\n",
      "      (0): Conv2dAuto(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (activate): ReLU(inplace=True)\n",
      "  (shortcut): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.ones((1, 32, 224, 224))\n",
    "\n",
    "block = ResNetBasicBlock(32, 64)\n",
    "block(dummy).shape\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "invisible-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBottleNeckBlock(ResNetResidualBlock):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, expansion=4, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "           conv_bn(self.in_channels, self.out_channels, self.conv, kernel_size=1),\n",
    "             activation_func(self.activation),\n",
    "             conv_bn(self.out_channels, self.out_channels, self.conv, kernel_size=3, stride=self.downsampling),\n",
    "             activation_func(self.activation),\n",
    "             conv_bn(self.out_channels, self.expanded_channels, self.conv, kernel_size=1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "packed-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example below uses list comprehension\n",
    "\n",
    "class ResNetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet layer composed by `n` blocks stacked one after the other\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, block=ResNetBasicBlock, n=1, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        # 'We perform downsampling directly by convolutional layers that have a stride of 2.'\n",
    "        downsampling = 2 if in_channels != out_channels else 1\n",
    "        self.blocks = nn.Sequential(\n",
    "            block(in_channels , out_channels, *args, **kwargs, downsampling=downsampling),\n",
    "            #This is a list comprehension pased to the Sequential function using * argument\n",
    "            *[block(out_channels * block.expansion, \n",
    "                    out_channels, downsampling=1, *args, **kwargs) for _ in range(n - 1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "meaningful-wales",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 24, 24])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = torch.ones((1, 64, 48, 48))\n",
    "\n",
    "layer = ResNetLayer(64, 128, block=ResNetBasicBlock, n=3)\n",
    "layer(dummy).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "crazy-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet encoder composed by layers with increasing features.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, blocks_sizes=[64, 128, 256, 512], deepths=[2,2,2,2], \n",
    "                 activation='relu', block=ResNetBasicBlock, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.blocks_sizes = blocks_sizes\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, self.blocks_sizes[0], kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(self.blocks_sizes[0]),\n",
    "            activation_func(activation),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        #zip, creates a tuple conbining both list, if one list is smaller result will take its lenght\n",
    "        self.in_out_block_sizes = list(zip(blocks_sizes, blocks_sizes[1:]))\n",
    "        self.blocks = nn.ModuleList([ \n",
    "            ResNetLayer(blocks_sizes[0], blocks_sizes[0], n=deepths[0], activation=activation, \n",
    "                        block=block,*args, **kwargs),\n",
    "            *[ResNetLayer(in_channels * block.expansion, \n",
    "                          out_channels, n=n, activation=activation, \n",
    "                          block=block, *args, **kwargs) \n",
    "              for (in_channels, out_channels), n in zip(self.in_out_block_sizes, deepths[1:])]       \n",
    "        ])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.gate(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "second-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This class represents the tail of ResNet. It performs a global pooling and maps the output to the\n",
    "    correct class by using a fully connected layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.decoder = nn.Linear(in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cross-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, n_classes, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNetEncoder(in_channels, *args, **kwargs)\n",
    "        self.decoder = ResnetDecoder(self.encoder.blocks[-1].blocks[-1].expanded_channels, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "metallic-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(in_channels, n_classes, block=ResNetBasicBlock, *args, **kwargs):\n",
    "    return ResNet(in_channels, n_classes, block=block, deepths=[2, 2, 2, 2], *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-ontario",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fitted-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "n_epochs = 2\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.001\n",
    "momentum = 0.5\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "peripheral-catalog",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = 'data/'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(data_dir, train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(data_dir, train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                                 ])),\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "    \n",
    "type(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "earned-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resnet model\n",
    "model = resnet18(1, 10)\n",
    "\n",
    "#Stochastic gradient decent\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "#Criterion\n",
    "loss_f = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "improved-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "bottom-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  #\n",
    "  model.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    \n",
    "    #set parameters gradients to 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #runs model with data and returns result in output\n",
    "    #here data contains a 64 * 1 * 28 * 28 tensor, 64 is batch size\n",
    "    output = model(data)\n",
    "    \n",
    "    loss = loss_f(output, target)\n",
    "    \n",
    "    #with this gradients are calculated\n",
    "    loss.backward()\n",
    "    \n",
    "    #update gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    #Display iteration statistics\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "      #torch.save(network.state_dict(), 'results/model.pth')\n",
    "      #torch.save(optimizer.state_dict(), 'results/optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "impressive-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  #Sets the module in evaluation mode\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "    \n",
    "  #dont update dynamic computation graph\n",
    "  with torch.no_grad():\n",
    "    #for every example in test\n",
    "    for data, target in test_loader:\n",
    "      #evaluate the model\n",
    "      output = model(data)\n",
    "      #acumulate the loss\n",
    "      test_loss += loss_f(output, target).item()\n",
    "      \n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "challenging-twenty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 995/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.552624\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.394929\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.778780\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.572034\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.453360\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.349396\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.405889\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.341048\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.264438\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.167402\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.158046\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.363899\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.282624\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.326158\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.324889\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.158284\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.245698\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.174442\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.205799\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.123801\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.228022\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.119928\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.224882\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.077035\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.129787\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.043625\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.139106\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.203882\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.154450\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.154915\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.198514\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.206274\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.116320\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.092613\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.138605\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.251988\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.227695\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.142028\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.135041\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.210937\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.108103\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.397070\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.116341\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.091869\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.151741\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.111501\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.029322\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.081134\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.206333\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.124868\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.157194\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.149102\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.122202\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.064312\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.047202\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.067801\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.113145\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.051339\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.061483\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.093086\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.068455\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.081026\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.086256\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.096903\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.096290\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.052577\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.151958\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.032382\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.127054\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.075389\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.097956\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.131387\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.175138\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.078106\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.043970\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.033513\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.058668\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.065716\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.093475\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.052746\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.027481\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.115681\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.086440\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.121980\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.084314\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.080936\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.056563\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.032231\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.359932\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.103407\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.105877\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.090839\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.036547\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.142495\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 9800/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.034549\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.038500\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.017195\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.039715\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.178849\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.090069\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.011259\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.074556\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.025071\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.036281\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.038555\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.052959\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.025968\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.018360\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.019255\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.030848\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.015996\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.039573\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.084723\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.050409\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.068388\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.028311\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.023870\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.044067\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.021119\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.047703\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.030844\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.010892\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.176567\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.093848\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.027583\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.020400\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.132321\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.101762\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.042573\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.068633\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.040744\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.048125\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.023389\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.071592\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.047033\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.022280\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.038402\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.019589\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.031798\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.132085\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.013126\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.065380\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.047772\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.043371\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.026724\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.148021\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.010915\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.070173\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.097600\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.113980\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.013091\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.016335\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.040956\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.051277\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.044768\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.113391\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.080228\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.095982\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.012868\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.071504\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.048961\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.053344\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.100870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.009476\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.016222\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.147067\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.067925\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.055920\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.021804\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.045934\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.015307\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.021421\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.049243\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.027736\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.144448\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.049044\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.046801\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.140901\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.025433\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.009941\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.096446\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.143761\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.064473\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.041295\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.107573\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.007886\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.030125\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.022741\n",
      "\n",
      "Test set: Avg. loss: 0.0000, Accuracy: 9849/10000 (98%)\n",
      "\n",
      "Time elapsed: 1026.8306 seconds\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "#n_epochs = 1\n",
    "#Check accuaracy before training\n",
    "test()\n",
    "tic = time.perf_counter()\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test()\n",
    "    \n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f\"Time elapsed: {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-ontario",
   "metadata": {},
   "source": [
    "###  Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-routine",
   "metadata": {},
   "source": [
    "* https://towardsdatascience.com/residual-network-implementing-resnet-a7da63c7b278"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-michael",
   "metadata": {},
   "source": [
    "### ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-letters",
   "metadata": {},
   "source": [
    "*  for (in_channels, out_channels), n in zip(self.in_out_block_sizes, deepths[1:])]. What is zip?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
