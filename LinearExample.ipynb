{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "further-sponsorship",
   "metadata": {},
   "source": [
    "# Linear regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-citation",
   "metadata": {},
   "source": [
    "Data was taken from: https://www.kaggle.com/mirichoi0218/insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medical-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "subjective-break",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get data from csv file\n",
    "data = pd.read_csv(\"LinearExample/insurance.csv\") \n",
    "# Preview the first 5 lines of the loaded data \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-currency",
   "metadata": {},
   "source": [
    "### Check bmi distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "juvenile-strip",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataNp = data.to_numpy()\n",
    "bmi = dataNp[:,2]\n",
    "bmi = bmi.astype('float64')\n",
    "\n",
    "plt.hist(bmi, bins=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-relaxation",
   "metadata": {},
   "source": [
    "### Check charges distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "excess-novelty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataNp = data.to_numpy()\n",
    "\n",
    "age = dataNp[:, 0]\n",
    "age = age.astype('float64')\n",
    "sex = dataNp[:, 1]\n",
    "children = dataNp[:, 3]\n",
    "charges = dataNp[:, 6]\n",
    "charges = charges.astype('float64')\n",
    "\n",
    "\n",
    "plt.hist(charges, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-windows",
   "metadata": {},
   "source": [
    "### Checking relationship between age and charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bored-benchmark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(age, charges, 'x');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-solution",
   "metadata": {},
   "source": [
    "Checking plot, seems that there is a linear relation between these variables. It's very spreaded though.\n",
    "\n",
    "Let's try to find a linear equation that fits this data just for fun.\n",
    "\n",
    "With this visualization, 3 clusters are visible.\n",
    "How do they relate with other variables?\n",
    "How can I find the relation between a cluster and another variable(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-nothing",
   "metadata": {},
   "source": [
    "### Train the model with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "monthly-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "earlier-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get normalize values before spliting sets\n",
    "ageNorm = np.linalg.norm(age)\n",
    "\n",
    "#divide train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(age, charges, train_size=0.65,test_size=0.35)\n",
    "\n",
    "#Get data sample sizes\n",
    "trainDataSize = X_train.size\n",
    "testDataSize = y_test.size\n",
    "\n",
    "#normalize data\n",
    "X_train =  X_train / ageNorm\n",
    "X_test =  X_test / ageNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-penalty",
   "metadata": {},
   "source": [
    "### Plot after normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "solved-enclosure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_train, y_train, 'x');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-general",
   "metadata": {},
   "source": [
    "### Use sklearn to train model and then plot calculated function and train data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "downtown-mirror",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B0: [2809.80924866], B1: [396630.52488458]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression()\n",
    "linr.fit(X_train.reshape(-1,1), y_train.reshape(-1,1))\n",
    "\n",
    "beta0 = linr.intercept_\n",
    "beta1 = linr.coef_[0]\n",
    "\n",
    "yplot = X_train * beta1 + beta0\n",
    "\n",
    "plt.plot(X_train, y_train, 'x');\n",
    "plt.plot(X_train, yplot, 'g')\n",
    "plt.show()\n",
    "\n",
    "print(f'B0: {beta0}, B1: {beta1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-botswana",
   "metadata": {},
   "source": [
    "### Use pytorch to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "exterior-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "batch_size_train = 16\n",
    "batch_size_test = 20\n",
    "learning_rate = 0.1\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "# transform to torch tensor\n",
    "tensor_train_x = torch.Tensor(X_train.reshape(-1, 1))\n",
    "tensor_train_y = torch.Tensor(y_train.reshape(-1, 1))\n",
    "\n",
    "# create your datset\n",
    "train_data = TensorDataset(tensor_train_x, tensor_train_y)\n",
    "\n",
    "#Create loader using data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True)\n",
    "\n",
    "# transform to torch tensor\n",
    "tensor_test_x = torch.Tensor(X_test.reshape(-1, 1))\n",
    "tensor_test_y = torch.Tensor(y_test.reshape(-1, 1))\n",
    "\n",
    "# create your datset\n",
    "test_data = TensorDataset(tensor_test_x, tensor_test_y)\n",
    "\n",
    "#Create loader using data\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size_test,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "brief-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear layer with single input and single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call to the layer to make predictions\n",
    "        return self.linear(x)\n",
    "    \n",
    "#Network model\n",
    "network = LayerLinearRegression()\n",
    "\n",
    "#Configure loss function\n",
    "LossFunction = nn.MSELoss(reduction='mean')\n",
    "\n",
    "#Stochastic gradient decent\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "understood-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "effective-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  #Configure network for training\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    \n",
    "    #set parameters gradients to 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #runs model with data and returns result in output\n",
    "    #here data contains a 64 * 1 * 28 * 28 tensor, 64 is batch size\n",
    "    output = network(data)\n",
    "    \n",
    "    #Use negative log likelihood loss.\n",
    "    loss = LossFunction(output, target)\n",
    "    \n",
    "    #with this gradients are calculated\n",
    "    loss.backward()\n",
    "    \n",
    "    #update gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    #Display iteration statistics\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "      torch.save(network.state_dict(), 'results/model.pth')\n",
    "      torch.save(optimizer.state_dict(), 'results/optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "rotary-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  #Sets the module in evaluation mode\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "    \n",
    "  #dont update dynamic computation graph\n",
    "  with torch.no_grad():\n",
    "    #for every example in test\n",
    "    for data, target in test_loader:\n",
    "      #evaluate the model\n",
    "      output = network(data)\n",
    "        \n",
    "      #acumulate the loss\n",
    "      test_loss += LossFunction(output, target).item()\n",
    "      \n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "organized-atlas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 7512699.0320, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 1 [0/869 (0%)]\tLoss: 132995928.000000\n",
      "Train Epoch: 1 [160/869 (18%)]\tLoss: 94727888.000000\n",
      "Train Epoch: 1 [320/869 (36%)]\tLoss: 39330016.000000\n",
      "Train Epoch: 1 [480/869 (55%)]\tLoss: 119759960.000000\n",
      "Train Epoch: 1 [640/869 (73%)]\tLoss: 72515528.000000\n",
      "Train Epoch: 1 [800/869 (91%)]\tLoss: 159702864.000000\n",
      "\n",
      "Test set: Avg. loss: 7699766.4051, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 2 [0/869 (0%)]\tLoss: 120988152.000000\n",
      "Train Epoch: 2 [160/869 (18%)]\tLoss: 95570704.000000\n",
      "Train Epoch: 2 [320/869 (36%)]\tLoss: 43704900.000000\n",
      "Train Epoch: 2 [480/869 (55%)]\tLoss: 232338512.000000\n",
      "Train Epoch: 2 [640/869 (73%)]\tLoss: 77343488.000000\n",
      "Train Epoch: 2 [800/869 (91%)]\tLoss: 85070336.000000\n",
      "\n",
      "Test set: Avg. loss: 7404207.4115, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 3 [0/869 (0%)]\tLoss: 66822616.000000\n",
      "Train Epoch: 3 [160/869 (18%)]\tLoss: 81168176.000000\n",
      "Train Epoch: 3 [320/869 (36%)]\tLoss: 143722128.000000\n",
      "Train Epoch: 3 [480/869 (55%)]\tLoss: 154547168.000000\n",
      "Train Epoch: 3 [640/869 (73%)]\tLoss: 197456880.000000\n",
      "Train Epoch: 3 [800/869 (91%)]\tLoss: 86355448.000000\n",
      "\n",
      "Test set: Avg. loss: 7420542.4648, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 4 [0/869 (0%)]\tLoss: 179372832.000000\n",
      "Train Epoch: 4 [160/869 (18%)]\tLoss: 44467392.000000\n",
      "Train Epoch: 4 [320/869 (36%)]\tLoss: 41071156.000000\n",
      "Train Epoch: 4 [480/869 (55%)]\tLoss: 242112608.000000\n",
      "Train Epoch: 4 [640/869 (73%)]\tLoss: 126256656.000000\n",
      "Train Epoch: 4 [800/869 (91%)]\tLoss: 50671068.000000\n",
      "\n",
      "Test set: Avg. loss: 7753968.8443, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 5 [0/869 (0%)]\tLoss: 79758952.000000\n",
      "Train Epoch: 5 [160/869 (18%)]\tLoss: 181373376.000000\n",
      "Train Epoch: 5 [320/869 (36%)]\tLoss: 83739704.000000\n",
      "Train Epoch: 5 [480/869 (55%)]\tLoss: 164708448.000000\n",
      "Train Epoch: 5 [640/869 (73%)]\tLoss: 45427976.000000\n",
      "Train Epoch: 5 [800/869 (91%)]\tLoss: 79351168.000000\n",
      "\n",
      "Test set: Avg. loss: 7575718.3497, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 6 [0/869 (0%)]\tLoss: 40717740.000000\n",
      "Train Epoch: 6 [160/869 (18%)]\tLoss: 150521776.000000\n",
      "Train Epoch: 6 [320/869 (36%)]\tLoss: 94854856.000000\n",
      "Train Epoch: 6 [480/869 (55%)]\tLoss: 185227296.000000\n",
      "Train Epoch: 6 [640/869 (73%)]\tLoss: 80480288.000000\n",
      "Train Epoch: 6 [800/869 (91%)]\tLoss: 213842304.000000\n",
      "\n",
      "Test set: Avg. loss: 7353994.6397, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 7 [0/869 (0%)]\tLoss: 85990920.000000\n",
      "Train Epoch: 7 [160/869 (18%)]\tLoss: 142222016.000000\n",
      "Train Epoch: 7 [320/869 (36%)]\tLoss: 131619808.000000\n",
      "Train Epoch: 7 [480/869 (55%)]\tLoss: 37155880.000000\n",
      "Train Epoch: 7 [640/869 (73%)]\tLoss: 141205200.000000\n",
      "Train Epoch: 7 [800/869 (91%)]\tLoss: 175642784.000000\n",
      "\n",
      "Test set: Avg. loss: 7649073.0959, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 8 [0/869 (0%)]\tLoss: 109081240.000000\n",
      "Train Epoch: 8 [160/869 (18%)]\tLoss: 89095072.000000\n",
      "Train Epoch: 8 [320/869 (36%)]\tLoss: 96164248.000000\n",
      "Train Epoch: 8 [480/869 (55%)]\tLoss: 179951344.000000\n",
      "Train Epoch: 8 [640/869 (73%)]\tLoss: 123231760.000000\n",
      "Train Epoch: 8 [800/869 (91%)]\tLoss: 84178120.000000\n",
      "\n",
      "Test set: Avg. loss: 7336839.1471, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 9 [0/869 (0%)]\tLoss: 212435184.000000\n",
      "Train Epoch: 9 [160/869 (18%)]\tLoss: 229336960.000000\n",
      "Train Epoch: 9 [320/869 (36%)]\tLoss: 135686544.000000\n",
      "Train Epoch: 9 [480/869 (55%)]\tLoss: 82896928.000000\n",
      "Train Epoch: 9 [640/869 (73%)]\tLoss: 125946088.000000\n",
      "Train Epoch: 9 [800/869 (91%)]\tLoss: 207483424.000000\n",
      "\n",
      "Test set: Avg. loss: 7667812.4947, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 10 [0/869 (0%)]\tLoss: 35545328.000000\n",
      "Train Epoch: 10 [160/869 (18%)]\tLoss: 148862720.000000\n",
      "Train Epoch: 10 [320/869 (36%)]\tLoss: 142806448.000000\n",
      "Train Epoch: 10 [480/869 (55%)]\tLoss: 189562592.000000\n",
      "Train Epoch: 10 [640/869 (73%)]\tLoss: 85017208.000000\n",
      "Train Epoch: 10 [800/869 (91%)]\tLoss: 89747416.000000\n",
      "\n",
      "Test set: Avg. loss: 7548726.9339, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 11 [0/869 (0%)]\tLoss: 64474900.000000\n",
      "Train Epoch: 11 [160/869 (18%)]\tLoss: 61744512.000000\n",
      "Train Epoch: 11 [320/869 (36%)]\tLoss: 152577952.000000\n",
      "Train Epoch: 11 [480/869 (55%)]\tLoss: 89555256.000000\n",
      "Train Epoch: 11 [640/869 (73%)]\tLoss: 46398900.000000\n",
      "Train Epoch: 11 [800/869 (91%)]\tLoss: 55891756.000000\n",
      "\n",
      "Test set: Avg. loss: 7545544.2729, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 12 [0/869 (0%)]\tLoss: 184004368.000000\n",
      "Train Epoch: 12 [160/869 (18%)]\tLoss: 95800848.000000\n",
      "Train Epoch: 12 [320/869 (36%)]\tLoss: 84056048.000000\n",
      "Train Epoch: 12 [480/869 (55%)]\tLoss: 101406808.000000\n",
      "Train Epoch: 12 [640/869 (73%)]\tLoss: 114935880.000000\n",
      "Train Epoch: 12 [800/869 (91%)]\tLoss: 209891216.000000\n",
      "\n",
      "Test set: Avg. loss: 7505717.5267, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 13 [0/869 (0%)]\tLoss: 145802592.000000\n",
      "Train Epoch: 13 [160/869 (18%)]\tLoss: 97581664.000000\n",
      "Train Epoch: 13 [320/869 (36%)]\tLoss: 94584760.000000\n",
      "Train Epoch: 13 [480/869 (55%)]\tLoss: 15252705.000000\n",
      "Train Epoch: 13 [640/869 (73%)]\tLoss: 112565216.000000\n",
      "Train Epoch: 13 [800/869 (91%)]\tLoss: 80261376.000000\n",
      "\n",
      "Test set: Avg. loss: 7767551.9488, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 14 [0/869 (0%)]\tLoss: 60605016.000000\n",
      "Train Epoch: 14 [160/869 (18%)]\tLoss: 276570624.000000\n",
      "Train Epoch: 14 [320/869 (36%)]\tLoss: 82306752.000000\n",
      "Train Epoch: 14 [480/869 (55%)]\tLoss: 58831356.000000\n",
      "Train Epoch: 14 [640/869 (73%)]\tLoss: 103905680.000000\n",
      "Train Epoch: 14 [800/869 (91%)]\tLoss: 179743408.000000\n",
      "\n",
      "Test set: Avg. loss: 7315029.2196, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 15 [0/869 (0%)]\tLoss: 172116736.000000\n",
      "Train Epoch: 15 [160/869 (18%)]\tLoss: 100162224.000000\n",
      "Train Epoch: 15 [320/869 (36%)]\tLoss: 96972976.000000\n",
      "Train Epoch: 15 [480/869 (55%)]\tLoss: 113173328.000000\n",
      "Train Epoch: 15 [640/869 (73%)]\tLoss: 77320440.000000\n",
      "Train Epoch: 15 [800/869 (91%)]\tLoss: 96627128.000000\n",
      "\n",
      "Test set: Avg. loss: 8293507.4115, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 16 [0/869 (0%)]\tLoss: 126675432.000000\n",
      "Train Epoch: 16 [160/869 (18%)]\tLoss: 201292496.000000\n",
      "Train Epoch: 16 [320/869 (36%)]\tLoss: 78550912.000000\n",
      "Train Epoch: 16 [480/869 (55%)]\tLoss: 111238880.000000\n",
      "Train Epoch: 16 [640/869 (73%)]\tLoss: 157156032.000000\n",
      "Train Epoch: 16 [800/869 (91%)]\tLoss: 153661056.000000\n",
      "\n",
      "Test set: Avg. loss: 7999499.1173, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 17 [0/869 (0%)]\tLoss: 160490528.000000\n",
      "Train Epoch: 17 [160/869 (18%)]\tLoss: 272958080.000000\n",
      "Train Epoch: 17 [320/869 (36%)]\tLoss: 129683328.000000\n",
      "Train Epoch: 17 [480/869 (55%)]\tLoss: 143317408.000000\n",
      "Train Epoch: 17 [640/869 (73%)]\tLoss: 145157824.000000\n",
      "Train Epoch: 17 [800/869 (91%)]\tLoss: 188270080.000000\n",
      "\n",
      "Test set: Avg. loss: 7487731.8678, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 18 [0/869 (0%)]\tLoss: 107293456.000000\n",
      "Train Epoch: 18 [160/869 (18%)]\tLoss: 120419632.000000\n",
      "Train Epoch: 18 [320/869 (36%)]\tLoss: 291990464.000000\n",
      "Train Epoch: 18 [480/869 (55%)]\tLoss: 72183872.000000\n",
      "Train Epoch: 18 [640/869 (73%)]\tLoss: 80936040.000000\n",
      "Train Epoch: 18 [800/869 (91%)]\tLoss: 225462400.000000\n",
      "\n",
      "Test set: Avg. loss: 7437201.6631, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 19 [0/869 (0%)]\tLoss: 103044192.000000\n",
      "Train Epoch: 19 [160/869 (18%)]\tLoss: 92665400.000000\n",
      "Train Epoch: 19 [320/869 (36%)]\tLoss: 133371136.000000\n",
      "Train Epoch: 19 [480/869 (55%)]\tLoss: 105159304.000000\n",
      "Train Epoch: 19 [640/869 (73%)]\tLoss: 59366848.000000\n",
      "Train Epoch: 19 [800/869 (91%)]\tLoss: 46596648.000000\n",
      "\n",
      "Test set: Avg. loss: 7531077.9104, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 20 [0/869 (0%)]\tLoss: 93716152.000000\n",
      "Train Epoch: 20 [160/869 (18%)]\tLoss: 89754304.000000\n",
      "Train Epoch: 20 [320/869 (36%)]\tLoss: 293262656.000000\n",
      "Train Epoch: 20 [480/869 (55%)]\tLoss: 169387488.000000\n",
      "Train Epoch: 20 [640/869 (73%)]\tLoss: 239494464.000000\n",
      "Train Epoch: 20 [800/869 (91%)]\tLoss: 215401424.000000\n",
      "\n",
      "Test set: Avg. loss: 7592509.1002, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 21 [0/869 (0%)]\tLoss: 135386032.000000\n",
      "Train Epoch: 21 [160/869 (18%)]\tLoss: 189953056.000000\n",
      "Train Epoch: 21 [320/869 (36%)]\tLoss: 90671640.000000\n",
      "Train Epoch: 21 [480/869 (55%)]\tLoss: 99635648.000000\n",
      "Train Epoch: 21 [640/869 (73%)]\tLoss: 103964984.000000\n",
      "Train Epoch: 21 [800/869 (91%)]\tLoss: 235725280.000000\n",
      "\n",
      "Test set: Avg. loss: 7310058.2175, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 22 [0/869 (0%)]\tLoss: 115425424.000000\n",
      "Train Epoch: 22 [160/869 (18%)]\tLoss: 61473232.000000\n",
      "Train Epoch: 22 [320/869 (36%)]\tLoss: 156029312.000000\n",
      "Train Epoch: 22 [480/869 (55%)]\tLoss: 126391448.000000\n",
      "Train Epoch: 22 [640/869 (73%)]\tLoss: 69759288.000000\n",
      "Train Epoch: 22 [800/869 (91%)]\tLoss: 214042816.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 7353764.6055, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 23 [0/869 (0%)]\tLoss: 133764528.000000\n",
      "Train Epoch: 23 [160/869 (18%)]\tLoss: 105787104.000000\n",
      "Train Epoch: 23 [320/869 (36%)]\tLoss: 40288176.000000\n",
      "Train Epoch: 23 [480/869 (55%)]\tLoss: 27395266.000000\n",
      "Train Epoch: 23 [640/869 (73%)]\tLoss: 137696784.000000\n",
      "Train Epoch: 23 [800/869 (91%)]\tLoss: 131029864.000000\n",
      "\n",
      "Test set: Avg. loss: 8231261.0235, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 24 [0/869 (0%)]\tLoss: 74248768.000000\n",
      "Train Epoch: 24 [160/869 (18%)]\tLoss: 87740032.000000\n",
      "Train Epoch: 24 [320/869 (36%)]\tLoss: 137454352.000000\n",
      "Train Epoch: 24 [480/869 (55%)]\tLoss: 183529280.000000\n",
      "Train Epoch: 24 [640/869 (73%)]\tLoss: 99240088.000000\n",
      "Train Epoch: 24 [800/869 (91%)]\tLoss: 71938592.000000\n",
      "\n",
      "Test set: Avg. loss: 7501279.7271, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 25 [0/869 (0%)]\tLoss: 177684896.000000\n",
      "Train Epoch: 25 [160/869 (18%)]\tLoss: 96611792.000000\n",
      "Train Epoch: 25 [320/869 (36%)]\tLoss: 101739864.000000\n",
      "Train Epoch: 25 [480/869 (55%)]\tLoss: 84065120.000000\n",
      "Train Epoch: 25 [640/869 (73%)]\tLoss: 49004808.000000\n",
      "Train Epoch: 25 [800/869 (91%)]\tLoss: 141794336.000000\n",
      "\n",
      "Test set: Avg. loss: 7621214.3881, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 26 [0/869 (0%)]\tLoss: 181987072.000000\n",
      "Train Epoch: 26 [160/869 (18%)]\tLoss: 176385920.000000\n",
      "Train Epoch: 26 [320/869 (36%)]\tLoss: 129334648.000000\n",
      "Train Epoch: 26 [480/869 (55%)]\tLoss: 84655408.000000\n",
      "Train Epoch: 26 [640/869 (73%)]\tLoss: 225659008.000000\n",
      "Train Epoch: 26 [800/869 (91%)]\tLoss: 120550848.000000\n",
      "\n",
      "Test set: Avg. loss: 7829906.7804, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 27 [0/869 (0%)]\tLoss: 160894496.000000\n",
      "Train Epoch: 27 [160/869 (18%)]\tLoss: 228571088.000000\n",
      "Train Epoch: 27 [320/869 (36%)]\tLoss: 163488992.000000\n",
      "Train Epoch: 27 [480/869 (55%)]\tLoss: 43586580.000000\n",
      "Train Epoch: 27 [640/869 (73%)]\tLoss: 192011216.000000\n",
      "Train Epoch: 27 [800/869 (91%)]\tLoss: 221469072.000000\n",
      "\n",
      "Test set: Avg. loss: 7327708.7079, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 28 [0/869 (0%)]\tLoss: 84645288.000000\n",
      "Train Epoch: 28 [160/869 (18%)]\tLoss: 59573840.000000\n",
      "Train Epoch: 28 [320/869 (36%)]\tLoss: 109986472.000000\n",
      "Train Epoch: 28 [480/869 (55%)]\tLoss: 98606992.000000\n",
      "Train Epoch: 28 [640/869 (73%)]\tLoss: 162233392.000000\n",
      "Train Epoch: 28 [800/869 (91%)]\tLoss: 114091344.000000\n",
      "\n",
      "Test set: Avg. loss: 7651629.5693, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 29 [0/869 (0%)]\tLoss: 80499896.000000\n",
      "Train Epoch: 29 [160/869 (18%)]\tLoss: 133493912.000000\n",
      "Train Epoch: 29 [320/869 (36%)]\tLoss: 183709920.000000\n",
      "Train Epoch: 29 [480/869 (55%)]\tLoss: 54837120.000000\n",
      "Train Epoch: 29 [640/869 (73%)]\tLoss: 115730424.000000\n",
      "Train Epoch: 29 [800/869 (91%)]\tLoss: 223627264.000000\n",
      "\n",
      "Test set: Avg. loss: 7474069.6461, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 30 [0/869 (0%)]\tLoss: 68129528.000000\n",
      "Train Epoch: 30 [160/869 (18%)]\tLoss: 49244884.000000\n",
      "Train Epoch: 30 [320/869 (36%)]\tLoss: 169790928.000000\n",
      "Train Epoch: 30 [480/869 (55%)]\tLoss: 117227368.000000\n",
      "Train Epoch: 30 [640/869 (73%)]\tLoss: 146316400.000000\n",
      "Train Epoch: 30 [800/869 (91%)]\tLoss: 103287136.000000\n",
      "\n",
      "Test set: Avg. loss: 7608470.2260, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 31 [0/869 (0%)]\tLoss: 148342640.000000\n",
      "Train Epoch: 31 [160/869 (18%)]\tLoss: 63842380.000000\n",
      "Train Epoch: 31 [320/869 (36%)]\tLoss: 49501456.000000\n",
      "Train Epoch: 31 [480/869 (55%)]\tLoss: 233829264.000000\n",
      "Train Epoch: 31 [640/869 (73%)]\tLoss: 84571416.000000\n",
      "Train Epoch: 31 [800/869 (91%)]\tLoss: 115106320.000000\n",
      "\n",
      "Test set: Avg. loss: 7317152.0853, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 32 [0/869 (0%)]\tLoss: 73889656.000000\n",
      "Train Epoch: 32 [160/869 (18%)]\tLoss: 153912880.000000\n",
      "Train Epoch: 32 [320/869 (36%)]\tLoss: 183442320.000000\n",
      "Train Epoch: 32 [480/869 (55%)]\tLoss: 84943744.000000\n",
      "Train Epoch: 32 [640/869 (73%)]\tLoss: 193154912.000000\n",
      "Train Epoch: 32 [800/869 (91%)]\tLoss: 101304232.000000\n",
      "\n",
      "Test set: Avg. loss: 7617192.8699, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 33 [0/869 (0%)]\tLoss: 55085760.000000\n",
      "Train Epoch: 33 [160/869 (18%)]\tLoss: 101059608.000000\n",
      "Train Epoch: 33 [320/869 (36%)]\tLoss: 142289760.000000\n",
      "Train Epoch: 33 [480/869 (55%)]\tLoss: 104207752.000000\n",
      "Train Epoch: 33 [640/869 (73%)]\tLoss: 152780816.000000\n",
      "Train Epoch: 33 [800/869 (91%)]\tLoss: 97549904.000000\n",
      "\n",
      "Test set: Avg. loss: 7669138.7591, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 34 [0/869 (0%)]\tLoss: 128372056.000000\n",
      "Train Epoch: 34 [160/869 (18%)]\tLoss: 137942272.000000\n",
      "Train Epoch: 34 [320/869 (36%)]\tLoss: 151902688.000000\n",
      "Train Epoch: 34 [480/869 (55%)]\tLoss: 167367936.000000\n",
      "Train Epoch: 34 [640/869 (73%)]\tLoss: 143092800.000000\n",
      "Train Epoch: 34 [800/869 (91%)]\tLoss: 128058992.000000\n",
      "\n",
      "Test set: Avg. loss: 7552381.7143, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 35 [0/869 (0%)]\tLoss: 117843968.000000\n",
      "Train Epoch: 35 [160/869 (18%)]\tLoss: 40838784.000000\n",
      "Train Epoch: 35 [320/869 (36%)]\tLoss: 132719688.000000\n",
      "Train Epoch: 35 [480/869 (55%)]\tLoss: 37561680.000000\n",
      "Train Epoch: 35 [640/869 (73%)]\tLoss: 155310000.000000\n",
      "Train Epoch: 35 [800/869 (91%)]\tLoss: 223328240.000000\n",
      "\n",
      "Test set: Avg. loss: 7670551.3092, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 36 [0/869 (0%)]\tLoss: 111600408.000000\n",
      "Train Epoch: 36 [160/869 (18%)]\tLoss: 172731200.000000\n",
      "Train Epoch: 36 [320/869 (36%)]\tLoss: 138252128.000000\n",
      "Train Epoch: 36 [480/869 (55%)]\tLoss: 122985536.000000\n",
      "Train Epoch: 36 [640/869 (73%)]\tLoss: 162303632.000000\n",
      "Train Epoch: 36 [800/869 (91%)]\tLoss: 84189488.000000\n",
      "\n",
      "Test set: Avg. loss: 7391307.3603, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 37 [0/869 (0%)]\tLoss: 184569280.000000\n",
      "Train Epoch: 37 [160/869 (18%)]\tLoss: 105370896.000000\n",
      "Train Epoch: 37 [320/869 (36%)]\tLoss: 186129648.000000\n",
      "Train Epoch: 37 [480/869 (55%)]\tLoss: 163249760.000000\n",
      "Train Epoch: 37 [640/869 (73%)]\tLoss: 158974112.000000\n",
      "Train Epoch: 37 [800/869 (91%)]\tLoss: 131847928.000000\n",
      "\n",
      "Test set: Avg. loss: 8232680.7164, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 38 [0/869 (0%)]\tLoss: 107918568.000000\n",
      "Train Epoch: 38 [160/869 (18%)]\tLoss: 75308952.000000\n",
      "Train Epoch: 38 [320/869 (36%)]\tLoss: 134511904.000000\n",
      "Train Epoch: 38 [480/869 (55%)]\tLoss: 111521856.000000\n",
      "Train Epoch: 38 [640/869 (73%)]\tLoss: 149742336.000000\n",
      "Train Epoch: 38 [800/869 (91%)]\tLoss: 41389076.000000\n",
      "\n",
      "Test set: Avg. loss: 7624231.3859, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 39 [0/869 (0%)]\tLoss: 98223280.000000\n",
      "Train Epoch: 39 [160/869 (18%)]\tLoss: 80292560.000000\n",
      "Train Epoch: 39 [320/869 (36%)]\tLoss: 173874624.000000\n",
      "Train Epoch: 39 [480/869 (55%)]\tLoss: 134530464.000000\n",
      "Train Epoch: 39 [640/869 (73%)]\tLoss: 112220336.000000\n",
      "Train Epoch: 39 [800/869 (91%)]\tLoss: 235777120.000000\n",
      "\n",
      "Test set: Avg. loss: 7398559.4200, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 40 [0/869 (0%)]\tLoss: 80200384.000000\n",
      "Train Epoch: 40 [160/869 (18%)]\tLoss: 132689592.000000\n",
      "Train Epoch: 40 [320/869 (36%)]\tLoss: 146325024.000000\n",
      "Train Epoch: 40 [480/869 (55%)]\tLoss: 108609920.000000\n",
      "Train Epoch: 40 [640/869 (73%)]\tLoss: 138231232.000000\n",
      "Train Epoch: 40 [800/869 (91%)]\tLoss: 223132992.000000\n",
      "\n",
      "Test set: Avg. loss: 7327849.8124, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 41 [0/869 (0%)]\tLoss: 53164940.000000\n",
      "Train Epoch: 41 [160/869 (18%)]\tLoss: 185790064.000000\n",
      "Train Epoch: 41 [320/869 (36%)]\tLoss: 162089712.000000\n",
      "Train Epoch: 41 [480/869 (55%)]\tLoss: 72783440.000000\n",
      "Train Epoch: 41 [640/869 (73%)]\tLoss: 91794600.000000\n",
      "Train Epoch: 41 [800/869 (91%)]\tLoss: 175125920.000000\n",
      "\n",
      "Test set: Avg. loss: 7412842.9638, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 42 [0/869 (0%)]\tLoss: 97845808.000000\n",
      "Train Epoch: 42 [160/869 (18%)]\tLoss: 119298344.000000\n",
      "Train Epoch: 42 [320/869 (36%)]\tLoss: 181618752.000000\n",
      "Train Epoch: 42 [480/869 (55%)]\tLoss: 95859200.000000\n",
      "Train Epoch: 42 [640/869 (73%)]\tLoss: 149650720.000000\n",
      "Train Epoch: 42 [800/869 (91%)]\tLoss: 105419208.000000\n",
      "\n",
      "Test set: Avg. loss: 7397904.4947, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 43 [0/869 (0%)]\tLoss: 184030336.000000\n",
      "Train Epoch: 43 [160/869 (18%)]\tLoss: 136894448.000000\n",
      "Train Epoch: 43 [320/869 (36%)]\tLoss: 67450712.000000\n",
      "Train Epoch: 43 [480/869 (55%)]\tLoss: 151792960.000000\n",
      "Train Epoch: 43 [640/869 (73%)]\tLoss: 144356032.000000\n",
      "Train Epoch: 43 [800/869 (91%)]\tLoss: 181245616.000000\n",
      "\n",
      "Test set: Avg. loss: 7528072.2729, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 44 [0/869 (0%)]\tLoss: 95187768.000000\n",
      "Train Epoch: 44 [160/869 (18%)]\tLoss: 72447408.000000\n",
      "Train Epoch: 44 [320/869 (36%)]\tLoss: 95891088.000000\n",
      "Train Epoch: 44 [480/869 (55%)]\tLoss: 47867416.000000\n",
      "Train Epoch: 44 [640/869 (73%)]\tLoss: 177684912.000000\n",
      "Train Epoch: 44 [800/869 (91%)]\tLoss: 58832736.000000\n",
      "\n",
      "Test set: Avg. loss: 7750137.6461, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 45 [0/869 (0%)]\tLoss: 139586000.000000\n",
      "Train Epoch: 45 [160/869 (18%)]\tLoss: 60094636.000000\n",
      "Train Epoch: 45 [320/869 (36%)]\tLoss: 165404128.000000\n",
      "Train Epoch: 45 [480/869 (55%)]\tLoss: 106122736.000000\n",
      "Train Epoch: 45 [640/869 (73%)]\tLoss: 76705544.000000\n",
      "Train Epoch: 45 [800/869 (91%)]\tLoss: 184229376.000000\n",
      "\n",
      "Test set: Avg. loss: 7431892.1876, Accuracy: 0/469 (0%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 46 [0/869 (0%)]\tLoss: 115731488.000000\n",
      "Train Epoch: 46 [160/869 (18%)]\tLoss: 360151776.000000\n",
      "Train Epoch: 46 [320/869 (36%)]\tLoss: 215490544.000000\n",
      "Train Epoch: 46 [480/869 (55%)]\tLoss: 99279328.000000\n",
      "Train Epoch: 46 [640/869 (73%)]\tLoss: 70950632.000000\n",
      "Train Epoch: 46 [800/869 (91%)]\tLoss: 46722840.000000\n",
      "\n",
      "Test set: Avg. loss: 7668666.0981, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 47 [0/869 (0%)]\tLoss: 49547788.000000\n",
      "Train Epoch: 47 [160/869 (18%)]\tLoss: 153443840.000000\n",
      "Train Epoch: 47 [320/869 (36%)]\tLoss: 233633856.000000\n",
      "Train Epoch: 47 [480/869 (55%)]\tLoss: 38500816.000000\n",
      "Train Epoch: 47 [640/869 (73%)]\tLoss: 50640072.000000\n",
      "Train Epoch: 47 [800/869 (91%)]\tLoss: 167625664.000000\n",
      "\n",
      "Test set: Avg. loss: 7478169.0576, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 48 [0/869 (0%)]\tLoss: 51912148.000000\n",
      "Train Epoch: 48 [160/869 (18%)]\tLoss: 113363208.000000\n",
      "Train Epoch: 48 [320/869 (36%)]\tLoss: 37553788.000000\n",
      "Train Epoch: 48 [480/869 (55%)]\tLoss: 159226288.000000\n",
      "Train Epoch: 48 [640/869 (73%)]\tLoss: 158631200.000000\n",
      "Train Epoch: 48 [800/869 (91%)]\tLoss: 220154816.000000\n",
      "\n",
      "Test set: Avg. loss: 7454649.5778, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 49 [0/869 (0%)]\tLoss: 99400672.000000\n",
      "Train Epoch: 49 [160/869 (18%)]\tLoss: 73468000.000000\n",
      "Train Epoch: 49 [320/869 (36%)]\tLoss: 216779456.000000\n",
      "Train Epoch: 49 [480/869 (55%)]\tLoss: 102689368.000000\n",
      "Train Epoch: 49 [640/869 (73%)]\tLoss: 175967264.000000\n",
      "Train Epoch: 49 [800/869 (91%)]\tLoss: 251978400.000000\n",
      "\n",
      "Test set: Avg. loss: 7636385.8593, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 50 [0/869 (0%)]\tLoss: 155441424.000000\n",
      "Train Epoch: 50 [160/869 (18%)]\tLoss: 104948352.000000\n",
      "Train Epoch: 50 [320/869 (36%)]\tLoss: 64098044.000000\n",
      "Train Epoch: 50 [480/869 (55%)]\tLoss: 155897120.000000\n",
      "Train Epoch: 50 [640/869 (73%)]\tLoss: 94220256.000000\n",
      "Train Epoch: 50 [800/869 (91%)]\tLoss: 120798416.000000\n",
      "\n",
      "Test set: Avg. loss: 7634675.7356, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 51 [0/869 (0%)]\tLoss: 273283840.000000\n",
      "Train Epoch: 51 [160/869 (18%)]\tLoss: 103441032.000000\n",
      "Train Epoch: 51 [320/869 (36%)]\tLoss: 186179616.000000\n",
      "Train Epoch: 51 [480/869 (55%)]\tLoss: 173729056.000000\n",
      "Train Epoch: 51 [640/869 (73%)]\tLoss: 102119200.000000\n",
      "Train Epoch: 51 [800/869 (91%)]\tLoss: 57011664.000000\n",
      "\n",
      "Test set: Avg. loss: 7456009.1343, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 52 [0/869 (0%)]\tLoss: 106270312.000000\n",
      "Train Epoch: 52 [160/869 (18%)]\tLoss: 114390440.000000\n",
      "Train Epoch: 52 [320/869 (36%)]\tLoss: 43393092.000000\n",
      "Train Epoch: 52 [480/869 (55%)]\tLoss: 62115704.000000\n",
      "Train Epoch: 52 [640/869 (73%)]\tLoss: 41586104.000000\n",
      "Train Epoch: 52 [800/869 (91%)]\tLoss: 78287856.000000\n",
      "\n",
      "Test set: Avg. loss: 7635103.0832, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 53 [0/869 (0%)]\tLoss: 74719648.000000\n",
      "Train Epoch: 53 [160/869 (18%)]\tLoss: 183589152.000000\n",
      "Train Epoch: 53 [320/869 (36%)]\tLoss: 185225504.000000\n",
      "Train Epoch: 53 [480/869 (55%)]\tLoss: 113844712.000000\n",
      "Train Epoch: 53 [640/869 (73%)]\tLoss: 112209488.000000\n",
      "Train Epoch: 53 [800/869 (91%)]\tLoss: 175240000.000000\n",
      "\n",
      "Test set: Avg. loss: 7426938.4648, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 54 [0/869 (0%)]\tLoss: 102906512.000000\n",
      "Train Epoch: 54 [160/869 (18%)]\tLoss: 197170384.000000\n",
      "Train Epoch: 54 [320/869 (36%)]\tLoss: 132701656.000000\n",
      "Train Epoch: 54 [480/869 (55%)]\tLoss: 82076400.000000\n",
      "Train Epoch: 54 [640/869 (73%)]\tLoss: 127991816.000000\n",
      "Train Epoch: 54 [800/869 (91%)]\tLoss: 98586968.000000\n",
      "\n",
      "Test set: Avg. loss: 7882122.6610, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 55 [0/869 (0%)]\tLoss: 89400448.000000\n",
      "Train Epoch: 55 [160/869 (18%)]\tLoss: 138577728.000000\n",
      "Train Epoch: 55 [320/869 (36%)]\tLoss: 161441808.000000\n",
      "Train Epoch: 55 [480/869 (55%)]\tLoss: 148891264.000000\n",
      "Train Epoch: 55 [640/869 (73%)]\tLoss: 125114520.000000\n",
      "Train Epoch: 55 [800/869 (91%)]\tLoss: 87138568.000000\n",
      "\n",
      "Test set: Avg. loss: 7632639.8891, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 56 [0/869 (0%)]\tLoss: 113976800.000000\n",
      "Train Epoch: 56 [160/869 (18%)]\tLoss: 211809616.000000\n",
      "Train Epoch: 56 [320/869 (36%)]\tLoss: 95673648.000000\n",
      "Train Epoch: 56 [480/869 (55%)]\tLoss: 84033520.000000\n",
      "Train Epoch: 56 [640/869 (73%)]\tLoss: 197085552.000000\n",
      "Train Epoch: 56 [800/869 (91%)]\tLoss: 118156792.000000\n",
      "\n",
      "Test set: Avg. loss: 7478123.9616, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 57 [0/869 (0%)]\tLoss: 26544702.000000\n",
      "Train Epoch: 57 [160/869 (18%)]\tLoss: 129696096.000000\n",
      "Train Epoch: 57 [320/869 (36%)]\tLoss: 124059536.000000\n",
      "Train Epoch: 57 [480/869 (55%)]\tLoss: 246209456.000000\n",
      "Train Epoch: 57 [640/869 (73%)]\tLoss: 129880984.000000\n",
      "Train Epoch: 57 [800/869 (91%)]\tLoss: 107143216.000000\n",
      "\n",
      "Test set: Avg. loss: 7419624.9723, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 58 [0/869 (0%)]\tLoss: 153410928.000000\n",
      "Train Epoch: 58 [160/869 (18%)]\tLoss: 143577760.000000\n",
      "Train Epoch: 58 [320/869 (36%)]\tLoss: 45572448.000000\n",
      "Train Epoch: 58 [480/869 (55%)]\tLoss: 168517872.000000\n",
      "Train Epoch: 58 [640/869 (73%)]\tLoss: 130093136.000000\n",
      "Train Epoch: 58 [800/869 (91%)]\tLoss: 125997128.000000\n",
      "\n",
      "Test set: Avg. loss: 7677010.3369, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 59 [0/869 (0%)]\tLoss: 146659328.000000\n",
      "Train Epoch: 59 [160/869 (18%)]\tLoss: 124288968.000000\n",
      "Train Epoch: 59 [320/869 (36%)]\tLoss: 87608784.000000\n",
      "Train Epoch: 59 [480/869 (55%)]\tLoss: 196837136.000000\n",
      "Train Epoch: 59 [640/869 (73%)]\tLoss: 128642840.000000\n",
      "Train Epoch: 59 [800/869 (91%)]\tLoss: 106655032.000000\n",
      "\n",
      "Test set: Avg. loss: 7634464.0256, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 60 [0/869 (0%)]\tLoss: 99587488.000000\n",
      "Train Epoch: 60 [160/869 (18%)]\tLoss: 70586416.000000\n",
      "Train Epoch: 60 [320/869 (36%)]\tLoss: 87374976.000000\n",
      "Train Epoch: 60 [480/869 (55%)]\tLoss: 50248164.000000\n",
      "Train Epoch: 60 [640/869 (73%)]\tLoss: 72979248.000000\n",
      "Train Epoch: 60 [800/869 (91%)]\tLoss: 135412992.000000\n",
      "\n",
      "Test set: Avg. loss: 7491543.1812, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 61 [0/869 (0%)]\tLoss: 110539104.000000\n",
      "Train Epoch: 61 [160/869 (18%)]\tLoss: 108722144.000000\n",
      "Train Epoch: 61 [320/869 (36%)]\tLoss: 98857704.000000\n",
      "Train Epoch: 61 [480/869 (55%)]\tLoss: 82486696.000000\n",
      "Train Epoch: 61 [640/869 (73%)]\tLoss: 89303640.000000\n",
      "Train Epoch: 61 [800/869 (91%)]\tLoss: 188016752.000000\n",
      "\n",
      "Test set: Avg. loss: 7416024.2985, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 62 [0/869 (0%)]\tLoss: 45698024.000000\n",
      "Train Epoch: 62 [160/869 (18%)]\tLoss: 58773672.000000\n",
      "Train Epoch: 62 [320/869 (36%)]\tLoss: 62924800.000000\n",
      "Train Epoch: 62 [480/869 (55%)]\tLoss: 87391904.000000\n",
      "Train Epoch: 62 [640/869 (73%)]\tLoss: 121953016.000000\n",
      "Train Epoch: 62 [800/869 (91%)]\tLoss: 341685440.000000\n",
      "\n",
      "Test set: Avg. loss: 7394510.7889, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 63 [0/869 (0%)]\tLoss: 59371384.000000\n",
      "Train Epoch: 63 [160/869 (18%)]\tLoss: 127381336.000000\n",
      "Train Epoch: 63 [320/869 (36%)]\tLoss: 124908600.000000\n",
      "Train Epoch: 63 [480/869 (55%)]\tLoss: 187610912.000000\n",
      "Train Epoch: 63 [640/869 (73%)]\tLoss: 113085944.000000\n",
      "Train Epoch: 63 [800/869 (91%)]\tLoss: 143453536.000000\n",
      "\n",
      "Test set: Avg. loss: 7472714.1066, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 64 [0/869 (0%)]\tLoss: 91359456.000000\n",
      "Train Epoch: 64 [160/869 (18%)]\tLoss: 99072072.000000\n",
      "Train Epoch: 64 [320/869 (36%)]\tLoss: 45553828.000000\n",
      "Train Epoch: 64 [480/869 (55%)]\tLoss: 96162136.000000\n",
      "Train Epoch: 64 [640/869 (73%)]\tLoss: 145545504.000000\n",
      "Train Epoch: 64 [800/869 (91%)]\tLoss: 49791360.000000\n",
      "\n",
      "Test set: Avg. loss: 7795604.7932, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 65 [0/869 (0%)]\tLoss: 68159120.000000\n",
      "Train Epoch: 65 [160/869 (18%)]\tLoss: 129309984.000000\n",
      "Train Epoch: 65 [320/869 (36%)]\tLoss: 63280140.000000\n",
      "Train Epoch: 65 [480/869 (55%)]\tLoss: 89876600.000000\n",
      "Train Epoch: 65 [640/869 (73%)]\tLoss: 137513760.000000\n",
      "Train Epoch: 65 [800/869 (91%)]\tLoss: 131643320.000000\n",
      "\n",
      "Test set: Avg. loss: 7687231.7441, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 66 [0/869 (0%)]\tLoss: 42465624.000000\n",
      "Train Epoch: 66 [160/869 (18%)]\tLoss: 107695488.000000\n",
      "Train Epoch: 66 [320/869 (36%)]\tLoss: 73285664.000000\n",
      "Train Epoch: 66 [480/869 (55%)]\tLoss: 34463692.000000\n",
      "Train Epoch: 66 [640/869 (73%)]\tLoss: 46789584.000000\n",
      "Train Epoch: 66 [800/869 (91%)]\tLoss: 49571748.000000\n",
      "\n",
      "Test set: Avg. loss: 7499251.3945, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 67 [0/869 (0%)]\tLoss: 151112000.000000\n",
      "Train Epoch: 67 [160/869 (18%)]\tLoss: 124916344.000000\n",
      "Train Epoch: 67 [320/869 (36%)]\tLoss: 85908240.000000\n",
      "Train Epoch: 67 [480/869 (55%)]\tLoss: 77713352.000000\n",
      "Train Epoch: 67 [640/869 (73%)]\tLoss: 64257700.000000\n",
      "Train Epoch: 67 [800/869 (91%)]\tLoss: 195764992.000000\n",
      "\n",
      "Test set: Avg. loss: 7340211.0192, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 68 [0/869 (0%)]\tLoss: 96449264.000000\n",
      "Train Epoch: 68 [160/869 (18%)]\tLoss: 149476640.000000\n",
      "Train Epoch: 68 [320/869 (36%)]\tLoss: 53746056.000000\n",
      "Train Epoch: 68 [480/869 (55%)]\tLoss: 141550016.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 68 [640/869 (73%)]\tLoss: 226593840.000000\n",
      "Train Epoch: 68 [800/869 (91%)]\tLoss: 69731184.000000\n",
      "\n",
      "Test set: Avg. loss: 7554536.1876, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 69 [0/869 (0%)]\tLoss: 143708544.000000\n",
      "Train Epoch: 69 [160/869 (18%)]\tLoss: 43147168.000000\n",
      "Train Epoch: 69 [320/869 (36%)]\tLoss: 148884464.000000\n",
      "Train Epoch: 69 [480/869 (55%)]\tLoss: 169197216.000000\n",
      "Train Epoch: 69 [640/869 (73%)]\tLoss: 130784944.000000\n",
      "Train Epoch: 69 [800/869 (91%)]\tLoss: 56086864.000000\n",
      "\n",
      "Test set: Avg. loss: 7567506.8145, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 70 [0/869 (0%)]\tLoss: 84430136.000000\n",
      "Train Epoch: 70 [160/869 (18%)]\tLoss: 109279280.000000\n",
      "Train Epoch: 70 [320/869 (36%)]\tLoss: 223898640.000000\n",
      "Train Epoch: 70 [480/869 (55%)]\tLoss: 121744744.000000\n",
      "Train Epoch: 70 [640/869 (73%)]\tLoss: 124746872.000000\n",
      "Train Epoch: 70 [800/869 (91%)]\tLoss: 104206928.000000\n",
      "\n",
      "Test set: Avg. loss: 7493825.9104, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 71 [0/869 (0%)]\tLoss: 47084904.000000\n",
      "Train Epoch: 71 [160/869 (18%)]\tLoss: 117693192.000000\n",
      "Train Epoch: 71 [320/869 (36%)]\tLoss: 96210440.000000\n",
      "Train Epoch: 71 [480/869 (55%)]\tLoss: 121861736.000000\n",
      "Train Epoch: 71 [640/869 (73%)]\tLoss: 98118704.000000\n",
      "Train Epoch: 71 [800/869 (91%)]\tLoss: 123062640.000000\n",
      "\n",
      "Test set: Avg. loss: 7390372.9382, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 72 [0/869 (0%)]\tLoss: 260673632.000000\n",
      "Train Epoch: 72 [160/869 (18%)]\tLoss: 51349264.000000\n",
      "Train Epoch: 72 [320/869 (36%)]\tLoss: 149657456.000000\n",
      "Train Epoch: 72 [480/869 (55%)]\tLoss: 126357808.000000\n",
      "Train Epoch: 72 [640/869 (73%)]\tLoss: 194907936.000000\n",
      "Train Epoch: 72 [800/869 (91%)]\tLoss: 42367712.000000\n",
      "\n",
      "Test set: Avg. loss: 7426967.5480, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 73 [0/869 (0%)]\tLoss: 124857632.000000\n",
      "Train Epoch: 73 [160/869 (18%)]\tLoss: 90428544.000000\n",
      "Train Epoch: 73 [320/869 (36%)]\tLoss: 90691264.000000\n",
      "Train Epoch: 73 [480/869 (55%)]\tLoss: 119974976.000000\n",
      "Train Epoch: 73 [640/869 (73%)]\tLoss: 169605824.000000\n",
      "Train Epoch: 73 [800/869 (91%)]\tLoss: 50034404.000000\n",
      "\n",
      "Test set: Avg. loss: 7466007.1386, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 74 [0/869 (0%)]\tLoss: 89067432.000000\n",
      "Train Epoch: 74 [160/869 (18%)]\tLoss: 252729568.000000\n",
      "Train Epoch: 74 [320/869 (36%)]\tLoss: 176888144.000000\n",
      "Train Epoch: 74 [480/869 (55%)]\tLoss: 117226240.000000\n",
      "Train Epoch: 74 [640/869 (73%)]\tLoss: 103472584.000000\n",
      "Train Epoch: 74 [800/869 (91%)]\tLoss: 138752864.000000\n",
      "\n",
      "Test set: Avg. loss: 7487168.3156, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 75 [0/869 (0%)]\tLoss: 91769376.000000\n",
      "Train Epoch: 75 [160/869 (18%)]\tLoss: 53973648.000000\n",
      "Train Epoch: 75 [320/869 (36%)]\tLoss: 52676860.000000\n",
      "Train Epoch: 75 [480/869 (55%)]\tLoss: 107195392.000000\n",
      "Train Epoch: 75 [640/869 (73%)]\tLoss: 82403040.000000\n",
      "Train Epoch: 75 [800/869 (91%)]\tLoss: 79947648.000000\n",
      "\n",
      "Test set: Avg. loss: 7750960.0938, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 76 [0/869 (0%)]\tLoss: 116539528.000000\n",
      "Train Epoch: 76 [160/869 (18%)]\tLoss: 101386736.000000\n",
      "Train Epoch: 76 [320/869 (36%)]\tLoss: 122484928.000000\n",
      "Train Epoch: 76 [480/869 (55%)]\tLoss: 160160960.000000\n",
      "Train Epoch: 76 [640/869 (73%)]\tLoss: 67174632.000000\n",
      "Train Epoch: 76 [800/869 (91%)]\tLoss: 129409984.000000\n",
      "\n",
      "Test set: Avg. loss: 7386884.8443, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 77 [0/869 (0%)]\tLoss: 140685280.000000\n",
      "Train Epoch: 77 [160/869 (18%)]\tLoss: 184234192.000000\n",
      "Train Epoch: 77 [320/869 (36%)]\tLoss: 55660660.000000\n",
      "Train Epoch: 77 [480/869 (55%)]\tLoss: 110036312.000000\n",
      "Train Epoch: 77 [640/869 (73%)]\tLoss: 115933520.000000\n",
      "Train Epoch: 77 [800/869 (91%)]\tLoss: 33255490.000000\n",
      "\n",
      "Test set: Avg. loss: 8097523.9915, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 78 [0/869 (0%)]\tLoss: 183959680.000000\n",
      "Train Epoch: 78 [160/869 (18%)]\tLoss: 108817744.000000\n",
      "Train Epoch: 78 [320/869 (36%)]\tLoss: 238401328.000000\n",
      "Train Epoch: 78 [480/869 (55%)]\tLoss: 124338232.000000\n",
      "Train Epoch: 78 [640/869 (73%)]\tLoss: 427049440.000000\n",
      "Train Epoch: 78 [800/869 (91%)]\tLoss: 90405600.000000\n",
      "\n",
      "Test set: Avg. loss: 7574871.5224, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 79 [0/869 (0%)]\tLoss: 249167904.000000\n",
      "Train Epoch: 79 [160/869 (18%)]\tLoss: 44081556.000000\n",
      "Train Epoch: 79 [320/869 (36%)]\tLoss: 104220400.000000\n",
      "Train Epoch: 79 [480/869 (55%)]\tLoss: 202684672.000000\n",
      "Train Epoch: 79 [640/869 (73%)]\tLoss: 87509112.000000\n",
      "Train Epoch: 79 [800/869 (91%)]\tLoss: 87381208.000000\n",
      "\n",
      "Test set: Avg. loss: 7812407.0192, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 80 [0/869 (0%)]\tLoss: 181751232.000000\n",
      "Train Epoch: 80 [160/869 (18%)]\tLoss: 76153488.000000\n",
      "Train Epoch: 80 [320/869 (36%)]\tLoss: 152796864.000000\n",
      "Train Epoch: 80 [480/869 (55%)]\tLoss: 99364664.000000\n",
      "Train Epoch: 80 [640/869 (73%)]\tLoss: 152586320.000000\n",
      "Train Epoch: 80 [800/869 (91%)]\tLoss: 44352972.000000\n",
      "\n",
      "Test set: Avg. loss: 7592132.6567, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 81 [0/869 (0%)]\tLoss: 231756224.000000\n",
      "Train Epoch: 81 [160/869 (18%)]\tLoss: 134291424.000000\n",
      "Train Epoch: 81 [320/869 (36%)]\tLoss: 51187520.000000\n",
      "Train Epoch: 81 [480/869 (55%)]\tLoss: 115007424.000000\n",
      "Train Epoch: 81 [640/869 (73%)]\tLoss: 91267544.000000\n",
      "Train Epoch: 81 [800/869 (91%)]\tLoss: 125231472.000000\n",
      "\n",
      "Test set: Avg. loss: 7386908.4009, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 82 [0/869 (0%)]\tLoss: 122609880.000000\n",
      "Train Epoch: 82 [160/869 (18%)]\tLoss: 157088512.000000\n",
      "Train Epoch: 82 [320/869 (36%)]\tLoss: 20370552.000000\n",
      "Train Epoch: 82 [480/869 (55%)]\tLoss: 59388236.000000\n",
      "Train Epoch: 82 [640/869 (73%)]\tLoss: 135683904.000000\n",
      "Train Epoch: 82 [800/869 (91%)]\tLoss: 170762624.000000\n",
      "\n",
      "Test set: Avg. loss: 7640560.7846, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 83 [0/869 (0%)]\tLoss: 186306720.000000\n",
      "Train Epoch: 83 [160/869 (18%)]\tLoss: 106100704.000000\n",
      "Train Epoch: 83 [320/869 (36%)]\tLoss: 223838464.000000\n",
      "Train Epoch: 83 [480/869 (55%)]\tLoss: 112347144.000000\n",
      "Train Epoch: 83 [640/869 (73%)]\tLoss: 152348352.000000\n",
      "Train Epoch: 83 [800/869 (91%)]\tLoss: 296436000.000000\n",
      "\n",
      "Test set: Avg. loss: 7666099.4115, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 84 [0/869 (0%)]\tLoss: 75102928.000000\n",
      "Train Epoch: 84 [160/869 (18%)]\tLoss: 103013472.000000\n",
      "Train Epoch: 84 [320/869 (36%)]\tLoss: 90261040.000000\n",
      "Train Epoch: 84 [480/869 (55%)]\tLoss: 49376560.000000\n",
      "Train Epoch: 84 [640/869 (73%)]\tLoss: 92227944.000000\n",
      "Train Epoch: 84 [800/869 (91%)]\tLoss: 127368272.000000\n",
      "\n",
      "Test set: Avg. loss: 7629635.0874, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 85 [0/869 (0%)]\tLoss: 44959668.000000\n",
      "Train Epoch: 85 [160/869 (18%)]\tLoss: 185621232.000000\n",
      "Train Epoch: 85 [320/869 (36%)]\tLoss: 36833516.000000\n",
      "Train Epoch: 85 [480/869 (55%)]\tLoss: 93433528.000000\n",
      "Train Epoch: 85 [640/869 (73%)]\tLoss: 130419752.000000\n",
      "Train Epoch: 85 [800/869 (91%)]\tLoss: 374212320.000000\n",
      "\n",
      "Test set: Avg. loss: 7572287.4115, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 86 [0/869 (0%)]\tLoss: 86263984.000000\n",
      "Train Epoch: 86 [160/869 (18%)]\tLoss: 128805112.000000\n",
      "Train Epoch: 86 [320/869 (36%)]\tLoss: 153561744.000000\n",
      "Train Epoch: 86 [480/869 (55%)]\tLoss: 102159384.000000\n",
      "Train Epoch: 86 [640/869 (73%)]\tLoss: 112138880.000000\n",
      "Train Epoch: 86 [800/869 (91%)]\tLoss: 97172960.000000\n",
      "\n",
      "Test set: Avg. loss: 7414007.6972, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 87 [0/869 (0%)]\tLoss: 195559424.000000\n",
      "Train Epoch: 87 [160/869 (18%)]\tLoss: 99685544.000000\n",
      "Train Epoch: 87 [320/869 (36%)]\tLoss: 44007896.000000\n",
      "Train Epoch: 87 [480/869 (55%)]\tLoss: 82021312.000000\n",
      "Train Epoch: 87 [640/869 (73%)]\tLoss: 154773248.000000\n",
      "Train Epoch: 87 [800/869 (91%)]\tLoss: 225978016.000000\n",
      "\n",
      "Test set: Avg. loss: 7307060.3923, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 88 [0/869 (0%)]\tLoss: 57484620.000000\n",
      "Train Epoch: 88 [160/869 (18%)]\tLoss: 146616416.000000\n",
      "Train Epoch: 88 [320/869 (36%)]\tLoss: 143598112.000000\n",
      "Train Epoch: 88 [480/869 (55%)]\tLoss: 132323368.000000\n",
      "Train Epoch: 88 [640/869 (73%)]\tLoss: 202214784.000000\n",
      "Train Epoch: 88 [800/869 (91%)]\tLoss: 40522524.000000\n",
      "\n",
      "Test set: Avg. loss: 7557727.8891, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 89 [0/869 (0%)]\tLoss: 60252140.000000\n",
      "Train Epoch: 89 [160/869 (18%)]\tLoss: 144343984.000000\n",
      "Train Epoch: 89 [320/869 (36%)]\tLoss: 59080980.000000\n",
      "Train Epoch: 89 [480/869 (55%)]\tLoss: 78615624.000000\n",
      "Train Epoch: 89 [640/869 (73%)]\tLoss: 83207024.000000\n",
      "Train Epoch: 89 [800/869 (91%)]\tLoss: 52679512.000000\n",
      "\n",
      "Test set: Avg. loss: 7670993.1770, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 90 [0/869 (0%)]\tLoss: 36653144.000000\n",
      "Train Epoch: 90 [160/869 (18%)]\tLoss: 46579632.000000\n",
      "Train Epoch: 90 [320/869 (36%)]\tLoss: 139729136.000000\n",
      "Train Epoch: 90 [480/869 (55%)]\tLoss: 159576336.000000\n",
      "Train Epoch: 90 [640/869 (73%)]\tLoss: 33800960.000000\n",
      "Train Epoch: 90 [800/869 (91%)]\tLoss: 275923648.000000\n",
      "\n",
      "Test set: Avg. loss: 7581975.1301, Accuracy: 0/469 (0%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 91 [0/869 (0%)]\tLoss: 139935616.000000\n",
      "Train Epoch: 91 [160/869 (18%)]\tLoss: 134927216.000000\n",
      "Train Epoch: 91 [320/869 (36%)]\tLoss: 325516800.000000\n",
      "Train Epoch: 91 [480/869 (55%)]\tLoss: 170768016.000000\n",
      "Train Epoch: 91 [640/869 (73%)]\tLoss: 107069472.000000\n",
      "Train Epoch: 91 [800/869 (91%)]\tLoss: 232838416.000000\n",
      "\n",
      "Test set: Avg. loss: 7565353.6290, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 92 [0/869 (0%)]\tLoss: 42278880.000000\n",
      "Train Epoch: 92 [160/869 (18%)]\tLoss: 60152616.000000\n",
      "Train Epoch: 92 [320/869 (36%)]\tLoss: 112364416.000000\n",
      "Train Epoch: 92 [480/869 (55%)]\tLoss: 229886336.000000\n",
      "Train Epoch: 92 [640/869 (73%)]\tLoss: 215975360.000000\n",
      "Train Epoch: 92 [800/869 (91%)]\tLoss: 90044936.000000\n",
      "\n",
      "Test set: Avg. loss: 7757263.0704, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 93 [0/869 (0%)]\tLoss: 139184320.000000\n",
      "Train Epoch: 93 [160/869 (18%)]\tLoss: 136136544.000000\n",
      "Train Epoch: 93 [320/869 (36%)]\tLoss: 131669304.000000\n",
      "Train Epoch: 93 [480/869 (55%)]\tLoss: 144973328.000000\n",
      "Train Epoch: 93 [640/869 (73%)]\tLoss: 96696376.000000\n",
      "Train Epoch: 93 [800/869 (91%)]\tLoss: 93995344.000000\n",
      "\n",
      "Test set: Avg. loss: 7745995.1045, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 94 [0/869 (0%)]\tLoss: 158346912.000000\n",
      "Train Epoch: 94 [160/869 (18%)]\tLoss: 96472768.000000\n",
      "Train Epoch: 94 [320/869 (36%)]\tLoss: 84772448.000000\n",
      "Train Epoch: 94 [480/869 (55%)]\tLoss: 199207136.000000\n",
      "Train Epoch: 94 [640/869 (73%)]\tLoss: 48998932.000000\n",
      "Train Epoch: 94 [800/869 (91%)]\tLoss: 110634296.000000\n",
      "\n",
      "Test set: Avg. loss: 7957727.3646, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 95 [0/869 (0%)]\tLoss: 22099676.000000\n",
      "Train Epoch: 95 [160/869 (18%)]\tLoss: 117068024.000000\n",
      "Train Epoch: 95 [320/869 (36%)]\tLoss: 80934200.000000\n",
      "Train Epoch: 95 [480/869 (55%)]\tLoss: 106233632.000000\n",
      "Train Epoch: 95 [640/869 (73%)]\tLoss: 67852664.000000\n",
      "Train Epoch: 95 [800/869 (91%)]\tLoss: 84078232.000000\n",
      "\n",
      "Test set: Avg. loss: 7553700.4179, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 96 [0/869 (0%)]\tLoss: 116429616.000000\n",
      "Train Epoch: 96 [160/869 (18%)]\tLoss: 74576256.000000\n",
      "Train Epoch: 96 [320/869 (36%)]\tLoss: 187785552.000000\n",
      "Train Epoch: 96 [480/869 (55%)]\tLoss: 210809584.000000\n",
      "Train Epoch: 96 [640/869 (73%)]\tLoss: 27714076.000000\n",
      "Train Epoch: 96 [800/869 (91%)]\tLoss: 158919648.000000\n",
      "\n",
      "Test set: Avg. loss: 7310368.9296, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 97 [0/869 (0%)]\tLoss: 286498880.000000\n",
      "Train Epoch: 97 [160/869 (18%)]\tLoss: 86107128.000000\n",
      "Train Epoch: 97 [320/869 (36%)]\tLoss: 189656432.000000\n",
      "Train Epoch: 97 [480/869 (55%)]\tLoss: 101180160.000000\n",
      "Train Epoch: 97 [640/869 (73%)]\tLoss: 182733504.000000\n",
      "Train Epoch: 97 [800/869 (91%)]\tLoss: 175211552.000000\n",
      "\n",
      "Test set: Avg. loss: 7554802.9510, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 98 [0/869 (0%)]\tLoss: 254122672.000000\n",
      "Train Epoch: 98 [160/869 (18%)]\tLoss: 171600912.000000\n",
      "Train Epoch: 98 [320/869 (36%)]\tLoss: 129128320.000000\n",
      "Train Epoch: 98 [480/869 (55%)]\tLoss: 60728332.000000\n",
      "Train Epoch: 98 [640/869 (73%)]\tLoss: 48307580.000000\n",
      "Train Epoch: 98 [800/869 (91%)]\tLoss: 145158720.000000\n",
      "\n",
      "Test set: Avg. loss: 7429258.3881, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 99 [0/869 (0%)]\tLoss: 95992704.000000\n",
      "Train Epoch: 99 [160/869 (18%)]\tLoss: 106509608.000000\n",
      "Train Epoch: 99 [320/869 (36%)]\tLoss: 225998688.000000\n",
      "Train Epoch: 99 [480/869 (55%)]\tLoss: 82969392.000000\n",
      "Train Epoch: 99 [640/869 (73%)]\tLoss: 97886608.000000\n",
      "Train Epoch: 99 [800/869 (91%)]\tLoss: 136986432.000000\n",
      "\n",
      "Test set: Avg. loss: 7537071.0704, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 100 [0/869 (0%)]\tLoss: 113573720.000000\n",
      "Train Epoch: 100 [160/869 (18%)]\tLoss: 102459968.000000\n",
      "Train Epoch: 100 [320/869 (36%)]\tLoss: 80267720.000000\n",
      "Train Epoch: 100 [480/869 (55%)]\tLoss: 97068016.000000\n",
      "Train Epoch: 100 [640/869 (73%)]\tLoss: 77632336.000000\n",
      "Train Epoch: 100 [800/869 (91%)]\tLoss: 158776928.000000\n",
      "\n",
      "Test set: Avg. loss: 7732207.5565, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 101 [0/869 (0%)]\tLoss: 129624776.000000\n",
      "Train Epoch: 101 [160/869 (18%)]\tLoss: 101658136.000000\n",
      "Train Epoch: 101 [320/869 (36%)]\tLoss: 161573664.000000\n",
      "Train Epoch: 101 [480/869 (55%)]\tLoss: 95291304.000000\n",
      "Train Epoch: 101 [640/869 (73%)]\tLoss: 185223536.000000\n",
      "Train Epoch: 101 [800/869 (91%)]\tLoss: 179894944.000000\n",
      "\n",
      "Test set: Avg. loss: 7555243.5053, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 102 [0/869 (0%)]\tLoss: 51517480.000000\n",
      "Train Epoch: 102 [160/869 (18%)]\tLoss: 85477640.000000\n",
      "Train Epoch: 102 [320/869 (36%)]\tLoss: 144230144.000000\n",
      "Train Epoch: 102 [480/869 (55%)]\tLoss: 50525576.000000\n",
      "Train Epoch: 102 [640/869 (73%)]\tLoss: 86398880.000000\n",
      "Train Epoch: 102 [800/869 (91%)]\tLoss: 82949256.000000\n",
      "\n",
      "Test set: Avg. loss: 7807364.1450, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 103 [0/869 (0%)]\tLoss: 118663264.000000\n",
      "Train Epoch: 103 [160/869 (18%)]\tLoss: 229280704.000000\n",
      "Train Epoch: 103 [320/869 (36%)]\tLoss: 186620240.000000\n",
      "Train Epoch: 103 [480/869 (55%)]\tLoss: 214978000.000000\n",
      "Train Epoch: 103 [640/869 (73%)]\tLoss: 150713872.000000\n",
      "Train Epoch: 103 [800/869 (91%)]\tLoss: 149593152.000000\n",
      "\n",
      "Test set: Avg. loss: 7313745.3049, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 104 [0/869 (0%)]\tLoss: 95932384.000000\n",
      "Train Epoch: 104 [160/869 (18%)]\tLoss: 232372560.000000\n",
      "Train Epoch: 104 [320/869 (36%)]\tLoss: 70470392.000000\n",
      "Train Epoch: 104 [480/869 (55%)]\tLoss: 127787736.000000\n",
      "Train Epoch: 104 [640/869 (73%)]\tLoss: 57918992.000000\n",
      "Train Epoch: 104 [800/869 (91%)]\tLoss: 43510152.000000\n",
      "\n",
      "Test set: Avg. loss: 8279504.4222, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 105 [0/869 (0%)]\tLoss: 136440032.000000\n",
      "Train Epoch: 105 [160/869 (18%)]\tLoss: 275228288.000000\n",
      "Train Epoch: 105 [320/869 (36%)]\tLoss: 134420368.000000\n",
      "Train Epoch: 105 [480/869 (55%)]\tLoss: 92997664.000000\n",
      "Train Epoch: 105 [640/869 (73%)]\tLoss: 93869648.000000\n",
      "Train Epoch: 105 [800/869 (91%)]\tLoss: 194794752.000000\n",
      "\n",
      "Test set: Avg. loss: 7523342.2516, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 106 [0/869 (0%)]\tLoss: 190215072.000000\n",
      "Train Epoch: 106 [160/869 (18%)]\tLoss: 144599536.000000\n",
      "Train Epoch: 106 [320/869 (36%)]\tLoss: 329787776.000000\n",
      "Train Epoch: 106 [480/869 (55%)]\tLoss: 248137568.000000\n",
      "Train Epoch: 106 [640/869 (73%)]\tLoss: 61368700.000000\n",
      "Train Epoch: 106 [800/869 (91%)]\tLoss: 24454420.000000\n",
      "\n",
      "Test set: Avg. loss: 7844586.5714, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 107 [0/869 (0%)]\tLoss: 267710016.000000\n",
      "Train Epoch: 107 [160/869 (18%)]\tLoss: 52365952.000000\n",
      "Train Epoch: 107 [320/869 (36%)]\tLoss: 233953184.000000\n",
      "Train Epoch: 107 [480/869 (55%)]\tLoss: 140033856.000000\n",
      "Train Epoch: 107 [640/869 (73%)]\tLoss: 89973032.000000\n",
      "Train Epoch: 107 [800/869 (91%)]\tLoss: 100601536.000000\n",
      "\n",
      "Test set: Avg. loss: 7570471.7953, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 108 [0/869 (0%)]\tLoss: 169309104.000000\n",
      "Train Epoch: 108 [160/869 (18%)]\tLoss: 156431184.000000\n",
      "Train Epoch: 108 [320/869 (36%)]\tLoss: 119488144.000000\n",
      "Train Epoch: 108 [480/869 (55%)]\tLoss: 91332528.000000\n",
      "Train Epoch: 108 [640/869 (73%)]\tLoss: 115046872.000000\n",
      "Train Epoch: 108 [800/869 (91%)]\tLoss: 83748544.000000\n",
      "\n",
      "Test set: Avg. loss: 7502677.7313, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 109 [0/869 (0%)]\tLoss: 97524696.000000\n",
      "Train Epoch: 109 [160/869 (18%)]\tLoss: 115963856.000000\n",
      "Train Epoch: 109 [320/869 (36%)]\tLoss: 352804032.000000\n",
      "Train Epoch: 109 [480/869 (55%)]\tLoss: 161918096.000000\n",
      "Train Epoch: 109 [640/869 (73%)]\tLoss: 167428000.000000\n",
      "Train Epoch: 109 [800/869 (91%)]\tLoss: 81779328.000000\n",
      "\n",
      "Test set: Avg. loss: 7645627.2239, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 110 [0/869 (0%)]\tLoss: 23153560.000000\n",
      "Train Epoch: 110 [160/869 (18%)]\tLoss: 63873072.000000\n",
      "Train Epoch: 110 [320/869 (36%)]\tLoss: 43593836.000000\n",
      "Train Epoch: 110 [480/869 (55%)]\tLoss: 106345856.000000\n",
      "Train Epoch: 110 [640/869 (73%)]\tLoss: 161041216.000000\n",
      "Train Epoch: 110 [800/869 (91%)]\tLoss: 88065256.000000\n",
      "\n",
      "Test set: Avg. loss: 7369300.6226, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 111 [0/869 (0%)]\tLoss: 225521728.000000\n",
      "Train Epoch: 111 [160/869 (18%)]\tLoss: 214979488.000000\n",
      "Train Epoch: 111 [320/869 (36%)]\tLoss: 111108072.000000\n",
      "Train Epoch: 111 [480/869 (55%)]\tLoss: 58322152.000000\n",
      "Train Epoch: 111 [640/869 (73%)]\tLoss: 25413934.000000\n",
      "Train Epoch: 111 [800/869 (91%)]\tLoss: 156077424.000000\n",
      "\n",
      "Test set: Avg. loss: 7401476.0256, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 112 [0/869 (0%)]\tLoss: 83700088.000000\n",
      "Train Epoch: 112 [160/869 (18%)]\tLoss: 225787616.000000\n",
      "Train Epoch: 112 [320/869 (36%)]\tLoss: 85363024.000000\n",
      "Train Epoch: 112 [480/869 (55%)]\tLoss: 206315728.000000\n",
      "Train Epoch: 112 [640/869 (73%)]\tLoss: 107989432.000000\n",
      "Train Epoch: 112 [800/869 (91%)]\tLoss: 50683756.000000\n",
      "\n",
      "Test set: Avg. loss: 7408588.6908, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 113 [0/869 (0%)]\tLoss: 116221472.000000\n",
      "Train Epoch: 113 [160/869 (18%)]\tLoss: 87765744.000000\n",
      "Train Epoch: 113 [320/869 (36%)]\tLoss: 174202432.000000\n",
      "Train Epoch: 113 [480/869 (55%)]\tLoss: 144415440.000000\n",
      "Train Epoch: 113 [640/869 (73%)]\tLoss: 73314344.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 113 [800/869 (91%)]\tLoss: 126248680.000000\n",
      "\n",
      "Test set: Avg. loss: 7808854.0640, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 114 [0/869 (0%)]\tLoss: 113518848.000000\n",
      "Train Epoch: 114 [160/869 (18%)]\tLoss: 112345888.000000\n",
      "Train Epoch: 114 [320/869 (36%)]\tLoss: 232955712.000000\n",
      "Train Epoch: 114 [480/869 (55%)]\tLoss: 137493920.000000\n",
      "Train Epoch: 114 [640/869 (73%)]\tLoss: 128246472.000000\n",
      "Train Epoch: 114 [800/869 (91%)]\tLoss: 127112128.000000\n",
      "\n",
      "Test set: Avg. loss: 7439599.4200, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 115 [0/869 (0%)]\tLoss: 92455440.000000\n",
      "Train Epoch: 115 [160/869 (18%)]\tLoss: 201746704.000000\n",
      "Train Epoch: 115 [320/869 (36%)]\tLoss: 214156624.000000\n",
      "Train Epoch: 115 [480/869 (55%)]\tLoss: 247071104.000000\n",
      "Train Epoch: 115 [640/869 (73%)]\tLoss: 119008664.000000\n",
      "Train Epoch: 115 [800/869 (91%)]\tLoss: 97504432.000000\n",
      "\n",
      "Test set: Avg. loss: 7533057.2281, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 116 [0/869 (0%)]\tLoss: 94564256.000000\n",
      "Train Epoch: 116 [160/869 (18%)]\tLoss: 137660976.000000\n",
      "Train Epoch: 116 [320/869 (36%)]\tLoss: 92384432.000000\n",
      "Train Epoch: 116 [480/869 (55%)]\tLoss: 100372816.000000\n",
      "Train Epoch: 116 [640/869 (73%)]\tLoss: 79989600.000000\n",
      "Train Epoch: 116 [800/869 (91%)]\tLoss: 198624672.000000\n",
      "\n",
      "Test set: Avg. loss: 7460742.9254, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 117 [0/869 (0%)]\tLoss: 304304576.000000\n",
      "Train Epoch: 117 [160/869 (18%)]\tLoss: 155667824.000000\n",
      "Train Epoch: 117 [320/869 (36%)]\tLoss: 37704032.000000\n",
      "Train Epoch: 117 [480/869 (55%)]\tLoss: 186100976.000000\n",
      "Train Epoch: 117 [640/869 (73%)]\tLoss: 133978904.000000\n",
      "Train Epoch: 117 [800/869 (91%)]\tLoss: 77725504.000000\n",
      "\n",
      "Test set: Avg. loss: 7432535.6333, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 118 [0/869 (0%)]\tLoss: 29429898.000000\n",
      "Train Epoch: 118 [160/869 (18%)]\tLoss: 177265312.000000\n",
      "Train Epoch: 118 [320/869 (36%)]\tLoss: 97728144.000000\n",
      "Train Epoch: 118 [480/869 (55%)]\tLoss: 153105456.000000\n",
      "Train Epoch: 118 [640/869 (73%)]\tLoss: 204543264.000000\n",
      "Train Epoch: 118 [800/869 (91%)]\tLoss: 102629952.000000\n",
      "\n",
      "Test set: Avg. loss: 7485693.4670, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 119 [0/869 (0%)]\tLoss: 167641856.000000\n",
      "Train Epoch: 119 [160/869 (18%)]\tLoss: 40252728.000000\n",
      "Train Epoch: 119 [320/869 (36%)]\tLoss: 156973952.000000\n",
      "Train Epoch: 119 [480/869 (55%)]\tLoss: 51572936.000000\n",
      "Train Epoch: 119 [640/869 (73%)]\tLoss: 97767960.000000\n",
      "Train Epoch: 119 [800/869 (91%)]\tLoss: 276459168.000000\n",
      "\n",
      "Test set: Avg. loss: 7568412.3156, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 120 [0/869 (0%)]\tLoss: 228957824.000000\n",
      "Train Epoch: 120 [160/869 (18%)]\tLoss: 126899248.000000\n",
      "Train Epoch: 120 [320/869 (36%)]\tLoss: 109414360.000000\n",
      "Train Epoch: 120 [480/869 (55%)]\tLoss: 94024280.000000\n",
      "Train Epoch: 120 [640/869 (73%)]\tLoss: 86551496.000000\n",
      "Train Epoch: 120 [800/869 (91%)]\tLoss: 68801464.000000\n",
      "\n",
      "Test set: Avg. loss: 7974746.0896, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 121 [0/869 (0%)]\tLoss: 85320160.000000\n",
      "Train Epoch: 121 [160/869 (18%)]\tLoss: 151969264.000000\n",
      "Train Epoch: 121 [320/869 (36%)]\tLoss: 91577752.000000\n",
      "Train Epoch: 121 [480/869 (55%)]\tLoss: 146699360.000000\n",
      "Train Epoch: 121 [640/869 (73%)]\tLoss: 124487712.000000\n",
      "Train Epoch: 121 [800/869 (91%)]\tLoss: 228664608.000000\n",
      "\n",
      "Test set: Avg. loss: 7424657.3817, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 122 [0/869 (0%)]\tLoss: 86523072.000000\n",
      "Train Epoch: 122 [160/869 (18%)]\tLoss: 124461928.000000\n",
      "Train Epoch: 122 [320/869 (36%)]\tLoss: 39794796.000000\n",
      "Train Epoch: 122 [480/869 (55%)]\tLoss: 197368864.000000\n",
      "Train Epoch: 122 [640/869 (73%)]\tLoss: 180672864.000000\n",
      "Train Epoch: 122 [800/869 (91%)]\tLoss: 147382528.000000\n",
      "\n",
      "Test set: Avg. loss: 7328245.5949, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 123 [0/869 (0%)]\tLoss: 56401784.000000\n",
      "Train Epoch: 123 [160/869 (18%)]\tLoss: 140791904.000000\n",
      "Train Epoch: 123 [320/869 (36%)]\tLoss: 206989296.000000\n",
      "Train Epoch: 123 [480/869 (55%)]\tLoss: 108191696.000000\n",
      "Train Epoch: 123 [640/869 (73%)]\tLoss: 123699472.000000\n",
      "Train Epoch: 123 [800/869 (91%)]\tLoss: 59282808.000000\n",
      "\n",
      "Test set: Avg. loss: 8696890.0128, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 124 [0/869 (0%)]\tLoss: 166779760.000000\n",
      "Train Epoch: 124 [160/869 (18%)]\tLoss: 85236040.000000\n",
      "Train Epoch: 124 [320/869 (36%)]\tLoss: 80925416.000000\n",
      "Train Epoch: 124 [480/869 (55%)]\tLoss: 158805232.000000\n",
      "Train Epoch: 124 [640/869 (73%)]\tLoss: 112055768.000000\n",
      "Train Epoch: 124 [800/869 (91%)]\tLoss: 231070880.000000\n",
      "\n",
      "Test set: Avg. loss: 7309489.2623, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 125 [0/869 (0%)]\tLoss: 153736640.000000\n",
      "Train Epoch: 125 [160/869 (18%)]\tLoss: 107193904.000000\n",
      "Train Epoch: 125 [320/869 (36%)]\tLoss: 91522776.000000\n",
      "Train Epoch: 125 [480/869 (55%)]\tLoss: 58962516.000000\n",
      "Train Epoch: 125 [640/869 (73%)]\tLoss: 92759104.000000\n",
      "Train Epoch: 125 [800/869 (91%)]\tLoss: 146533184.000000\n",
      "\n",
      "Test set: Avg. loss: 7718481.6887, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 126 [0/869 (0%)]\tLoss: 94796904.000000\n",
      "Train Epoch: 126 [160/869 (18%)]\tLoss: 73281680.000000\n",
      "Train Epoch: 126 [320/869 (36%)]\tLoss: 138818176.000000\n",
      "Train Epoch: 126 [480/869 (55%)]\tLoss: 107829680.000000\n",
      "Train Epoch: 126 [640/869 (73%)]\tLoss: 138976480.000000\n",
      "Train Epoch: 126 [800/869 (91%)]\tLoss: 155063408.000000\n",
      "\n",
      "Test set: Avg. loss: 8002473.4328, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 127 [0/869 (0%)]\tLoss: 110409864.000000\n",
      "Train Epoch: 127 [160/869 (18%)]\tLoss: 248566448.000000\n",
      "Train Epoch: 127 [320/869 (36%)]\tLoss: 60048840.000000\n",
      "Train Epoch: 127 [480/869 (55%)]\tLoss: 160648608.000000\n",
      "Train Epoch: 127 [640/869 (73%)]\tLoss: 53531956.000000\n",
      "Train Epoch: 127 [800/869 (91%)]\tLoss: 93243552.000000\n",
      "\n",
      "Test set: Avg. loss: 7304357.9190, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 128 [0/869 (0%)]\tLoss: 316564192.000000\n",
      "Train Epoch: 128 [160/869 (18%)]\tLoss: 234143728.000000\n",
      "Train Epoch: 128 [320/869 (36%)]\tLoss: 135610352.000000\n",
      "Train Epoch: 128 [480/869 (55%)]\tLoss: 90518016.000000\n",
      "Train Epoch: 128 [640/869 (73%)]\tLoss: 80060952.000000\n",
      "Train Epoch: 128 [800/869 (91%)]\tLoss: 119549488.000000\n",
      "\n",
      "Test set: Avg. loss: 7594470.7889, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 129 [0/869 (0%)]\tLoss: 162499152.000000\n",
      "Train Epoch: 129 [160/869 (18%)]\tLoss: 107388984.000000\n",
      "Train Epoch: 129 [320/869 (36%)]\tLoss: 123604224.000000\n",
      "Train Epoch: 129 [480/869 (55%)]\tLoss: 204812464.000000\n",
      "Train Epoch: 129 [640/869 (73%)]\tLoss: 34123540.000000\n",
      "Train Epoch: 129 [800/869 (91%)]\tLoss: 181197472.000000\n",
      "\n",
      "Test set: Avg. loss: 7786898.0000, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 130 [0/869 (0%)]\tLoss: 107506376.000000\n",
      "Train Epoch: 130 [160/869 (18%)]\tLoss: 54524220.000000\n",
      "Train Epoch: 130 [320/869 (36%)]\tLoss: 49614320.000000\n",
      "Train Epoch: 130 [480/869 (55%)]\tLoss: 151403808.000000\n",
      "Train Epoch: 130 [640/869 (73%)]\tLoss: 95486104.000000\n",
      "Train Epoch: 130 [800/869 (91%)]\tLoss: 223344032.000000\n",
      "\n",
      "Test set: Avg. loss: 8931135.0107, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 131 [0/869 (0%)]\tLoss: 164892832.000000\n",
      "Train Epoch: 131 [160/869 (18%)]\tLoss: 226906016.000000\n",
      "Train Epoch: 131 [320/869 (36%)]\tLoss: 415211552.000000\n",
      "Train Epoch: 131 [480/869 (55%)]\tLoss: 121927576.000000\n",
      "Train Epoch: 131 [640/869 (73%)]\tLoss: 38967036.000000\n",
      "Train Epoch: 131 [800/869 (91%)]\tLoss: 301884032.000000\n",
      "\n",
      "Test set: Avg. loss: 7513952.2857, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 132 [0/869 (0%)]\tLoss: 100893992.000000\n",
      "Train Epoch: 132 [160/869 (18%)]\tLoss: 104475280.000000\n",
      "Train Epoch: 132 [320/869 (36%)]\tLoss: 91947096.000000\n",
      "Train Epoch: 132 [480/869 (55%)]\tLoss: 168556192.000000\n",
      "Train Epoch: 132 [640/869 (73%)]\tLoss: 224654016.000000\n",
      "Train Epoch: 132 [800/869 (91%)]\tLoss: 122415936.000000\n",
      "\n",
      "Test set: Avg. loss: 7452496.2303, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 133 [0/869 (0%)]\tLoss: 125141520.000000\n",
      "Train Epoch: 133 [160/869 (18%)]\tLoss: 130637488.000000\n",
      "Train Epoch: 133 [320/869 (36%)]\tLoss: 135009824.000000\n",
      "Train Epoch: 133 [480/869 (55%)]\tLoss: 166809584.000000\n",
      "Train Epoch: 133 [640/869 (73%)]\tLoss: 203699824.000000\n",
      "Train Epoch: 133 [800/869 (91%)]\tLoss: 91865560.000000\n",
      "\n",
      "Test set: Avg. loss: 7348302.7719, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 134 [0/869 (0%)]\tLoss: 61101244.000000\n",
      "Train Epoch: 134 [160/869 (18%)]\tLoss: 77824400.000000\n",
      "Train Epoch: 134 [320/869 (36%)]\tLoss: 43052904.000000\n",
      "Train Epoch: 134 [480/869 (55%)]\tLoss: 219906496.000000\n",
      "Train Epoch: 134 [640/869 (73%)]\tLoss: 70853232.000000\n",
      "Train Epoch: 134 [800/869 (91%)]\tLoss: 235591936.000000\n",
      "\n",
      "Test set: Avg. loss: 7727049.2623, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 135 [0/869 (0%)]\tLoss: 136573392.000000\n",
      "Train Epoch: 135 [160/869 (18%)]\tLoss: 93469696.000000\n",
      "Train Epoch: 135 [320/869 (36%)]\tLoss: 175762176.000000\n",
      "Train Epoch: 135 [480/869 (55%)]\tLoss: 179297328.000000\n",
      "Train Epoch: 135 [640/869 (73%)]\tLoss: 167155920.000000\n",
      "Train Epoch: 135 [800/869 (91%)]\tLoss: 44604712.000000\n",
      "\n",
      "Test set: Avg. loss: 7512060.7591, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 136 [0/869 (0%)]\tLoss: 48323120.000000\n",
      "Train Epoch: 136 [160/869 (18%)]\tLoss: 42299852.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 136 [320/869 (36%)]\tLoss: 38674044.000000\n",
      "Train Epoch: 136 [480/869 (55%)]\tLoss: 248674208.000000\n",
      "Train Epoch: 136 [640/869 (73%)]\tLoss: 118646888.000000\n",
      "Train Epoch: 136 [800/869 (91%)]\tLoss: 90763288.000000\n",
      "\n",
      "Test set: Avg. loss: 7406328.4691, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 137 [0/869 (0%)]\tLoss: 129529840.000000\n",
      "Train Epoch: 137 [160/869 (18%)]\tLoss: 87880024.000000\n",
      "Train Epoch: 137 [320/869 (36%)]\tLoss: 103623864.000000\n",
      "Train Epoch: 137 [480/869 (55%)]\tLoss: 110280480.000000\n",
      "Train Epoch: 137 [640/869 (73%)]\tLoss: 113349392.000000\n",
      "Train Epoch: 137 [800/869 (91%)]\tLoss: 47791156.000000\n",
      "\n",
      "Test set: Avg. loss: 7392792.6482, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 138 [0/869 (0%)]\tLoss: 88217432.000000\n",
      "Train Epoch: 138 [160/869 (18%)]\tLoss: 139421504.000000\n",
      "Train Epoch: 138 [320/869 (36%)]\tLoss: 99692960.000000\n",
      "Train Epoch: 138 [480/869 (55%)]\tLoss: 273547520.000000\n",
      "Train Epoch: 138 [640/869 (73%)]\tLoss: 153123744.000000\n",
      "Train Epoch: 138 [800/869 (91%)]\tLoss: 136966800.000000\n",
      "\n",
      "Test set: Avg. loss: 7561582.8571, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 139 [0/869 (0%)]\tLoss: 35615548.000000\n",
      "Train Epoch: 139 [160/869 (18%)]\tLoss: 222650048.000000\n",
      "Train Epoch: 139 [320/869 (36%)]\tLoss: 133251616.000000\n",
      "Train Epoch: 139 [480/869 (55%)]\tLoss: 143106480.000000\n",
      "Train Epoch: 139 [640/869 (73%)]\tLoss: 266803712.000000\n",
      "Train Epoch: 139 [800/869 (91%)]\tLoss: 200398432.000000\n",
      "\n",
      "Test set: Avg. loss: 7380130.5160, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 140 [0/869 (0%)]\tLoss: 86152528.000000\n",
      "Train Epoch: 140 [160/869 (18%)]\tLoss: 48367520.000000\n",
      "Train Epoch: 140 [320/869 (36%)]\tLoss: 167217408.000000\n",
      "Train Epoch: 140 [480/869 (55%)]\tLoss: 83123248.000000\n",
      "Train Epoch: 140 [640/869 (73%)]\tLoss: 182540464.000000\n",
      "Train Epoch: 140 [800/869 (91%)]\tLoss: 153962816.000000\n",
      "\n",
      "Test set: Avg. loss: 8041448.5458, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 141 [0/869 (0%)]\tLoss: 202057248.000000\n",
      "Train Epoch: 141 [160/869 (18%)]\tLoss: 138129472.000000\n",
      "Train Epoch: 141 [320/869 (36%)]\tLoss: 79304552.000000\n",
      "Train Epoch: 141 [480/869 (55%)]\tLoss: 95290144.000000\n",
      "Train Epoch: 141 [640/869 (73%)]\tLoss: 88867536.000000\n",
      "Train Epoch: 141 [800/869 (91%)]\tLoss: 82738184.000000\n",
      "\n",
      "Test set: Avg. loss: 7432606.0469, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 142 [0/869 (0%)]\tLoss: 93983544.000000\n",
      "Train Epoch: 142 [160/869 (18%)]\tLoss: 245641680.000000\n",
      "Train Epoch: 142 [320/869 (36%)]\tLoss: 91198456.000000\n",
      "Train Epoch: 142 [480/869 (55%)]\tLoss: 101535568.000000\n",
      "Train Epoch: 142 [640/869 (73%)]\tLoss: 75234016.000000\n",
      "Train Epoch: 142 [800/869 (91%)]\tLoss: 33767808.000000\n",
      "\n",
      "Test set: Avg. loss: 7961970.4051, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 143 [0/869 (0%)]\tLoss: 135318832.000000\n",
      "Train Epoch: 143 [160/869 (18%)]\tLoss: 83856240.000000\n",
      "Train Epoch: 143 [320/869 (36%)]\tLoss: 92590224.000000\n",
      "Train Epoch: 143 [480/869 (55%)]\tLoss: 101234224.000000\n",
      "Train Epoch: 143 [640/869 (73%)]\tLoss: 25735262.000000\n",
      "Train Epoch: 143 [800/869 (91%)]\tLoss: 109592080.000000\n",
      "\n",
      "Test set: Avg. loss: 7519880.8358, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 144 [0/869 (0%)]\tLoss: 86570968.000000\n",
      "Train Epoch: 144 [160/869 (18%)]\tLoss: 191831472.000000\n",
      "Train Epoch: 144 [320/869 (36%)]\tLoss: 195023328.000000\n",
      "Train Epoch: 144 [480/869 (55%)]\tLoss: 173132496.000000\n",
      "Train Epoch: 144 [640/869 (73%)]\tLoss: 201372224.000000\n",
      "Train Epoch: 144 [800/869 (91%)]\tLoss: 113915184.000000\n",
      "\n",
      "Test set: Avg. loss: 7645616.0981, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 145 [0/869 (0%)]\tLoss: 80466368.000000\n",
      "Train Epoch: 145 [160/869 (18%)]\tLoss: 124152992.000000\n",
      "Train Epoch: 145 [320/869 (36%)]\tLoss: 105826208.000000\n",
      "Train Epoch: 145 [480/869 (55%)]\tLoss: 45213768.000000\n",
      "Train Epoch: 145 [640/869 (73%)]\tLoss: 107058376.000000\n",
      "Train Epoch: 145 [800/869 (91%)]\tLoss: 165169184.000000\n",
      "\n",
      "Test set: Avg. loss: 7490008.6823, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 146 [0/869 (0%)]\tLoss: 55083072.000000\n",
      "Train Epoch: 146 [160/869 (18%)]\tLoss: 91454272.000000\n",
      "Train Epoch: 146 [320/869 (36%)]\tLoss: 30005408.000000\n",
      "Train Epoch: 146 [480/869 (55%)]\tLoss: 93500256.000000\n",
      "Train Epoch: 146 [640/869 (73%)]\tLoss: 133438312.000000\n",
      "Train Epoch: 146 [800/869 (91%)]\tLoss: 150173136.000000\n",
      "\n",
      "Test set: Avg. loss: 7432355.7868, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 147 [0/869 (0%)]\tLoss: 121745680.000000\n",
      "Train Epoch: 147 [160/869 (18%)]\tLoss: 82766528.000000\n",
      "Train Epoch: 147 [320/869 (36%)]\tLoss: 78227992.000000\n",
      "Train Epoch: 147 [480/869 (55%)]\tLoss: 135761968.000000\n",
      "Train Epoch: 147 [640/869 (73%)]\tLoss: 263241968.000000\n",
      "Train Epoch: 147 [800/869 (91%)]\tLoss: 229248896.000000\n",
      "\n",
      "Test set: Avg. loss: 7380129.2452, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 148 [0/869 (0%)]\tLoss: 130292888.000000\n",
      "Train Epoch: 148 [160/869 (18%)]\tLoss: 222249440.000000\n",
      "Train Epoch: 148 [320/869 (36%)]\tLoss: 90776264.000000\n",
      "Train Epoch: 148 [480/869 (55%)]\tLoss: 224246752.000000\n",
      "Train Epoch: 148 [640/869 (73%)]\tLoss: 94017336.000000\n",
      "Train Epoch: 148 [800/869 (91%)]\tLoss: 217447376.000000\n",
      "\n",
      "Test set: Avg. loss: 7302994.0043, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 149 [0/869 (0%)]\tLoss: 92853008.000000\n",
      "Train Epoch: 149 [160/869 (18%)]\tLoss: 78986984.000000\n",
      "Train Epoch: 149 [320/869 (36%)]\tLoss: 94926768.000000\n",
      "Train Epoch: 149 [480/869 (55%)]\tLoss: 158888496.000000\n",
      "Train Epoch: 149 [640/869 (73%)]\tLoss: 31277674.000000\n",
      "Train Epoch: 149 [800/869 (91%)]\tLoss: 191011072.000000\n",
      "\n",
      "Test set: Avg. loss: 7579816.3070, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 150 [0/869 (0%)]\tLoss: 100090008.000000\n",
      "Train Epoch: 150 [160/869 (18%)]\tLoss: 122636600.000000\n",
      "Train Epoch: 150 [320/869 (36%)]\tLoss: 100144160.000000\n",
      "Train Epoch: 150 [480/869 (55%)]\tLoss: 136052560.000000\n",
      "Train Epoch: 150 [640/869 (73%)]\tLoss: 137406800.000000\n",
      "Train Epoch: 150 [800/869 (91%)]\tLoss: 101292488.000000\n",
      "\n",
      "Test set: Avg. loss: 7508115.9147, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 151 [0/869 (0%)]\tLoss: 207007248.000000\n",
      "Train Epoch: 151 [160/869 (18%)]\tLoss: 23686500.000000\n",
      "Train Epoch: 151 [320/869 (36%)]\tLoss: 94784472.000000\n",
      "Train Epoch: 151 [480/869 (55%)]\tLoss: 41943804.000000\n",
      "Train Epoch: 151 [640/869 (73%)]\tLoss: 177129056.000000\n",
      "Train Epoch: 151 [800/869 (91%)]\tLoss: 227919376.000000\n",
      "\n",
      "Test set: Avg. loss: 7748033.9574, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 152 [0/869 (0%)]\tLoss: 96695704.000000\n",
      "Train Epoch: 152 [160/869 (18%)]\tLoss: 129106592.000000\n",
      "Train Epoch: 152 [320/869 (36%)]\tLoss: 80584592.000000\n",
      "Train Epoch: 152 [480/869 (55%)]\tLoss: 92185336.000000\n",
      "Train Epoch: 152 [640/869 (73%)]\tLoss: 82693664.000000\n",
      "Train Epoch: 152 [800/869 (91%)]\tLoss: 200614464.000000\n",
      "\n",
      "Test set: Avg. loss: 7450112.6738, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 153 [0/869 (0%)]\tLoss: 180062416.000000\n",
      "Train Epoch: 153 [160/869 (18%)]\tLoss: 102155328.000000\n",
      "Train Epoch: 153 [320/869 (36%)]\tLoss: 81841736.000000\n",
      "Train Epoch: 153 [480/869 (55%)]\tLoss: 96274568.000000\n",
      "Train Epoch: 153 [640/869 (73%)]\tLoss: 39573324.000000\n",
      "Train Epoch: 153 [800/869 (91%)]\tLoss: 101641352.000000\n",
      "\n",
      "Test set: Avg. loss: 7580570.3198, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 154 [0/869 (0%)]\tLoss: 148397216.000000\n",
      "Train Epoch: 154 [160/869 (18%)]\tLoss: 145392208.000000\n",
      "Train Epoch: 154 [320/869 (36%)]\tLoss: 68193504.000000\n",
      "Train Epoch: 154 [480/869 (55%)]\tLoss: 133194984.000000\n",
      "Train Epoch: 154 [640/869 (73%)]\tLoss: 80194208.000000\n",
      "Train Epoch: 154 [800/869 (91%)]\tLoss: 263519056.000000\n",
      "\n",
      "Test set: Avg. loss: 7410946.3966, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 155 [0/869 (0%)]\tLoss: 73858504.000000\n",
      "Train Epoch: 155 [160/869 (18%)]\tLoss: 204610688.000000\n",
      "Train Epoch: 155 [320/869 (36%)]\tLoss: 305373856.000000\n",
      "Train Epoch: 155 [480/869 (55%)]\tLoss: 69395480.000000\n",
      "Train Epoch: 155 [640/869 (73%)]\tLoss: 83147296.000000\n",
      "Train Epoch: 155 [800/869 (91%)]\tLoss: 91269768.000000\n",
      "\n",
      "Test set: Avg. loss: 7467681.0746, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 156 [0/869 (0%)]\tLoss: 81596816.000000\n",
      "Train Epoch: 156 [160/869 (18%)]\tLoss: 96920984.000000\n",
      "Train Epoch: 156 [320/869 (36%)]\tLoss: 246579312.000000\n",
      "Train Epoch: 156 [480/869 (55%)]\tLoss: 101236088.000000\n",
      "Train Epoch: 156 [640/869 (73%)]\tLoss: 82316920.000000\n",
      "Train Epoch: 156 [800/869 (91%)]\tLoss: 285142432.000000\n",
      "\n",
      "Test set: Avg. loss: 7512539.9318, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 157 [0/869 (0%)]\tLoss: 131524264.000000\n",
      "Train Epoch: 157 [160/869 (18%)]\tLoss: 122332736.000000\n",
      "Train Epoch: 157 [320/869 (36%)]\tLoss: 229894352.000000\n",
      "Train Epoch: 157 [480/869 (55%)]\tLoss: 93215040.000000\n",
      "Train Epoch: 157 [640/869 (73%)]\tLoss: 79965312.000000\n",
      "Train Epoch: 157 [800/869 (91%)]\tLoss: 71115472.000000\n",
      "\n",
      "Test set: Avg. loss: 7384398.4904, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 158 [0/869 (0%)]\tLoss: 79221104.000000\n",
      "Train Epoch: 158 [160/869 (18%)]\tLoss: 160888320.000000\n",
      "Train Epoch: 158 [320/869 (36%)]\tLoss: 126472816.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 158 [480/869 (55%)]\tLoss: 216008816.000000\n",
      "Train Epoch: 158 [640/869 (73%)]\tLoss: 73718264.000000\n",
      "Train Epoch: 158 [800/869 (91%)]\tLoss: 179426208.000000\n",
      "\n",
      "Test set: Avg. loss: 7364247.7612, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 159 [0/869 (0%)]\tLoss: 85359976.000000\n",
      "Train Epoch: 159 [160/869 (18%)]\tLoss: 282299232.000000\n",
      "Train Epoch: 159 [320/869 (36%)]\tLoss: 144834656.000000\n",
      "Train Epoch: 159 [480/869 (55%)]\tLoss: 192207216.000000\n",
      "Train Epoch: 159 [640/869 (73%)]\tLoss: 132938664.000000\n",
      "Train Epoch: 159 [800/869 (91%)]\tLoss: 175066080.000000\n",
      "\n",
      "Test set: Avg. loss: 7718851.3945, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 160 [0/869 (0%)]\tLoss: 148392064.000000\n",
      "Train Epoch: 160 [160/869 (18%)]\tLoss: 93007792.000000\n",
      "Train Epoch: 160 [320/869 (36%)]\tLoss: 27979628.000000\n",
      "Train Epoch: 160 [480/869 (55%)]\tLoss: 160349376.000000\n",
      "Train Epoch: 160 [640/869 (73%)]\tLoss: 106220520.000000\n",
      "Train Epoch: 160 [800/869 (91%)]\tLoss: 83123496.000000\n",
      "\n",
      "Test set: Avg. loss: 8163219.3774, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 161 [0/869 (0%)]\tLoss: 141359536.000000\n",
      "Train Epoch: 161 [160/869 (18%)]\tLoss: 41812656.000000\n",
      "Train Epoch: 161 [320/869 (36%)]\tLoss: 84837096.000000\n",
      "Train Epoch: 161 [480/869 (55%)]\tLoss: 29196542.000000\n",
      "Train Epoch: 161 [640/869 (73%)]\tLoss: 237621424.000000\n",
      "Train Epoch: 161 [800/869 (91%)]\tLoss: 138688928.000000\n",
      "\n",
      "Test set: Avg. loss: 7718489.4670, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 162 [0/869 (0%)]\tLoss: 87604744.000000\n",
      "Train Epoch: 162 [160/869 (18%)]\tLoss: 114105840.000000\n",
      "Train Epoch: 162 [320/869 (36%)]\tLoss: 261920240.000000\n",
      "Train Epoch: 162 [480/869 (55%)]\tLoss: 173764288.000000\n",
      "Train Epoch: 162 [640/869 (73%)]\tLoss: 35247312.000000\n",
      "Train Epoch: 162 [800/869 (91%)]\tLoss: 134347872.000000\n",
      "\n",
      "Test set: Avg. loss: 7324297.7484, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 163 [0/869 (0%)]\tLoss: 146908304.000000\n",
      "Train Epoch: 163 [160/869 (18%)]\tLoss: 119117176.000000\n",
      "Train Epoch: 163 [320/869 (36%)]\tLoss: 104647440.000000\n",
      "Train Epoch: 163 [480/869 (55%)]\tLoss: 119860840.000000\n",
      "Train Epoch: 163 [640/869 (73%)]\tLoss: 152601312.000000\n",
      "Train Epoch: 163 [800/869 (91%)]\tLoss: 144829264.000000\n",
      "\n",
      "Test set: Avg. loss: 8113200.1962, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 164 [0/869 (0%)]\tLoss: 143638896.000000\n",
      "Train Epoch: 164 [160/869 (18%)]\tLoss: 59744008.000000\n",
      "Train Epoch: 164 [320/869 (36%)]\tLoss: 77042120.000000\n",
      "Train Epoch: 164 [480/869 (55%)]\tLoss: 188642144.000000\n",
      "Train Epoch: 164 [640/869 (73%)]\tLoss: 93799304.000000\n",
      "Train Epoch: 164 [800/869 (91%)]\tLoss: 194348752.000000\n",
      "\n",
      "Test set: Avg. loss: 8071563.4584, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 165 [0/869 (0%)]\tLoss: 244033648.000000\n",
      "Train Epoch: 165 [160/869 (18%)]\tLoss: 230033280.000000\n",
      "Train Epoch: 165 [320/869 (36%)]\tLoss: 207394176.000000\n",
      "Train Epoch: 165 [480/869 (55%)]\tLoss: 132551944.000000\n",
      "Train Epoch: 165 [640/869 (73%)]\tLoss: 81033816.000000\n",
      "Train Epoch: 165 [800/869 (91%)]\tLoss: 25774252.000000\n",
      "\n",
      "Test set: Avg. loss: 7877516.4350, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 166 [0/869 (0%)]\tLoss: 214351328.000000\n",
      "Train Epoch: 166 [160/869 (18%)]\tLoss: 309640032.000000\n",
      "Train Epoch: 166 [320/869 (36%)]\tLoss: 80528136.000000\n",
      "Train Epoch: 166 [480/869 (55%)]\tLoss: 109312416.000000\n",
      "Train Epoch: 166 [640/869 (73%)]\tLoss: 40363704.000000\n",
      "Train Epoch: 166 [800/869 (91%)]\tLoss: 190837616.000000\n",
      "\n",
      "Test set: Avg. loss: 7540645.0917, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 167 [0/869 (0%)]\tLoss: 389121472.000000\n",
      "Train Epoch: 167 [160/869 (18%)]\tLoss: 25580488.000000\n",
      "Train Epoch: 167 [320/869 (36%)]\tLoss: 417463808.000000\n",
      "Train Epoch: 167 [480/869 (55%)]\tLoss: 33109116.000000\n",
      "Train Epoch: 167 [640/869 (73%)]\tLoss: 84124512.000000\n",
      "Train Epoch: 167 [800/869 (91%)]\tLoss: 252232880.000000\n",
      "\n",
      "Test set: Avg. loss: 7569158.4392, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 168 [0/869 (0%)]\tLoss: 97875056.000000\n",
      "Train Epoch: 168 [160/869 (18%)]\tLoss: 41221116.000000\n",
      "Train Epoch: 168 [320/869 (36%)]\tLoss: 95604192.000000\n",
      "Train Epoch: 168 [480/869 (55%)]\tLoss: 205320720.000000\n",
      "Train Epoch: 168 [640/869 (73%)]\tLoss: 149674896.000000\n",
      "Train Epoch: 168 [800/869 (91%)]\tLoss: 195534496.000000\n",
      "\n",
      "Test set: Avg. loss: 7368804.2729, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 169 [0/869 (0%)]\tLoss: 163208672.000000\n",
      "Train Epoch: 169 [160/869 (18%)]\tLoss: 179832352.000000\n",
      "Train Epoch: 169 [320/869 (36%)]\tLoss: 94505368.000000\n",
      "Train Epoch: 169 [480/869 (55%)]\tLoss: 139106048.000000\n",
      "Train Epoch: 169 [640/869 (73%)]\tLoss: 193594032.000000\n",
      "Train Epoch: 169 [800/869 (91%)]\tLoss: 87115608.000000\n",
      "\n",
      "Test set: Avg. loss: 7468847.0618, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 170 [0/869 (0%)]\tLoss: 54466032.000000\n",
      "Train Epoch: 170 [160/869 (18%)]\tLoss: 144466944.000000\n",
      "Train Epoch: 170 [320/869 (36%)]\tLoss: 78006688.000000\n",
      "Train Epoch: 170 [480/869 (55%)]\tLoss: 343733088.000000\n",
      "Train Epoch: 170 [640/869 (73%)]\tLoss: 101352896.000000\n",
      "Train Epoch: 170 [800/869 (91%)]\tLoss: 78606120.000000\n",
      "\n",
      "Test set: Avg. loss: 7317541.2111, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 171 [0/869 (0%)]\tLoss: 61338896.000000\n",
      "Train Epoch: 171 [160/869 (18%)]\tLoss: 161155552.000000\n",
      "Train Epoch: 171 [320/869 (36%)]\tLoss: 142193792.000000\n",
      "Train Epoch: 171 [480/869 (55%)]\tLoss: 180658256.000000\n",
      "Train Epoch: 171 [640/869 (73%)]\tLoss: 151396320.000000\n",
      "Train Epoch: 171 [800/869 (91%)]\tLoss: 140649408.000000\n",
      "\n",
      "Test set: Avg. loss: 7426106.5075, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 172 [0/869 (0%)]\tLoss: 81727656.000000\n",
      "Train Epoch: 172 [160/869 (18%)]\tLoss: 96858112.000000\n",
      "Train Epoch: 172 [320/869 (36%)]\tLoss: 79783304.000000\n",
      "Train Epoch: 172 [480/869 (55%)]\tLoss: 49365072.000000\n",
      "Train Epoch: 172 [640/869 (73%)]\tLoss: 199041424.000000\n",
      "Train Epoch: 172 [800/869 (91%)]\tLoss: 67601816.000000\n",
      "\n",
      "Test set: Avg. loss: 7308342.2004, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 173 [0/869 (0%)]\tLoss: 155798896.000000\n",
      "Train Epoch: 173 [160/869 (18%)]\tLoss: 88561944.000000\n",
      "Train Epoch: 173 [320/869 (36%)]\tLoss: 55272392.000000\n",
      "Train Epoch: 173 [480/869 (55%)]\tLoss: 45292584.000000\n",
      "Train Epoch: 173 [640/869 (73%)]\tLoss: 125449528.000000\n",
      "Train Epoch: 173 [800/869 (91%)]\tLoss: 106938656.000000\n",
      "\n",
      "Test set: Avg. loss: 7524600.9979, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 174 [0/869 (0%)]\tLoss: 56694512.000000\n",
      "Train Epoch: 174 [160/869 (18%)]\tLoss: 161145920.000000\n",
      "Train Epoch: 174 [320/869 (36%)]\tLoss: 203089712.000000\n",
      "Train Epoch: 174 [480/869 (55%)]\tLoss: 111665712.000000\n",
      "Train Epoch: 174 [640/869 (73%)]\tLoss: 143068848.000000\n",
      "Train Epoch: 174 [800/869 (91%)]\tLoss: 126388104.000000\n",
      "\n",
      "Test set: Avg. loss: 7504245.4499, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 175 [0/869 (0%)]\tLoss: 160721008.000000\n",
      "Train Epoch: 175 [160/869 (18%)]\tLoss: 56576852.000000\n",
      "Train Epoch: 175 [320/869 (36%)]\tLoss: 101449888.000000\n",
      "Train Epoch: 175 [480/869 (55%)]\tLoss: 129870480.000000\n",
      "Train Epoch: 175 [640/869 (73%)]\tLoss: 134659264.000000\n",
      "Train Epoch: 175 [800/869 (91%)]\tLoss: 40777464.000000\n",
      "\n",
      "Test set: Avg. loss: 7809421.3817, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 176 [0/869 (0%)]\tLoss: 146909792.000000\n",
      "Train Epoch: 176 [160/869 (18%)]\tLoss: 99397056.000000\n",
      "Train Epoch: 176 [320/869 (36%)]\tLoss: 162233840.000000\n",
      "Train Epoch: 176 [480/869 (55%)]\tLoss: 292299232.000000\n",
      "Train Epoch: 176 [640/869 (73%)]\tLoss: 174252448.000000\n",
      "Train Epoch: 176 [800/869 (91%)]\tLoss: 117498992.000000\n",
      "\n",
      "Test set: Avg. loss: 7467547.3262, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 177 [0/869 (0%)]\tLoss: 61110040.000000\n",
      "Train Epoch: 177 [160/869 (18%)]\tLoss: 81495704.000000\n",
      "Train Epoch: 177 [320/869 (36%)]\tLoss: 195037392.000000\n",
      "Train Epoch: 177 [480/869 (55%)]\tLoss: 157103008.000000\n",
      "Train Epoch: 177 [640/869 (73%)]\tLoss: 48119612.000000\n",
      "Train Epoch: 177 [800/869 (91%)]\tLoss: 127856144.000000\n",
      "\n",
      "Test set: Avg. loss: 7318528.0256, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 178 [0/869 (0%)]\tLoss: 148660672.000000\n",
      "Train Epoch: 178 [160/869 (18%)]\tLoss: 167784752.000000\n",
      "Train Epoch: 178 [320/869 (36%)]\tLoss: 55603044.000000\n",
      "Train Epoch: 178 [480/869 (55%)]\tLoss: 84089120.000000\n",
      "Train Epoch: 178 [640/869 (73%)]\tLoss: 71572664.000000\n",
      "Train Epoch: 178 [800/869 (91%)]\tLoss: 194653264.000000\n",
      "\n",
      "Test set: Avg. loss: 7389408.9638, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 179 [0/869 (0%)]\tLoss: 149023968.000000\n",
      "Train Epoch: 179 [160/869 (18%)]\tLoss: 93638464.000000\n",
      "Train Epoch: 179 [320/869 (36%)]\tLoss: 123376760.000000\n",
      "Train Epoch: 179 [480/869 (55%)]\tLoss: 136803616.000000\n",
      "Train Epoch: 179 [640/869 (73%)]\tLoss: 153187360.000000\n",
      "Train Epoch: 179 [800/869 (91%)]\tLoss: 94909632.000000\n",
      "\n",
      "Test set: Avg. loss: 7450941.1599, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 180 [0/869 (0%)]\tLoss: 134281232.000000\n",
      "Train Epoch: 180 [160/869 (18%)]\tLoss: 135716416.000000\n",
      "Train Epoch: 180 [320/869 (36%)]\tLoss: 139827616.000000\n",
      "Train Epoch: 180 [480/869 (55%)]\tLoss: 172928112.000000\n",
      "Train Epoch: 180 [640/869 (73%)]\tLoss: 77864992.000000\n",
      "Train Epoch: 180 [800/869 (91%)]\tLoss: 106101656.000000\n",
      "\n",
      "Test set: Avg. loss: 7852102.5032, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 181 [0/869 (0%)]\tLoss: 202448768.000000\n",
      "Train Epoch: 181 [160/869 (18%)]\tLoss: 206349216.000000\n",
      "Train Epoch: 181 [320/869 (36%)]\tLoss: 27911594.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 181 [480/869 (55%)]\tLoss: 193031520.000000\n",
      "Train Epoch: 181 [640/869 (73%)]\tLoss: 122818912.000000\n",
      "Train Epoch: 181 [800/869 (91%)]\tLoss: 81206304.000000\n",
      "\n",
      "Test set: Avg. loss: 7554250.1493, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 182 [0/869 (0%)]\tLoss: 130328040.000000\n",
      "Train Epoch: 182 [160/869 (18%)]\tLoss: 186697872.000000\n",
      "Train Epoch: 182 [320/869 (36%)]\tLoss: 302335296.000000\n",
      "Train Epoch: 182 [480/869 (55%)]\tLoss: 162629600.000000\n",
      "Train Epoch: 182 [640/869 (73%)]\tLoss: 43228148.000000\n",
      "Train Epoch: 182 [800/869 (91%)]\tLoss: 129462016.000000\n",
      "\n",
      "Test set: Avg. loss: 7420928.2388, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 183 [0/869 (0%)]\tLoss: 142397472.000000\n",
      "Train Epoch: 183 [160/869 (18%)]\tLoss: 87084696.000000\n",
      "Train Epoch: 183 [320/869 (36%)]\tLoss: 188404112.000000\n",
      "Train Epoch: 183 [480/869 (55%)]\tLoss: 62715560.000000\n",
      "Train Epoch: 183 [640/869 (73%)]\tLoss: 140763392.000000\n",
      "Train Epoch: 183 [800/869 (91%)]\tLoss: 166532160.000000\n",
      "\n",
      "Test set: Avg. loss: 7394941.1045, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 184 [0/869 (0%)]\tLoss: 79748760.000000\n",
      "Train Epoch: 184 [160/869 (18%)]\tLoss: 160148240.000000\n",
      "Train Epoch: 184 [320/869 (36%)]\tLoss: 75875632.000000\n",
      "Train Epoch: 184 [480/869 (55%)]\tLoss: 127064064.000000\n",
      "Train Epoch: 184 [640/869 (73%)]\tLoss: 142579408.000000\n",
      "Train Epoch: 184 [800/869 (91%)]\tLoss: 236097808.000000\n",
      "\n",
      "Test set: Avg. loss: 7422180.3838, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 185 [0/869 (0%)]\tLoss: 148310000.000000\n",
      "Train Epoch: 185 [160/869 (18%)]\tLoss: 53258892.000000\n",
      "Train Epoch: 185 [320/869 (36%)]\tLoss: 82535240.000000\n",
      "Train Epoch: 185 [480/869 (55%)]\tLoss: 41063624.000000\n",
      "Train Epoch: 185 [640/869 (73%)]\tLoss: 119984144.000000\n",
      "Train Epoch: 185 [800/869 (91%)]\tLoss: 54319948.000000\n",
      "\n",
      "Test set: Avg. loss: 7431668.8443, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 186 [0/869 (0%)]\tLoss: 103755232.000000\n",
      "Train Epoch: 186 [160/869 (18%)]\tLoss: 57401752.000000\n",
      "Train Epoch: 186 [320/869 (36%)]\tLoss: 150192672.000000\n",
      "Train Epoch: 186 [480/869 (55%)]\tLoss: 173554112.000000\n",
      "Train Epoch: 186 [640/869 (73%)]\tLoss: 82240984.000000\n",
      "Train Epoch: 186 [800/869 (91%)]\tLoss: 88776064.000000\n",
      "\n",
      "Test set: Avg. loss: 7304249.7313, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 187 [0/869 (0%)]\tLoss: 94362848.000000\n",
      "Train Epoch: 187 [160/869 (18%)]\tLoss: 115078488.000000\n",
      "Train Epoch: 187 [320/869 (36%)]\tLoss: 369317344.000000\n",
      "Train Epoch: 187 [480/869 (55%)]\tLoss: 92997112.000000\n",
      "Train Epoch: 187 [640/869 (73%)]\tLoss: 236148768.000000\n",
      "Train Epoch: 187 [800/869 (91%)]\tLoss: 95091936.000000\n",
      "\n",
      "Test set: Avg. loss: 7526694.9382, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 188 [0/869 (0%)]\tLoss: 144618656.000000\n",
      "Train Epoch: 188 [160/869 (18%)]\tLoss: 272281632.000000\n",
      "Train Epoch: 188 [320/869 (36%)]\tLoss: 235324096.000000\n",
      "Train Epoch: 188 [480/869 (55%)]\tLoss: 75197952.000000\n",
      "Train Epoch: 188 [640/869 (73%)]\tLoss: 39120416.000000\n",
      "Train Epoch: 188 [800/869 (91%)]\tLoss: 131976856.000000\n",
      "\n",
      "Test set: Avg. loss: 7401649.2367, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 189 [0/869 (0%)]\tLoss: 86022128.000000\n",
      "Train Epoch: 189 [160/869 (18%)]\tLoss: 100423008.000000\n",
      "Train Epoch: 189 [320/869 (36%)]\tLoss: 133704984.000000\n",
      "Train Epoch: 189 [480/869 (55%)]\tLoss: 190735728.000000\n",
      "Train Epoch: 189 [640/869 (73%)]\tLoss: 80073312.000000\n",
      "Train Epoch: 189 [800/869 (91%)]\tLoss: 92537720.000000\n",
      "\n",
      "Test set: Avg. loss: 7780346.3795, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 190 [0/869 (0%)]\tLoss: 49973276.000000\n",
      "Train Epoch: 190 [160/869 (18%)]\tLoss: 194301968.000000\n",
      "Train Epoch: 190 [320/869 (36%)]\tLoss: 33992704.000000\n",
      "Train Epoch: 190 [480/869 (55%)]\tLoss: 44453324.000000\n",
      "Train Epoch: 190 [640/869 (73%)]\tLoss: 91977920.000000\n",
      "Train Epoch: 190 [800/869 (91%)]\tLoss: 140201552.000000\n",
      "\n",
      "Test set: Avg. loss: 7538829.4414, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 191 [0/869 (0%)]\tLoss: 195994048.000000\n",
      "Train Epoch: 191 [160/869 (18%)]\tLoss: 47014456.000000\n",
      "Train Epoch: 191 [320/869 (36%)]\tLoss: 100702888.000000\n",
      "Train Epoch: 191 [480/869 (55%)]\tLoss: 95798688.000000\n",
      "Train Epoch: 191 [640/869 (73%)]\tLoss: 246924480.000000\n",
      "Train Epoch: 191 [800/869 (91%)]\tLoss: 114862624.000000\n",
      "\n",
      "Test set: Avg. loss: 7522496.5885, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 192 [0/869 (0%)]\tLoss: 181050960.000000\n",
      "Train Epoch: 192 [160/869 (18%)]\tLoss: 102619312.000000\n",
      "Train Epoch: 192 [320/869 (36%)]\tLoss: 90350664.000000\n",
      "Train Epoch: 192 [480/869 (55%)]\tLoss: 88286488.000000\n",
      "Train Epoch: 192 [640/869 (73%)]\tLoss: 139832000.000000\n",
      "Train Epoch: 192 [800/869 (91%)]\tLoss: 125792664.000000\n",
      "\n",
      "Test set: Avg. loss: 7824422.1919, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 193 [0/869 (0%)]\tLoss: 207180304.000000\n",
      "Train Epoch: 193 [160/869 (18%)]\tLoss: 106891688.000000\n",
      "Train Epoch: 193 [320/869 (36%)]\tLoss: 240592416.000000\n",
      "Train Epoch: 193 [480/869 (55%)]\tLoss: 170958528.000000\n",
      "Train Epoch: 193 [640/869 (73%)]\tLoss: 102846824.000000\n",
      "Train Epoch: 193 [800/869 (91%)]\tLoss: 102607408.000000\n",
      "\n",
      "Test set: Avg. loss: 7667804.7846, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 194 [0/869 (0%)]\tLoss: 74924456.000000\n",
      "Train Epoch: 194 [160/869 (18%)]\tLoss: 36530232.000000\n",
      "Train Epoch: 194 [320/869 (36%)]\tLoss: 131356400.000000\n",
      "Train Epoch: 194 [480/869 (55%)]\tLoss: 274341760.000000\n",
      "Train Epoch: 194 [640/869 (73%)]\tLoss: 308228864.000000\n",
      "Train Epoch: 194 [800/869 (91%)]\tLoss: 92800272.000000\n",
      "\n",
      "Test set: Avg. loss: 7471353.8849, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 195 [0/869 (0%)]\tLoss: 83244560.000000\n",
      "Train Epoch: 195 [160/869 (18%)]\tLoss: 125072664.000000\n",
      "Train Epoch: 195 [320/869 (36%)]\tLoss: 261216688.000000\n",
      "Train Epoch: 195 [480/869 (55%)]\tLoss: 114358120.000000\n",
      "Train Epoch: 195 [640/869 (73%)]\tLoss: 198249536.000000\n",
      "Train Epoch: 195 [800/869 (91%)]\tLoss: 89572528.000000\n",
      "\n",
      "Test set: Avg. loss: 7334675.7356, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 196 [0/869 (0%)]\tLoss: 171887744.000000\n",
      "Train Epoch: 196 [160/869 (18%)]\tLoss: 80821096.000000\n",
      "Train Epoch: 196 [320/869 (36%)]\tLoss: 134742576.000000\n",
      "Train Epoch: 196 [480/869 (55%)]\tLoss: 209696736.000000\n",
      "Train Epoch: 196 [640/869 (73%)]\tLoss: 75131304.000000\n",
      "Train Epoch: 196 [800/869 (91%)]\tLoss: 110327664.000000\n",
      "\n",
      "Test set: Avg. loss: 7576432.3497, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 197 [0/869 (0%)]\tLoss: 35901156.000000\n",
      "Train Epoch: 197 [160/869 (18%)]\tLoss: 333036576.000000\n",
      "Train Epoch: 197 [320/869 (36%)]\tLoss: 31678292.000000\n",
      "Train Epoch: 197 [480/869 (55%)]\tLoss: 143497312.000000\n",
      "Train Epoch: 197 [640/869 (73%)]\tLoss: 91770136.000000\n",
      "Train Epoch: 197 [800/869 (91%)]\tLoss: 168753520.000000\n",
      "\n",
      "Test set: Avg. loss: 7519411.6333, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 198 [0/869 (0%)]\tLoss: 234017184.000000\n",
      "Train Epoch: 198 [160/869 (18%)]\tLoss: 134183072.000000\n",
      "Train Epoch: 198 [320/869 (36%)]\tLoss: 292839520.000000\n",
      "Train Epoch: 198 [480/869 (55%)]\tLoss: 155238176.000000\n",
      "Train Epoch: 198 [640/869 (73%)]\tLoss: 120139440.000000\n",
      "Train Epoch: 198 [800/869 (91%)]\tLoss: 23698642.000000\n",
      "\n",
      "Test set: Avg. loss: 7758924.4264, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 199 [0/869 (0%)]\tLoss: 73822064.000000\n",
      "Train Epoch: 199 [160/869 (18%)]\tLoss: 160857936.000000\n",
      "Train Epoch: 199 [320/869 (36%)]\tLoss: 74079088.000000\n",
      "Train Epoch: 199 [480/869 (55%)]\tLoss: 141254976.000000\n",
      "Train Epoch: 199 [640/869 (73%)]\tLoss: 115215472.000000\n",
      "Train Epoch: 199 [800/869 (91%)]\tLoss: 94475624.000000\n",
      "\n",
      "Test set: Avg. loss: 7320165.8849, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 200 [0/869 (0%)]\tLoss: 177060080.000000\n",
      "Train Epoch: 200 [160/869 (18%)]\tLoss: 138581136.000000\n",
      "Train Epoch: 200 [320/869 (36%)]\tLoss: 157663216.000000\n",
      "Train Epoch: 200 [480/869 (55%)]\tLoss: 143952032.000000\n",
      "Train Epoch: 200 [640/869 (73%)]\tLoss: 74309896.000000\n",
      "Train Epoch: 200 [800/869 (91%)]\tLoss: 73122064.000000\n",
      "\n",
      "Test set: Avg. loss: 7717911.0192, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 201 [0/869 (0%)]\tLoss: 54313464.000000\n",
      "Train Epoch: 201 [160/869 (18%)]\tLoss: 163891680.000000\n",
      "Train Epoch: 201 [320/869 (36%)]\tLoss: 102339424.000000\n",
      "Train Epoch: 201 [480/869 (55%)]\tLoss: 81872816.000000\n",
      "Train Epoch: 201 [640/869 (73%)]\tLoss: 86552976.000000\n",
      "Train Epoch: 201 [800/869 (91%)]\tLoss: 299627552.000000\n",
      "\n",
      "Test set: Avg. loss: 7339199.7186, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 202 [0/869 (0%)]\tLoss: 150773968.000000\n",
      "Train Epoch: 202 [160/869 (18%)]\tLoss: 85642528.000000\n",
      "Train Epoch: 202 [320/869 (36%)]\tLoss: 127321792.000000\n",
      "Train Epoch: 202 [480/869 (55%)]\tLoss: 87550472.000000\n",
      "Train Epoch: 202 [640/869 (73%)]\tLoss: 235726992.000000\n",
      "Train Epoch: 202 [800/869 (91%)]\tLoss: 57167172.000000\n",
      "\n",
      "Test set: Avg. loss: 7333733.9360, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 203 [0/869 (0%)]\tLoss: 116243240.000000\n",
      "Train Epoch: 203 [160/869 (18%)]\tLoss: 180576448.000000\n",
      "Train Epoch: 203 [320/869 (36%)]\tLoss: 176280352.000000\n",
      "Train Epoch: 203 [480/869 (55%)]\tLoss: 177764160.000000\n",
      "Train Epoch: 203 [640/869 (73%)]\tLoss: 170562800.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 203 [800/869 (91%)]\tLoss: 132068288.000000\n",
      "\n",
      "Test set: Avg. loss: 7757511.4200, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 204 [0/869 (0%)]\tLoss: 79325608.000000\n",
      "Train Epoch: 204 [160/869 (18%)]\tLoss: 240594112.000000\n",
      "Train Epoch: 204 [320/869 (36%)]\tLoss: 138743504.000000\n",
      "Train Epoch: 204 [480/869 (55%)]\tLoss: 184948480.000000\n",
      "Train Epoch: 204 [640/869 (73%)]\tLoss: 77541536.000000\n",
      "Train Epoch: 204 [800/869 (91%)]\tLoss: 209264720.000000\n",
      "\n",
      "Test set: Avg. loss: 7464103.6588, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 205 [0/869 (0%)]\tLoss: 87898024.000000\n",
      "Train Epoch: 205 [160/869 (18%)]\tLoss: 156791312.000000\n",
      "Train Epoch: 205 [320/869 (36%)]\tLoss: 147826528.000000\n",
      "Train Epoch: 205 [480/869 (55%)]\tLoss: 79453080.000000\n",
      "Train Epoch: 205 [640/869 (73%)]\tLoss: 36913440.000000\n",
      "Train Epoch: 205 [800/869 (91%)]\tLoss: 78910464.000000\n",
      "\n",
      "Test set: Avg. loss: 7544514.2004, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 206 [0/869 (0%)]\tLoss: 128538304.000000\n",
      "Train Epoch: 206 [160/869 (18%)]\tLoss: 105701432.000000\n",
      "Train Epoch: 206 [320/869 (36%)]\tLoss: 136999456.000000\n",
      "Train Epoch: 206 [480/869 (55%)]\tLoss: 135033888.000000\n",
      "Train Epoch: 206 [640/869 (73%)]\tLoss: 180916992.000000\n",
      "Train Epoch: 206 [800/869 (91%)]\tLoss: 149085680.000000\n",
      "\n",
      "Test set: Avg. loss: 7331287.8294, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 207 [0/869 (0%)]\tLoss: 94784712.000000\n",
      "Train Epoch: 207 [160/869 (18%)]\tLoss: 92874136.000000\n",
      "Train Epoch: 207 [320/869 (36%)]\tLoss: 126109840.000000\n",
      "Train Epoch: 207 [480/869 (55%)]\tLoss: 102237728.000000\n",
      "Train Epoch: 207 [640/869 (73%)]\tLoss: 160282288.000000\n",
      "Train Epoch: 207 [800/869 (91%)]\tLoss: 96379736.000000\n",
      "\n",
      "Test set: Avg. loss: 7691917.5096, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 208 [0/869 (0%)]\tLoss: 143258704.000000\n",
      "Train Epoch: 208 [160/869 (18%)]\tLoss: 83124432.000000\n",
      "Train Epoch: 208 [320/869 (36%)]\tLoss: 53218052.000000\n",
      "Train Epoch: 208 [480/869 (55%)]\tLoss: 41398864.000000\n",
      "Train Epoch: 208 [640/869 (73%)]\tLoss: 102132016.000000\n",
      "Train Epoch: 208 [800/869 (91%)]\tLoss: 425220000.000000\n",
      "\n",
      "Test set: Avg. loss: 7728398.4733, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 209 [0/869 (0%)]\tLoss: 121415312.000000\n",
      "Train Epoch: 209 [160/869 (18%)]\tLoss: 90691640.000000\n",
      "Train Epoch: 209 [320/869 (36%)]\tLoss: 144195232.000000\n",
      "Train Epoch: 209 [480/869 (55%)]\tLoss: 84109168.000000\n",
      "Train Epoch: 209 [640/869 (73%)]\tLoss: 93017752.000000\n",
      "Train Epoch: 209 [800/869 (91%)]\tLoss: 158325760.000000\n",
      "\n",
      "Test set: Avg. loss: 7347476.2644, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 210 [0/869 (0%)]\tLoss: 53012776.000000\n",
      "Train Epoch: 210 [160/869 (18%)]\tLoss: 131348968.000000\n",
      "Train Epoch: 210 [320/869 (36%)]\tLoss: 54580372.000000\n",
      "Train Epoch: 210 [480/869 (55%)]\tLoss: 196638112.000000\n",
      "Train Epoch: 210 [640/869 (73%)]\tLoss: 191129760.000000\n",
      "Train Epoch: 210 [800/869 (91%)]\tLoss: 96396688.000000\n",
      "\n",
      "Test set: Avg. loss: 7354168.3497, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 211 [0/869 (0%)]\tLoss: 153194240.000000\n",
      "Train Epoch: 211 [160/869 (18%)]\tLoss: 83516056.000000\n",
      "Train Epoch: 211 [320/869 (36%)]\tLoss: 149243616.000000\n",
      "Train Epoch: 211 [480/869 (55%)]\tLoss: 137797104.000000\n",
      "Train Epoch: 211 [640/869 (73%)]\tLoss: 81162920.000000\n",
      "Train Epoch: 211 [800/869 (91%)]\tLoss: 70370232.000000\n",
      "\n",
      "Test set: Avg. loss: 7793699.4328, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 212 [0/869 (0%)]\tLoss: 232546496.000000\n",
      "Train Epoch: 212 [160/869 (18%)]\tLoss: 112193880.000000\n",
      "Train Epoch: 212 [320/869 (36%)]\tLoss: 174742496.000000\n",
      "Train Epoch: 212 [480/869 (55%)]\tLoss: 124705352.000000\n",
      "Train Epoch: 212 [640/869 (73%)]\tLoss: 38648556.000000\n",
      "Train Epoch: 212 [800/869 (91%)]\tLoss: 79771720.000000\n",
      "\n",
      "Test set: Avg. loss: 7331121.2793, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 213 [0/869 (0%)]\tLoss: 152009216.000000\n",
      "Train Epoch: 213 [160/869 (18%)]\tLoss: 154404464.000000\n",
      "Train Epoch: 213 [320/869 (36%)]\tLoss: 77879760.000000\n",
      "Train Epoch: 213 [480/869 (55%)]\tLoss: 80191312.000000\n",
      "Train Epoch: 213 [640/869 (73%)]\tLoss: 139932880.000000\n",
      "Train Epoch: 213 [800/869 (91%)]\tLoss: 115944512.000000\n",
      "\n",
      "Test set: Avg. loss: 7472955.4286, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 214 [0/869 (0%)]\tLoss: 245424768.000000\n",
      "Train Epoch: 214 [160/869 (18%)]\tLoss: 102704976.000000\n",
      "Train Epoch: 214 [320/869 (36%)]\tLoss: 48800904.000000\n",
      "Train Epoch: 214 [480/869 (55%)]\tLoss: 32124800.000000\n",
      "Train Epoch: 214 [640/869 (73%)]\tLoss: 40466304.000000\n",
      "Train Epoch: 214 [800/869 (91%)]\tLoss: 198916992.000000\n",
      "\n",
      "Test set: Avg. loss: 7379807.2751, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 215 [0/869 (0%)]\tLoss: 115224808.000000\n",
      "Train Epoch: 215 [160/869 (18%)]\tLoss: 192542768.000000\n",
      "Train Epoch: 215 [320/869 (36%)]\tLoss: 77967056.000000\n",
      "Train Epoch: 215 [480/869 (55%)]\tLoss: 171937312.000000\n",
      "Train Epoch: 215 [640/869 (73%)]\tLoss: 144470064.000000\n",
      "Train Epoch: 215 [800/869 (91%)]\tLoss: 62437412.000000\n",
      "\n",
      "Test set: Avg. loss: 7521233.9787, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 216 [0/869 (0%)]\tLoss: 28092284.000000\n",
      "Train Epoch: 216 [160/869 (18%)]\tLoss: 40925084.000000\n",
      "Train Epoch: 216 [320/869 (36%)]\tLoss: 148254944.000000\n",
      "Train Epoch: 216 [480/869 (55%)]\tLoss: 88502088.000000\n",
      "Train Epoch: 216 [640/869 (73%)]\tLoss: 125512560.000000\n",
      "Train Epoch: 216 [800/869 (91%)]\tLoss: 123709808.000000\n",
      "\n",
      "Test set: Avg. loss: 7638642.5032, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 217 [0/869 (0%)]\tLoss: 159138864.000000\n",
      "Train Epoch: 217 [160/869 (18%)]\tLoss: 264115072.000000\n",
      "Train Epoch: 217 [320/869 (36%)]\tLoss: 153172784.000000\n",
      "Train Epoch: 217 [480/869 (55%)]\tLoss: 242897664.000000\n",
      "Train Epoch: 217 [640/869 (73%)]\tLoss: 98750712.000000\n",
      "Train Epoch: 217 [800/869 (91%)]\tLoss: 120215744.000000\n",
      "\n",
      "Test set: Avg. loss: 7554743.4883, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 218 [0/869 (0%)]\tLoss: 110912552.000000\n",
      "Train Epoch: 218 [160/869 (18%)]\tLoss: 34138276.000000\n",
      "Train Epoch: 218 [320/869 (36%)]\tLoss: 159669040.000000\n",
      "Train Epoch: 218 [480/869 (55%)]\tLoss: 84736336.000000\n",
      "Train Epoch: 218 [640/869 (73%)]\tLoss: 255243424.000000\n",
      "Train Epoch: 218 [800/869 (91%)]\tLoss: 91516808.000000\n",
      "\n",
      "Test set: Avg. loss: 7868110.0128, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 219 [0/869 (0%)]\tLoss: 43818676.000000\n",
      "Train Epoch: 219 [160/869 (18%)]\tLoss: 95427832.000000\n",
      "Train Epoch: 219 [320/869 (36%)]\tLoss: 122198880.000000\n",
      "Train Epoch: 219 [480/869 (55%)]\tLoss: 302060576.000000\n",
      "Train Epoch: 219 [640/869 (73%)]\tLoss: 92623552.000000\n",
      "Train Epoch: 219 [800/869 (91%)]\tLoss: 89252144.000000\n",
      "\n",
      "Test set: Avg. loss: 7443298.8913, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 220 [0/869 (0%)]\tLoss: 118336968.000000\n",
      "Train Epoch: 220 [160/869 (18%)]\tLoss: 259360272.000000\n",
      "Train Epoch: 220 [320/869 (36%)]\tLoss: 119682920.000000\n",
      "Train Epoch: 220 [480/869 (55%)]\tLoss: 99261368.000000\n",
      "Train Epoch: 220 [640/869 (73%)]\tLoss: 345361408.000000\n",
      "Train Epoch: 220 [800/869 (91%)]\tLoss: 167513136.000000\n",
      "\n",
      "Test set: Avg. loss: 7317192.9893, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 221 [0/869 (0%)]\tLoss: 183175744.000000\n",
      "Train Epoch: 221 [160/869 (18%)]\tLoss: 227374640.000000\n",
      "Train Epoch: 221 [320/869 (36%)]\tLoss: 81638152.000000\n",
      "Train Epoch: 221 [480/869 (55%)]\tLoss: 140097744.000000\n",
      "Train Epoch: 221 [640/869 (73%)]\tLoss: 211286912.000000\n",
      "Train Epoch: 221 [800/869 (91%)]\tLoss: 51702912.000000\n",
      "\n",
      "Test set: Avg. loss: 7449456.5800, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 222 [0/869 (0%)]\tLoss: 109457248.000000\n",
      "Train Epoch: 222 [160/869 (18%)]\tLoss: 134618480.000000\n",
      "Train Epoch: 222 [320/869 (36%)]\tLoss: 150855536.000000\n",
      "Train Epoch: 222 [480/869 (55%)]\tLoss: 125982096.000000\n",
      "Train Epoch: 222 [640/869 (73%)]\tLoss: 94961328.000000\n",
      "Train Epoch: 222 [800/869 (91%)]\tLoss: 91813336.000000\n",
      "\n",
      "Test set: Avg. loss: 7788582.0725, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 223 [0/869 (0%)]\tLoss: 40098000.000000\n",
      "Train Epoch: 223 [160/869 (18%)]\tLoss: 140094640.000000\n",
      "Train Epoch: 223 [320/869 (36%)]\tLoss: 143342544.000000\n",
      "Train Epoch: 223 [480/869 (55%)]\tLoss: 160049744.000000\n",
      "Train Epoch: 223 [640/869 (73%)]\tLoss: 46095996.000000\n",
      "Train Epoch: 223 [800/869 (91%)]\tLoss: 86020488.000000\n",
      "\n",
      "Test set: Avg. loss: 7856260.7079, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 224 [0/869 (0%)]\tLoss: 207043712.000000\n",
      "Train Epoch: 224 [160/869 (18%)]\tLoss: 25657476.000000\n",
      "Train Epoch: 224 [320/869 (36%)]\tLoss: 255552576.000000\n",
      "Train Epoch: 224 [480/869 (55%)]\tLoss: 168188560.000000\n",
      "Train Epoch: 224 [640/869 (73%)]\tLoss: 106321784.000000\n",
      "Train Epoch: 224 [800/869 (91%)]\tLoss: 126983680.000000\n",
      "\n",
      "Test set: Avg. loss: 7509526.6525, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 225 [0/869 (0%)]\tLoss: 180122272.000000\n",
      "Train Epoch: 225 [160/869 (18%)]\tLoss: 138329520.000000\n",
      "Train Epoch: 225 [320/869 (36%)]\tLoss: 294728448.000000\n",
      "Train Epoch: 225 [480/869 (55%)]\tLoss: 112488624.000000\n",
      "Train Epoch: 225 [640/869 (73%)]\tLoss: 150803232.000000\n",
      "Train Epoch: 225 [800/869 (91%)]\tLoss: 90819640.000000\n",
      "\n",
      "Test set: Avg. loss: 8393400.3539, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 226 [0/869 (0%)]\tLoss: 93053864.000000\n",
      "Train Epoch: 226 [160/869 (18%)]\tLoss: 73519792.000000\n",
      "Train Epoch: 226 [320/869 (36%)]\tLoss: 149721184.000000\n",
      "Train Epoch: 226 [480/869 (55%)]\tLoss: 88363680.000000\n",
      "Train Epoch: 226 [640/869 (73%)]\tLoss: 53992996.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 226 [800/869 (91%)]\tLoss: 144123680.000000\n",
      "\n",
      "Test set: Avg. loss: 7594508.7761, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 227 [0/869 (0%)]\tLoss: 132850960.000000\n",
      "Train Epoch: 227 [160/869 (18%)]\tLoss: 154958656.000000\n",
      "Train Epoch: 227 [320/869 (36%)]\tLoss: 432075424.000000\n",
      "Train Epoch: 227 [480/869 (55%)]\tLoss: 96728704.000000\n",
      "Train Epoch: 227 [640/869 (73%)]\tLoss: 190057888.000000\n",
      "Train Epoch: 227 [800/869 (91%)]\tLoss: 90204928.000000\n",
      "\n",
      "Test set: Avg. loss: 7470473.2537, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 228 [0/869 (0%)]\tLoss: 239231584.000000\n",
      "Train Epoch: 228 [160/869 (18%)]\tLoss: 59058800.000000\n",
      "Train Epoch: 228 [320/869 (36%)]\tLoss: 162017664.000000\n",
      "Train Epoch: 228 [480/869 (55%)]\tLoss: 143897264.000000\n",
      "Train Epoch: 228 [640/869 (73%)]\tLoss: 75112784.000000\n",
      "Train Epoch: 228 [800/869 (91%)]\tLoss: 90822032.000000\n",
      "\n",
      "Test set: Avg. loss: 7487556.0256, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 229 [0/869 (0%)]\tLoss: 126724392.000000\n",
      "Train Epoch: 229 [160/869 (18%)]\tLoss: 209861952.000000\n",
      "Train Epoch: 229 [320/869 (36%)]\tLoss: 91957224.000000\n",
      "Train Epoch: 229 [480/869 (55%)]\tLoss: 132761952.000000\n",
      "Train Epoch: 229 [640/869 (73%)]\tLoss: 50590904.000000\n",
      "Train Epoch: 229 [800/869 (91%)]\tLoss: 49530036.000000\n",
      "\n",
      "Test set: Avg. loss: 7611880.1876, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 230 [0/869 (0%)]\tLoss: 92474680.000000\n",
      "Train Epoch: 230 [160/869 (18%)]\tLoss: 143957216.000000\n",
      "Train Epoch: 230 [320/869 (36%)]\tLoss: 198190384.000000\n",
      "Train Epoch: 230 [480/869 (55%)]\tLoss: 138446672.000000\n",
      "Train Epoch: 230 [640/869 (73%)]\tLoss: 71993240.000000\n",
      "Train Epoch: 230 [800/869 (91%)]\tLoss: 185690048.000000\n",
      "\n",
      "Test set: Avg. loss: 7354767.9232, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 231 [0/869 (0%)]\tLoss: 35725524.000000\n",
      "Train Epoch: 231 [160/869 (18%)]\tLoss: 79837272.000000\n",
      "Train Epoch: 231 [320/869 (36%)]\tLoss: 111373960.000000\n",
      "Train Epoch: 231 [480/869 (55%)]\tLoss: 130449648.000000\n",
      "Train Epoch: 231 [640/869 (73%)]\tLoss: 227604032.000000\n",
      "Train Epoch: 231 [800/869 (91%)]\tLoss: 132668704.000000\n",
      "\n",
      "Test set: Avg. loss: 7513219.6418, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 232 [0/869 (0%)]\tLoss: 107824336.000000\n",
      "Train Epoch: 232 [160/869 (18%)]\tLoss: 37600220.000000\n",
      "Train Epoch: 232 [320/869 (36%)]\tLoss: 139339568.000000\n",
      "Train Epoch: 232 [480/869 (55%)]\tLoss: 85257616.000000\n",
      "Train Epoch: 232 [640/869 (73%)]\tLoss: 104852088.000000\n",
      "Train Epoch: 232 [800/869 (91%)]\tLoss: 165467520.000000\n",
      "\n",
      "Test set: Avg. loss: 7406281.4414, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 233 [0/869 (0%)]\tLoss: 130994856.000000\n",
      "Train Epoch: 233 [160/869 (18%)]\tLoss: 191699936.000000\n",
      "Train Epoch: 233 [320/869 (36%)]\tLoss: 103563632.000000\n",
      "Train Epoch: 233 [480/869 (55%)]\tLoss: 141790736.000000\n",
      "Train Epoch: 233 [640/869 (73%)]\tLoss: 133054936.000000\n",
      "Train Epoch: 233 [800/869 (91%)]\tLoss: 170817856.000000\n",
      "\n",
      "Test set: Avg. loss: 7363171.7356, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 234 [0/869 (0%)]\tLoss: 119731008.000000\n",
      "Train Epoch: 234 [160/869 (18%)]\tLoss: 136934496.000000\n",
      "Train Epoch: 234 [320/869 (36%)]\tLoss: 117721840.000000\n",
      "Train Epoch: 234 [480/869 (55%)]\tLoss: 92410256.000000\n",
      "Train Epoch: 234 [640/869 (73%)]\tLoss: 86783248.000000\n",
      "Train Epoch: 234 [800/869 (91%)]\tLoss: 186744080.000000\n",
      "\n",
      "Test set: Avg. loss: 7903711.5650, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 235 [0/869 (0%)]\tLoss: 148297840.000000\n",
      "Train Epoch: 235 [160/869 (18%)]\tLoss: 109275472.000000\n",
      "Train Epoch: 235 [320/869 (36%)]\tLoss: 89089904.000000\n",
      "Train Epoch: 235 [480/869 (55%)]\tLoss: 65872572.000000\n",
      "Train Epoch: 235 [640/869 (73%)]\tLoss: 75880544.000000\n",
      "Train Epoch: 235 [800/869 (91%)]\tLoss: 142171856.000000\n",
      "\n",
      "Test set: Avg. loss: 7693320.3923, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 236 [0/869 (0%)]\tLoss: 104507080.000000\n",
      "Train Epoch: 236 [160/869 (18%)]\tLoss: 202300032.000000\n",
      "Train Epoch: 236 [320/869 (36%)]\tLoss: 25755674.000000\n",
      "Train Epoch: 236 [480/869 (55%)]\tLoss: 94471680.000000\n",
      "Train Epoch: 236 [640/869 (73%)]\tLoss: 55888024.000000\n",
      "Train Epoch: 236 [800/869 (91%)]\tLoss: 101180840.000000\n",
      "\n",
      "Test set: Avg. loss: 7703709.3817, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 237 [0/869 (0%)]\tLoss: 43252076.000000\n",
      "Train Epoch: 237 [160/869 (18%)]\tLoss: 152164928.000000\n",
      "Train Epoch: 237 [320/869 (36%)]\tLoss: 179254160.000000\n",
      "Train Epoch: 237 [480/869 (55%)]\tLoss: 74506032.000000\n",
      "Train Epoch: 237 [640/869 (73%)]\tLoss: 31874066.000000\n",
      "Train Epoch: 237 [800/869 (91%)]\tLoss: 161231072.000000\n",
      "\n",
      "Test set: Avg. loss: 7650294.0384, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 238 [0/869 (0%)]\tLoss: 267174928.000000\n",
      "Train Epoch: 238 [160/869 (18%)]\tLoss: 240486736.000000\n",
      "Train Epoch: 238 [320/869 (36%)]\tLoss: 144338736.000000\n",
      "Train Epoch: 238 [480/869 (55%)]\tLoss: 85389888.000000\n",
      "Train Epoch: 238 [640/869 (73%)]\tLoss: 209520576.000000\n",
      "Train Epoch: 238 [800/869 (91%)]\tLoss: 243895952.000000\n",
      "\n",
      "Test set: Avg. loss: 7703572.5714, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 239 [0/869 (0%)]\tLoss: 162587472.000000\n",
      "Train Epoch: 239 [160/869 (18%)]\tLoss: 71212080.000000\n",
      "Train Epoch: 239 [320/869 (36%)]\tLoss: 151242960.000000\n",
      "Train Epoch: 239 [480/869 (55%)]\tLoss: 46422032.000000\n",
      "Train Epoch: 239 [640/869 (73%)]\tLoss: 117034208.000000\n",
      "Train Epoch: 239 [800/869 (91%)]\tLoss: 230648336.000000\n",
      "\n",
      "Test set: Avg. loss: 7505860.5117, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 240 [0/869 (0%)]\tLoss: 193601120.000000\n",
      "Train Epoch: 240 [160/869 (18%)]\tLoss: 125804720.000000\n",
      "Train Epoch: 240 [320/869 (36%)]\tLoss: 118952280.000000\n",
      "Train Epoch: 240 [480/869 (55%)]\tLoss: 98563800.000000\n",
      "Train Epoch: 240 [640/869 (73%)]\tLoss: 170559520.000000\n",
      "Train Epoch: 240 [800/869 (91%)]\tLoss: 156895344.000000\n",
      "\n",
      "Test set: Avg. loss: 7550895.7527, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 241 [0/869 (0%)]\tLoss: 184459952.000000\n",
      "Train Epoch: 241 [160/869 (18%)]\tLoss: 171841536.000000\n",
      "Train Epoch: 241 [320/869 (36%)]\tLoss: 181706080.000000\n",
      "Train Epoch: 241 [480/869 (55%)]\tLoss: 183592736.000000\n",
      "Train Epoch: 241 [640/869 (73%)]\tLoss: 52371568.000000\n",
      "Train Epoch: 241 [800/869 (91%)]\tLoss: 81362168.000000\n",
      "\n",
      "Test set: Avg. loss: 7345513.1002, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 242 [0/869 (0%)]\tLoss: 122662128.000000\n",
      "Train Epoch: 242 [160/869 (18%)]\tLoss: 103625944.000000\n",
      "Train Epoch: 242 [320/869 (36%)]\tLoss: 97419792.000000\n",
      "Train Epoch: 242 [480/869 (55%)]\tLoss: 81985064.000000\n",
      "Train Epoch: 242 [640/869 (73%)]\tLoss: 132337040.000000\n",
      "Train Epoch: 242 [800/869 (91%)]\tLoss: 174641472.000000\n",
      "\n",
      "Test set: Avg. loss: 7497155.9574, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 243 [0/869 (0%)]\tLoss: 123054824.000000\n",
      "Train Epoch: 243 [160/869 (18%)]\tLoss: 168036416.000000\n",
      "Train Epoch: 243 [320/869 (36%)]\tLoss: 96314376.000000\n",
      "Train Epoch: 243 [480/869 (55%)]\tLoss: 250802672.000000\n",
      "Train Epoch: 243 [640/869 (73%)]\tLoss: 100950456.000000\n",
      "Train Epoch: 243 [800/869 (91%)]\tLoss: 47414744.000000\n",
      "\n",
      "Test set: Avg. loss: 7679257.1599, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 244 [0/869 (0%)]\tLoss: 181824512.000000\n",
      "Train Epoch: 244 [160/869 (18%)]\tLoss: 147543888.000000\n",
      "Train Epoch: 244 [320/869 (36%)]\tLoss: 56869328.000000\n",
      "Train Epoch: 244 [480/869 (55%)]\tLoss: 133068944.000000\n",
      "Train Epoch: 244 [640/869 (73%)]\tLoss: 166807552.000000\n",
      "Train Epoch: 244 [800/869 (91%)]\tLoss: 117978448.000000\n",
      "\n",
      "Test set: Avg. loss: 7358128.2900, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 245 [0/869 (0%)]\tLoss: 112180608.000000\n",
      "Train Epoch: 245 [160/869 (18%)]\tLoss: 51509632.000000\n",
      "Train Epoch: 245 [320/869 (36%)]\tLoss: 138074944.000000\n",
      "Train Epoch: 245 [480/869 (55%)]\tLoss: 74250824.000000\n",
      "Train Epoch: 245 [640/869 (73%)]\tLoss: 114937840.000000\n",
      "Train Epoch: 245 [800/869 (91%)]\tLoss: 54928912.000000\n",
      "\n",
      "Test set: Avg. loss: 7565620.1962, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 246 [0/869 (0%)]\tLoss: 29867326.000000\n",
      "Train Epoch: 246 [160/869 (18%)]\tLoss: 362712480.000000\n",
      "Train Epoch: 246 [320/869 (36%)]\tLoss: 96684688.000000\n",
      "Train Epoch: 246 [480/869 (55%)]\tLoss: 137600960.000000\n",
      "Train Epoch: 246 [640/869 (73%)]\tLoss: 120056744.000000\n",
      "Train Epoch: 246 [800/869 (91%)]\tLoss: 80277120.000000\n",
      "\n",
      "Test set: Avg. loss: 7434677.0661, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 247 [0/869 (0%)]\tLoss: 86899480.000000\n",
      "Train Epoch: 247 [160/869 (18%)]\tLoss: 162919424.000000\n",
      "Train Epoch: 247 [320/869 (36%)]\tLoss: 149654912.000000\n",
      "Train Epoch: 247 [480/869 (55%)]\tLoss: 125811656.000000\n",
      "Train Epoch: 247 [640/869 (73%)]\tLoss: 38589280.000000\n",
      "Train Epoch: 247 [800/869 (91%)]\tLoss: 47778740.000000\n",
      "\n",
      "Test set: Avg. loss: 7585307.0021, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 248 [0/869 (0%)]\tLoss: 141642256.000000\n",
      "Train Epoch: 248 [160/869 (18%)]\tLoss: 211939392.000000\n",
      "Train Epoch: 248 [320/869 (36%)]\tLoss: 285075104.000000\n",
      "Train Epoch: 248 [480/869 (55%)]\tLoss: 141564864.000000\n",
      "Train Epoch: 248 [640/869 (73%)]\tLoss: 86508864.000000\n",
      "Train Epoch: 248 [800/869 (91%)]\tLoss: 197011184.000000\n",
      "\n",
      "Test set: Avg. loss: 7597621.7740, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 249 [0/869 (0%)]\tLoss: 275308224.000000\n",
      "Train Epoch: 249 [160/869 (18%)]\tLoss: 85692824.000000\n",
      "Train Epoch: 249 [320/869 (36%)]\tLoss: 99457184.000000\n",
      "Train Epoch: 249 [480/869 (55%)]\tLoss: 163784560.000000\n",
      "Train Epoch: 249 [640/869 (73%)]\tLoss: 129363392.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 249 [800/869 (91%)]\tLoss: 103208136.000000\n",
      "\n",
      "Test set: Avg. loss: 7526853.9360, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 250 [0/869 (0%)]\tLoss: 125405576.000000\n",
      "Train Epoch: 250 [160/869 (18%)]\tLoss: 183503568.000000\n",
      "Train Epoch: 250 [320/869 (36%)]\tLoss: 273324704.000000\n",
      "Train Epoch: 250 [480/869 (55%)]\tLoss: 166774016.000000\n",
      "Train Epoch: 250 [640/869 (73%)]\tLoss: 73973096.000000\n",
      "Train Epoch: 250 [800/869 (91%)]\tLoss: 42843172.000000\n",
      "\n",
      "Test set: Avg. loss: 7612630.0725, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 251 [0/869 (0%)]\tLoss: 82725328.000000\n",
      "Train Epoch: 251 [160/869 (18%)]\tLoss: 94735160.000000\n",
      "Train Epoch: 251 [320/869 (36%)]\tLoss: 182143904.000000\n",
      "Train Epoch: 251 [480/869 (55%)]\tLoss: 76729584.000000\n",
      "Train Epoch: 251 [640/869 (73%)]\tLoss: 144067248.000000\n",
      "Train Epoch: 251 [800/869 (91%)]\tLoss: 152269072.000000\n",
      "\n",
      "Test set: Avg. loss: 7552162.9680, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 252 [0/869 (0%)]\tLoss: 34928332.000000\n",
      "Train Epoch: 252 [160/869 (18%)]\tLoss: 216255792.000000\n",
      "Train Epoch: 252 [320/869 (36%)]\tLoss: 122622312.000000\n",
      "Train Epoch: 252 [480/869 (55%)]\tLoss: 71526656.000000\n",
      "Train Epoch: 252 [640/869 (73%)]\tLoss: 98158688.000000\n",
      "Train Epoch: 252 [800/869 (91%)]\tLoss: 103638360.000000\n",
      "\n",
      "Test set: Avg. loss: 7364114.0554, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 253 [0/869 (0%)]\tLoss: 80288968.000000\n",
      "Train Epoch: 253 [160/869 (18%)]\tLoss: 144858112.000000\n",
      "Train Epoch: 253 [320/869 (36%)]\tLoss: 87210384.000000\n",
      "Train Epoch: 253 [480/869 (55%)]\tLoss: 110888928.000000\n",
      "Train Epoch: 253 [640/869 (73%)]\tLoss: 152141648.000000\n",
      "Train Epoch: 253 [800/869 (91%)]\tLoss: 216816304.000000\n",
      "\n",
      "Test set: Avg. loss: 7567012.3753, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 254 [0/869 (0%)]\tLoss: 141335952.000000\n",
      "Train Epoch: 254 [160/869 (18%)]\tLoss: 86016504.000000\n",
      "Train Epoch: 254 [320/869 (36%)]\tLoss: 127473096.000000\n",
      "Train Epoch: 254 [480/869 (55%)]\tLoss: 93291464.000000\n",
      "Train Epoch: 254 [640/869 (73%)]\tLoss: 88216064.000000\n",
      "Train Epoch: 254 [800/869 (91%)]\tLoss: 153869456.000000\n",
      "\n",
      "Test set: Avg. loss: 7399845.4670, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 255 [0/869 (0%)]\tLoss: 94670880.000000\n",
      "Train Epoch: 255 [160/869 (18%)]\tLoss: 87187176.000000\n",
      "Train Epoch: 255 [320/869 (36%)]\tLoss: 76776104.000000\n",
      "Train Epoch: 255 [480/869 (55%)]\tLoss: 89075576.000000\n",
      "Train Epoch: 255 [640/869 (73%)]\tLoss: 37363328.000000\n",
      "Train Epoch: 255 [800/869 (91%)]\tLoss: 137967984.000000\n",
      "\n",
      "Test set: Avg. loss: 7838398.0384, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 256 [0/869 (0%)]\tLoss: 139044400.000000\n",
      "Train Epoch: 256 [160/869 (18%)]\tLoss: 249895664.000000\n",
      "Train Epoch: 256 [320/869 (36%)]\tLoss: 99146888.000000\n",
      "Train Epoch: 256 [480/869 (55%)]\tLoss: 108078344.000000\n",
      "Train Epoch: 256 [640/869 (73%)]\tLoss: 235916512.000000\n",
      "Train Epoch: 256 [800/869 (91%)]\tLoss: 88200112.000000\n",
      "\n",
      "Test set: Avg. loss: 7737900.2942, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 257 [0/869 (0%)]\tLoss: 188912832.000000\n",
      "Train Epoch: 257 [160/869 (18%)]\tLoss: 45591476.000000\n",
      "Train Epoch: 257 [320/869 (36%)]\tLoss: 134182984.000000\n",
      "Train Epoch: 257 [480/869 (55%)]\tLoss: 176749696.000000\n",
      "Train Epoch: 257 [640/869 (73%)]\tLoss: 39102092.000000\n",
      "Train Epoch: 257 [800/869 (91%)]\tLoss: 232515328.000000\n",
      "\n",
      "Test set: Avg. loss: 7428524.3923, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 258 [0/869 (0%)]\tLoss: 47334024.000000\n",
      "Train Epoch: 258 [160/869 (18%)]\tLoss: 108407752.000000\n",
      "Train Epoch: 258 [320/869 (36%)]\tLoss: 88943328.000000\n",
      "Train Epoch: 258 [480/869 (55%)]\tLoss: 116644168.000000\n",
      "Train Epoch: 258 [640/869 (73%)]\tLoss: 91839376.000000\n",
      "Train Epoch: 258 [800/869 (91%)]\tLoss: 99718808.000000\n",
      "\n",
      "Test set: Avg. loss: 7636361.7910, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 259 [0/869 (0%)]\tLoss: 39894624.000000\n",
      "Train Epoch: 259 [160/869 (18%)]\tLoss: 131900792.000000\n",
      "Train Epoch: 259 [320/869 (36%)]\tLoss: 97696800.000000\n",
      "Train Epoch: 259 [480/869 (55%)]\tLoss: 281650144.000000\n",
      "Train Epoch: 259 [640/869 (73%)]\tLoss: 234623584.000000\n",
      "Train Epoch: 259 [800/869 (91%)]\tLoss: 129418248.000000\n",
      "\n",
      "Test set: Avg. loss: 7574665.1343, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 260 [0/869 (0%)]\tLoss: 291604224.000000\n",
      "Train Epoch: 260 [160/869 (18%)]\tLoss: 149244944.000000\n",
      "Train Epoch: 260 [320/869 (36%)]\tLoss: 241880832.000000\n",
      "Train Epoch: 260 [480/869 (55%)]\tLoss: 318141792.000000\n",
      "Train Epoch: 260 [640/869 (73%)]\tLoss: 51945644.000000\n",
      "Train Epoch: 260 [800/869 (91%)]\tLoss: 162661808.000000\n",
      "\n",
      "Test set: Avg. loss: 7872643.2665, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 261 [0/869 (0%)]\tLoss: 100027424.000000\n",
      "Train Epoch: 261 [160/869 (18%)]\tLoss: 204108288.000000\n",
      "Train Epoch: 261 [320/869 (36%)]\tLoss: 130835472.000000\n",
      "Train Epoch: 261 [480/869 (55%)]\tLoss: 52789240.000000\n",
      "Train Epoch: 261 [640/869 (73%)]\tLoss: 203916192.000000\n",
      "Train Epoch: 261 [800/869 (91%)]\tLoss: 140156064.000000\n",
      "\n",
      "Test set: Avg. loss: 7724649.9446, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 262 [0/869 (0%)]\tLoss: 208263072.000000\n",
      "Train Epoch: 262 [160/869 (18%)]\tLoss: 43325216.000000\n",
      "Train Epoch: 262 [320/869 (36%)]\tLoss: 128077800.000000\n",
      "Train Epoch: 262 [480/869 (55%)]\tLoss: 138833952.000000\n",
      "Train Epoch: 262 [640/869 (73%)]\tLoss: 184656400.000000\n",
      "Train Epoch: 262 [800/869 (91%)]\tLoss: 157208112.000000\n",
      "\n",
      "Test set: Avg. loss: 7324367.6844, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 263 [0/869 (0%)]\tLoss: 94581192.000000\n",
      "Train Epoch: 263 [160/869 (18%)]\tLoss: 30824422.000000\n",
      "Train Epoch: 263 [320/869 (36%)]\tLoss: 81283872.000000\n",
      "Train Epoch: 263 [480/869 (55%)]\tLoss: 99841800.000000\n",
      "Train Epoch: 263 [640/869 (73%)]\tLoss: 131051144.000000\n",
      "Train Epoch: 263 [800/869 (91%)]\tLoss: 35099140.000000\n",
      "\n",
      "Test set: Avg. loss: 7619268.5800, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 264 [0/869 (0%)]\tLoss: 134557536.000000\n",
      "Train Epoch: 264 [160/869 (18%)]\tLoss: 129408512.000000\n",
      "Train Epoch: 264 [320/869 (36%)]\tLoss: 43771452.000000\n",
      "Train Epoch: 264 [480/869 (55%)]\tLoss: 70610560.000000\n",
      "Train Epoch: 264 [640/869 (73%)]\tLoss: 81006448.000000\n",
      "Train Epoch: 264 [800/869 (91%)]\tLoss: 146272896.000000\n",
      "\n",
      "Test set: Avg. loss: 7656581.7484, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 265 [0/869 (0%)]\tLoss: 140061824.000000\n",
      "Train Epoch: 265 [160/869 (18%)]\tLoss: 38494120.000000\n",
      "Train Epoch: 265 [320/869 (36%)]\tLoss: 85287856.000000\n",
      "Train Epoch: 265 [480/869 (55%)]\tLoss: 170852224.000000\n",
      "Train Epoch: 265 [640/869 (73%)]\tLoss: 360960128.000000\n",
      "Train Epoch: 265 [800/869 (91%)]\tLoss: 87099424.000000\n",
      "\n",
      "Test set: Avg. loss: 7628199.2452, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 266 [0/869 (0%)]\tLoss: 200157296.000000\n",
      "Train Epoch: 266 [160/869 (18%)]\tLoss: 78913200.000000\n",
      "Train Epoch: 266 [320/869 (36%)]\tLoss: 87696104.000000\n",
      "Train Epoch: 266 [480/869 (55%)]\tLoss: 87861392.000000\n",
      "Train Epoch: 266 [640/869 (73%)]\tLoss: 101371056.000000\n",
      "Train Epoch: 266 [800/869 (91%)]\tLoss: 116177176.000000\n",
      "\n",
      "Test set: Avg. loss: 7817203.2239, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 267 [0/869 (0%)]\tLoss: 136001472.000000\n",
      "Train Epoch: 267 [160/869 (18%)]\tLoss: 129278504.000000\n",
      "Train Epoch: 267 [320/869 (36%)]\tLoss: 233815104.000000\n",
      "Train Epoch: 267 [480/869 (55%)]\tLoss: 127199720.000000\n",
      "Train Epoch: 267 [640/869 (73%)]\tLoss: 116596280.000000\n",
      "Train Epoch: 267 [800/869 (91%)]\tLoss: 132963432.000000\n",
      "\n",
      "Test set: Avg. loss: 7421421.4925, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 268 [0/869 (0%)]\tLoss: 147544240.000000\n",
      "Train Epoch: 268 [160/869 (18%)]\tLoss: 98777816.000000\n",
      "Train Epoch: 268 [320/869 (36%)]\tLoss: 98333984.000000\n",
      "Train Epoch: 268 [480/869 (55%)]\tLoss: 37261144.000000\n",
      "Train Epoch: 268 [640/869 (73%)]\tLoss: 78316352.000000\n",
      "Train Epoch: 268 [800/869 (91%)]\tLoss: 51624136.000000\n",
      "\n",
      "Test set: Avg. loss: 7759548.9467, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 269 [0/869 (0%)]\tLoss: 206984928.000000\n",
      "Train Epoch: 269 [160/869 (18%)]\tLoss: 304450400.000000\n",
      "Train Epoch: 269 [320/869 (36%)]\tLoss: 151964288.000000\n",
      "Train Epoch: 269 [480/869 (55%)]\tLoss: 87120528.000000\n",
      "Train Epoch: 269 [640/869 (73%)]\tLoss: 36437300.000000\n",
      "Train Epoch: 269 [800/869 (91%)]\tLoss: 57421836.000000\n",
      "\n",
      "Test set: Avg. loss: 7549146.4222, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 270 [0/869 (0%)]\tLoss: 145196096.000000\n",
      "Train Epoch: 270 [160/869 (18%)]\tLoss: 216353408.000000\n",
      "Train Epoch: 270 [320/869 (36%)]\tLoss: 166553184.000000\n",
      "Train Epoch: 270 [480/869 (55%)]\tLoss: 38557304.000000\n",
      "Train Epoch: 270 [640/869 (73%)]\tLoss: 97147384.000000\n",
      "Train Epoch: 270 [800/869 (91%)]\tLoss: 154576048.000000\n",
      "\n",
      "Test set: Avg. loss: 7517241.2878, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 271 [0/869 (0%)]\tLoss: 55204260.000000\n",
      "Train Epoch: 271 [160/869 (18%)]\tLoss: 122982112.000000\n",
      "Train Epoch: 271 [320/869 (36%)]\tLoss: 93917496.000000\n",
      "Train Epoch: 271 [480/869 (55%)]\tLoss: 168912656.000000\n",
      "Train Epoch: 271 [640/869 (73%)]\tLoss: 136873248.000000\n",
      "Train Epoch: 271 [800/869 (91%)]\tLoss: 102022456.000000\n",
      "\n",
      "Test set: Avg. loss: 7948772.1109, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 272 [0/869 (0%)]\tLoss: 170081088.000000\n",
      "Train Epoch: 272 [160/869 (18%)]\tLoss: 120239072.000000\n",
      "Train Epoch: 272 [320/869 (36%)]\tLoss: 80112080.000000\n",
      "Train Epoch: 272 [480/869 (55%)]\tLoss: 154760272.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 272 [640/869 (73%)]\tLoss: 34281064.000000\n",
      "Train Epoch: 272 [800/869 (91%)]\tLoss: 150022736.000000\n",
      "\n",
      "Test set: Avg. loss: 8209266.5075, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 273 [0/869 (0%)]\tLoss: 157374624.000000\n",
      "Train Epoch: 273 [160/869 (18%)]\tLoss: 96789288.000000\n",
      "Train Epoch: 273 [320/869 (36%)]\tLoss: 54893108.000000\n",
      "Train Epoch: 273 [480/869 (55%)]\tLoss: 195952848.000000\n",
      "Train Epoch: 273 [640/869 (73%)]\tLoss: 95225272.000000\n",
      "Train Epoch: 273 [800/869 (91%)]\tLoss: 196282096.000000\n",
      "\n",
      "Test set: Avg. loss: 7604159.3348, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 274 [0/869 (0%)]\tLoss: 70473768.000000\n",
      "Train Epoch: 274 [160/869 (18%)]\tLoss: 99285352.000000\n",
      "Train Epoch: 274 [320/869 (36%)]\tLoss: 78039552.000000\n",
      "Train Epoch: 274 [480/869 (55%)]\tLoss: 89112688.000000\n",
      "Train Epoch: 274 [640/869 (73%)]\tLoss: 218615024.000000\n",
      "Train Epoch: 274 [800/869 (91%)]\tLoss: 143498880.000000\n",
      "\n",
      "Test set: Avg. loss: 7347288.2814, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 275 [0/869 (0%)]\tLoss: 44574752.000000\n",
      "Train Epoch: 275 [160/869 (18%)]\tLoss: 209946320.000000\n",
      "Train Epoch: 275 [320/869 (36%)]\tLoss: 79232320.000000\n",
      "Train Epoch: 275 [480/869 (55%)]\tLoss: 105048368.000000\n",
      "Train Epoch: 275 [640/869 (73%)]\tLoss: 143465232.000000\n",
      "Train Epoch: 275 [800/869 (91%)]\tLoss: 80844344.000000\n",
      "\n",
      "Test set: Avg. loss: 7862253.5309, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 276 [0/869 (0%)]\tLoss: 249401344.000000\n",
      "Train Epoch: 276 [160/869 (18%)]\tLoss: 123830736.000000\n",
      "Train Epoch: 276 [320/869 (36%)]\tLoss: 144010528.000000\n",
      "Train Epoch: 276 [480/869 (55%)]\tLoss: 186069952.000000\n",
      "Train Epoch: 276 [640/869 (73%)]\tLoss: 126316560.000000\n",
      "Train Epoch: 276 [800/869 (91%)]\tLoss: 31956010.000000\n",
      "\n",
      "Test set: Avg. loss: 7526431.7271, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 277 [0/869 (0%)]\tLoss: 139934240.000000\n",
      "Train Epoch: 277 [160/869 (18%)]\tLoss: 217929072.000000\n",
      "Train Epoch: 277 [320/869 (36%)]\tLoss: 121704680.000000\n",
      "Train Epoch: 277 [480/869 (55%)]\tLoss: 87317704.000000\n",
      "Train Epoch: 277 [640/869 (73%)]\tLoss: 43333676.000000\n",
      "Train Epoch: 277 [800/869 (91%)]\tLoss: 38828504.000000\n",
      "\n",
      "Test set: Avg. loss: 7363443.6674, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 278 [0/869 (0%)]\tLoss: 268592448.000000\n",
      "Train Epoch: 278 [160/869 (18%)]\tLoss: 188013184.000000\n",
      "Train Epoch: 278 [320/869 (36%)]\tLoss: 88310824.000000\n",
      "Train Epoch: 278 [480/869 (55%)]\tLoss: 294955744.000000\n",
      "Train Epoch: 278 [640/869 (73%)]\tLoss: 130092312.000000\n",
      "Train Epoch: 278 [800/869 (91%)]\tLoss: 87343208.000000\n",
      "\n",
      "Test set: Avg. loss: 7434689.5352, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 279 [0/869 (0%)]\tLoss: 61584496.000000\n",
      "Train Epoch: 279 [160/869 (18%)]\tLoss: 138023904.000000\n",
      "Train Epoch: 279 [320/869 (36%)]\tLoss: 102371552.000000\n",
      "Train Epoch: 279 [480/869 (55%)]\tLoss: 249289984.000000\n",
      "Train Epoch: 279 [640/869 (73%)]\tLoss: 31442076.000000\n",
      "Train Epoch: 279 [800/869 (91%)]\tLoss: 229113312.000000\n",
      "\n",
      "Test set: Avg. loss: 7443181.1343, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 280 [0/869 (0%)]\tLoss: 125641640.000000\n",
      "Train Epoch: 280 [160/869 (18%)]\tLoss: 41720220.000000\n",
      "Train Epoch: 280 [320/869 (36%)]\tLoss: 156141504.000000\n",
      "Train Epoch: 280 [480/869 (55%)]\tLoss: 179533840.000000\n",
      "Train Epoch: 280 [640/869 (73%)]\tLoss: 112340488.000000\n",
      "Train Epoch: 280 [800/869 (91%)]\tLoss: 103018728.000000\n",
      "\n",
      "Test set: Avg. loss: 7448406.1919, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 281 [0/869 (0%)]\tLoss: 58830576.000000\n",
      "Train Epoch: 281 [160/869 (18%)]\tLoss: 94698288.000000\n",
      "Train Epoch: 281 [320/869 (36%)]\tLoss: 238016064.000000\n",
      "Train Epoch: 281 [480/869 (55%)]\tLoss: 198160880.000000\n",
      "Train Epoch: 281 [640/869 (73%)]\tLoss: 145071904.000000\n",
      "Train Epoch: 281 [800/869 (91%)]\tLoss: 99968928.000000\n",
      "\n",
      "Test set: Avg. loss: 7559571.8124, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 282 [0/869 (0%)]\tLoss: 108584544.000000\n",
      "Train Epoch: 282 [160/869 (18%)]\tLoss: 59915396.000000\n",
      "Train Epoch: 282 [320/869 (36%)]\tLoss: 244466368.000000\n",
      "Train Epoch: 282 [480/869 (55%)]\tLoss: 55754712.000000\n",
      "Train Epoch: 282 [640/869 (73%)]\tLoss: 171119152.000000\n",
      "Train Epoch: 282 [800/869 (91%)]\tLoss: 121828912.000000\n",
      "\n",
      "Test set: Avg. loss: 7487135.2836, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 283 [0/869 (0%)]\tLoss: 150597280.000000\n",
      "Train Epoch: 283 [160/869 (18%)]\tLoss: 171062272.000000\n",
      "Train Epoch: 283 [320/869 (36%)]\tLoss: 92546680.000000\n",
      "Train Epoch: 283 [480/869 (55%)]\tLoss: 135966192.000000\n",
      "Train Epoch: 283 [640/869 (73%)]\tLoss: 122619312.000000\n",
      "Train Epoch: 283 [800/869 (91%)]\tLoss: 49992060.000000\n",
      "\n",
      "Test set: Avg. loss: 8109211.9318, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 284 [0/869 (0%)]\tLoss: 77792216.000000\n",
      "Train Epoch: 284 [160/869 (18%)]\tLoss: 146592336.000000\n",
      "Train Epoch: 284 [320/869 (36%)]\tLoss: 117064848.000000\n",
      "Train Epoch: 284 [480/869 (55%)]\tLoss: 207115136.000000\n",
      "Train Epoch: 284 [640/869 (73%)]\tLoss: 222247808.000000\n",
      "Train Epoch: 284 [800/869 (91%)]\tLoss: 158773504.000000\n",
      "\n",
      "Test set: Avg. loss: 7513547.3689, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 285 [0/869 (0%)]\tLoss: 91499432.000000\n",
      "Train Epoch: 285 [160/869 (18%)]\tLoss: 107002320.000000\n",
      "Train Epoch: 285 [320/869 (36%)]\tLoss: 183550144.000000\n",
      "Train Epoch: 285 [480/869 (55%)]\tLoss: 73351352.000000\n",
      "Train Epoch: 285 [640/869 (73%)]\tLoss: 61395868.000000\n",
      "Train Epoch: 285 [800/869 (91%)]\tLoss: 27018908.000000\n",
      "\n",
      "Test set: Avg. loss: 7315152.6226, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 286 [0/869 (0%)]\tLoss: 151223328.000000\n",
      "Train Epoch: 286 [160/869 (18%)]\tLoss: 130736992.000000\n",
      "Train Epoch: 286 [320/869 (36%)]\tLoss: 135544832.000000\n",
      "Train Epoch: 286 [480/869 (55%)]\tLoss: 145498944.000000\n",
      "Train Epoch: 286 [640/869 (73%)]\tLoss: 179267248.000000\n",
      "Train Epoch: 286 [800/869 (91%)]\tLoss: 98325376.000000\n",
      "\n",
      "Test set: Avg. loss: 7643525.7697, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 287 [0/869 (0%)]\tLoss: 323077184.000000\n",
      "Train Epoch: 287 [160/869 (18%)]\tLoss: 172321552.000000\n",
      "Train Epoch: 287 [320/869 (36%)]\tLoss: 174157936.000000\n",
      "Train Epoch: 287 [480/869 (55%)]\tLoss: 181805024.000000\n",
      "Train Epoch: 287 [640/869 (73%)]\tLoss: 243131536.000000\n",
      "Train Epoch: 287 [800/869 (91%)]\tLoss: 112871496.000000\n",
      "\n",
      "Test set: Avg. loss: 7550741.4243, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 288 [0/869 (0%)]\tLoss: 42489628.000000\n",
      "Train Epoch: 288 [160/869 (18%)]\tLoss: 145958624.000000\n",
      "Train Epoch: 288 [320/869 (36%)]\tLoss: 92837648.000000\n",
      "Train Epoch: 288 [480/869 (55%)]\tLoss: 78913144.000000\n",
      "Train Epoch: 288 [640/869 (73%)]\tLoss: 308272672.000000\n",
      "Train Epoch: 288 [800/869 (91%)]\tLoss: 237334112.000000\n",
      "\n",
      "Test set: Avg. loss: 7410385.4499, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 289 [0/869 (0%)]\tLoss: 157811728.000000\n",
      "Train Epoch: 289 [160/869 (18%)]\tLoss: 119085104.000000\n",
      "Train Epoch: 289 [320/869 (36%)]\tLoss: 49251696.000000\n",
      "Train Epoch: 289 [480/869 (55%)]\tLoss: 41264032.000000\n",
      "Train Epoch: 289 [640/869 (73%)]\tLoss: 115831504.000000\n",
      "Train Epoch: 289 [800/869 (91%)]\tLoss: 123620416.000000\n",
      "\n",
      "Test set: Avg. loss: 7505905.3817, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 290 [0/869 (0%)]\tLoss: 169321024.000000\n",
      "Train Epoch: 290 [160/869 (18%)]\tLoss: 89412296.000000\n",
      "Train Epoch: 290 [320/869 (36%)]\tLoss: 41216224.000000\n",
      "Train Epoch: 290 [480/869 (55%)]\tLoss: 162347536.000000\n",
      "Train Epoch: 290 [640/869 (73%)]\tLoss: 118680792.000000\n",
      "Train Epoch: 290 [800/869 (91%)]\tLoss: 160379312.000000\n",
      "\n",
      "Test set: Avg. loss: 7423186.3028, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 291 [0/869 (0%)]\tLoss: 139390416.000000\n",
      "Train Epoch: 291 [160/869 (18%)]\tLoss: 45233184.000000\n",
      "Train Epoch: 291 [320/869 (36%)]\tLoss: 288279360.000000\n",
      "Train Epoch: 291 [480/869 (55%)]\tLoss: 144527008.000000\n",
      "Train Epoch: 291 [640/869 (73%)]\tLoss: 126234848.000000\n",
      "Train Epoch: 291 [800/869 (91%)]\tLoss: 162613024.000000\n",
      "\n",
      "Test set: Avg. loss: 8044507.8827, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 292 [0/869 (0%)]\tLoss: 152380368.000000\n",
      "Train Epoch: 292 [160/869 (18%)]\tLoss: 91254536.000000\n",
      "Train Epoch: 292 [320/869 (36%)]\tLoss: 37503144.000000\n",
      "Train Epoch: 292 [480/869 (55%)]\tLoss: 82478848.000000\n",
      "Train Epoch: 292 [640/869 (73%)]\tLoss: 133306648.000000\n",
      "Train Epoch: 292 [800/869 (91%)]\tLoss: 172841552.000000\n",
      "\n",
      "Test set: Avg. loss: 7439083.6759, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 293 [0/869 (0%)]\tLoss: 37665116.000000\n",
      "Train Epoch: 293 [160/869 (18%)]\tLoss: 133860608.000000\n",
      "Train Epoch: 293 [320/869 (36%)]\tLoss: 100847008.000000\n",
      "Train Epoch: 293 [480/869 (55%)]\tLoss: 36582880.000000\n",
      "Train Epoch: 293 [640/869 (73%)]\tLoss: 175613712.000000\n",
      "Train Epoch: 293 [800/869 (91%)]\tLoss: 142696496.000000\n",
      "\n",
      "Test set: Avg. loss: 7569537.2878, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 294 [0/869 (0%)]\tLoss: 242433808.000000\n",
      "Train Epoch: 294 [160/869 (18%)]\tLoss: 115491760.000000\n",
      "Train Epoch: 294 [320/869 (36%)]\tLoss: 94691872.000000\n",
      "Train Epoch: 294 [480/869 (55%)]\tLoss: 90226448.000000\n",
      "Train Epoch: 294 [640/869 (73%)]\tLoss: 102254832.000000\n",
      "Train Epoch: 294 [800/869 (91%)]\tLoss: 38083812.000000\n",
      "\n",
      "Test set: Avg. loss: 7312785.2537, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 295 [0/869 (0%)]\tLoss: 52136648.000000\n",
      "Train Epoch: 295 [160/869 (18%)]\tLoss: 59447792.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 295 [320/869 (36%)]\tLoss: 173565472.000000\n",
      "Train Epoch: 295 [480/869 (55%)]\tLoss: 145798368.000000\n",
      "Train Epoch: 295 [640/869 (73%)]\tLoss: 113141952.000000\n",
      "Train Epoch: 295 [800/869 (91%)]\tLoss: 166588816.000000\n",
      "\n",
      "Test set: Avg. loss: 7598671.6077, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 296 [0/869 (0%)]\tLoss: 190111120.000000\n",
      "Train Epoch: 296 [160/869 (18%)]\tLoss: 115534024.000000\n",
      "Train Epoch: 296 [320/869 (36%)]\tLoss: 141288416.000000\n",
      "Train Epoch: 296 [480/869 (55%)]\tLoss: 127819440.000000\n",
      "Train Epoch: 296 [640/869 (73%)]\tLoss: 169215936.000000\n",
      "Train Epoch: 296 [800/869 (91%)]\tLoss: 84319632.000000\n",
      "\n",
      "Test set: Avg. loss: 7354852.7164, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 297 [0/869 (0%)]\tLoss: 148080688.000000\n",
      "Train Epoch: 297 [160/869 (18%)]\tLoss: 173486304.000000\n",
      "Train Epoch: 297 [320/869 (36%)]\tLoss: 257174464.000000\n",
      "Train Epoch: 297 [480/869 (55%)]\tLoss: 67712400.000000\n",
      "Train Epoch: 297 [640/869 (73%)]\tLoss: 76588744.000000\n",
      "Train Epoch: 297 [800/869 (91%)]\tLoss: 121906912.000000\n",
      "\n",
      "Test set: Avg. loss: 7382378.3028, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 298 [0/869 (0%)]\tLoss: 53554352.000000\n",
      "Train Epoch: 298 [160/869 (18%)]\tLoss: 42067768.000000\n",
      "Train Epoch: 298 [320/869 (36%)]\tLoss: 273676352.000000\n",
      "Train Epoch: 298 [480/869 (55%)]\tLoss: 88453288.000000\n",
      "Train Epoch: 298 [640/869 (73%)]\tLoss: 51049720.000000\n",
      "Train Epoch: 298 [800/869 (91%)]\tLoss: 124484856.000000\n",
      "\n",
      "Test set: Avg. loss: 7521766.9595, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 299 [0/869 (0%)]\tLoss: 88811264.000000\n",
      "Train Epoch: 299 [160/869 (18%)]\tLoss: 140294656.000000\n",
      "Train Epoch: 299 [320/869 (36%)]\tLoss: 104011952.000000\n",
      "Train Epoch: 299 [480/869 (55%)]\tLoss: 33430554.000000\n",
      "Train Epoch: 299 [640/869 (73%)]\tLoss: 163030320.000000\n",
      "Train Epoch: 299 [800/869 (91%)]\tLoss: 171044832.000000\n",
      "\n",
      "Test set: Avg. loss: 7551874.0043, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 300 [0/869 (0%)]\tLoss: 119010664.000000\n",
      "Train Epoch: 300 [160/869 (18%)]\tLoss: 82635568.000000\n",
      "Train Epoch: 300 [320/869 (36%)]\tLoss: 142580352.000000\n",
      "Train Epoch: 300 [480/869 (55%)]\tLoss: 84931600.000000\n",
      "Train Epoch: 300 [640/869 (73%)]\tLoss: 47011112.000000\n",
      "Train Epoch: 300 [800/869 (91%)]\tLoss: 141661008.000000\n",
      "\n",
      "Test set: Avg. loss: 7343614.5928, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 301 [0/869 (0%)]\tLoss: 98715784.000000\n",
      "Train Epoch: 301 [160/869 (18%)]\tLoss: 121834016.000000\n",
      "Train Epoch: 301 [320/869 (36%)]\tLoss: 49709296.000000\n",
      "Train Epoch: 301 [480/869 (55%)]\tLoss: 75010656.000000\n",
      "Train Epoch: 301 [640/869 (73%)]\tLoss: 93533704.000000\n",
      "Train Epoch: 301 [800/869 (91%)]\tLoss: 289227968.000000\n",
      "\n",
      "Test set: Avg. loss: 7341801.6972, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 302 [0/869 (0%)]\tLoss: 139913632.000000\n",
      "Train Epoch: 302 [160/869 (18%)]\tLoss: 95853280.000000\n",
      "Train Epoch: 302 [320/869 (36%)]\tLoss: 140607712.000000\n",
      "Train Epoch: 302 [480/869 (55%)]\tLoss: 139433888.000000\n",
      "Train Epoch: 302 [640/869 (73%)]\tLoss: 143522016.000000\n",
      "Train Epoch: 302 [800/869 (91%)]\tLoss: 26016746.000000\n",
      "\n",
      "Test set: Avg. loss: 8068909.1002, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 303 [0/869 (0%)]\tLoss: 88910784.000000\n",
      "Train Epoch: 303 [160/869 (18%)]\tLoss: 45450320.000000\n",
      "Train Epoch: 303 [320/869 (36%)]\tLoss: 298077600.000000\n",
      "Train Epoch: 303 [480/869 (55%)]\tLoss: 222598544.000000\n",
      "Train Epoch: 303 [640/869 (73%)]\tLoss: 149651744.000000\n",
      "Train Epoch: 303 [800/869 (91%)]\tLoss: 92566288.000000\n",
      "\n",
      "Test set: Avg. loss: 7452830.1066, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 304 [0/869 (0%)]\tLoss: 139201280.000000\n",
      "Train Epoch: 304 [160/869 (18%)]\tLoss: 160154288.000000\n",
      "Train Epoch: 304 [320/869 (36%)]\tLoss: 73155496.000000\n",
      "Train Epoch: 304 [480/869 (55%)]\tLoss: 88163568.000000\n",
      "Train Epoch: 304 [640/869 (73%)]\tLoss: 102365688.000000\n",
      "Train Epoch: 304 [800/869 (91%)]\tLoss: 135521248.000000\n",
      "\n",
      "Test set: Avg. loss: 7476372.7420, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 305 [0/869 (0%)]\tLoss: 160329504.000000\n",
      "Train Epoch: 305 [160/869 (18%)]\tLoss: 139367248.000000\n",
      "Train Epoch: 305 [320/869 (36%)]\tLoss: 31917032.000000\n",
      "Train Epoch: 305 [480/869 (55%)]\tLoss: 87624432.000000\n",
      "Train Epoch: 305 [640/869 (73%)]\tLoss: 134786592.000000\n",
      "Train Epoch: 305 [800/869 (91%)]\tLoss: 162131984.000000\n",
      "\n",
      "Test set: Avg. loss: 7772777.9957, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 306 [0/869 (0%)]\tLoss: 102430720.000000\n",
      "Train Epoch: 306 [160/869 (18%)]\tLoss: 215249760.000000\n",
      "Train Epoch: 306 [320/869 (36%)]\tLoss: 176366368.000000\n",
      "Train Epoch: 306 [480/869 (55%)]\tLoss: 170629888.000000\n",
      "Train Epoch: 306 [640/869 (73%)]\tLoss: 20697536.000000\n",
      "Train Epoch: 306 [800/869 (91%)]\tLoss: 169308288.000000\n",
      "\n",
      "Test set: Avg. loss: 7366747.6930, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 307 [0/869 (0%)]\tLoss: 88437080.000000\n",
      "Train Epoch: 307 [160/869 (18%)]\tLoss: 178373712.000000\n",
      "Train Epoch: 307 [320/869 (36%)]\tLoss: 145892384.000000\n",
      "Train Epoch: 307 [480/869 (55%)]\tLoss: 126566832.000000\n",
      "Train Epoch: 307 [640/869 (73%)]\tLoss: 161436416.000000\n",
      "Train Epoch: 307 [800/869 (91%)]\tLoss: 122640560.000000\n",
      "\n",
      "Test set: Avg. loss: 7509333.4414, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 308 [0/869 (0%)]\tLoss: 265738304.000000\n",
      "Train Epoch: 308 [160/869 (18%)]\tLoss: 180778592.000000\n",
      "Train Epoch: 308 [320/869 (36%)]\tLoss: 64129848.000000\n",
      "Train Epoch: 308 [480/869 (55%)]\tLoss: 136997216.000000\n",
      "Train Epoch: 308 [640/869 (73%)]\tLoss: 83666048.000000\n",
      "Train Epoch: 308 [800/869 (91%)]\tLoss: 126746448.000000\n",
      "\n",
      "Test set: Avg. loss: 7492203.0704, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 309 [0/869 (0%)]\tLoss: 74824528.000000\n",
      "Train Epoch: 309 [160/869 (18%)]\tLoss: 223127104.000000\n",
      "Train Epoch: 309 [320/869 (36%)]\tLoss: 134974032.000000\n",
      "Train Epoch: 309 [480/869 (55%)]\tLoss: 97933152.000000\n",
      "Train Epoch: 309 [640/869 (73%)]\tLoss: 144878944.000000\n",
      "Train Epoch: 309 [800/869 (91%)]\tLoss: 116804272.000000\n",
      "\n",
      "Test set: Avg. loss: 7508120.8785, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 310 [0/869 (0%)]\tLoss: 46815592.000000\n",
      "Train Epoch: 310 [160/869 (18%)]\tLoss: 151411776.000000\n",
      "Train Epoch: 310 [320/869 (36%)]\tLoss: 276525408.000000\n",
      "Train Epoch: 310 [480/869 (55%)]\tLoss: 179289856.000000\n",
      "Train Epoch: 310 [640/869 (73%)]\tLoss: 261117488.000000\n",
      "Train Epoch: 310 [800/869 (91%)]\tLoss: 184672272.000000\n",
      "\n",
      "Test set: Avg. loss: 7496989.3987, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 311 [0/869 (0%)]\tLoss: 119365960.000000\n",
      "Train Epoch: 311 [160/869 (18%)]\tLoss: 240126720.000000\n",
      "Train Epoch: 311 [320/869 (36%)]\tLoss: 88045400.000000\n",
      "Train Epoch: 311 [480/869 (55%)]\tLoss: 53699840.000000\n",
      "Train Epoch: 311 [640/869 (73%)]\tLoss: 119310216.000000\n",
      "Train Epoch: 311 [800/869 (91%)]\tLoss: 87924944.000000\n",
      "\n",
      "Test set: Avg. loss: 7436716.2729, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 312 [0/869 (0%)]\tLoss: 84241712.000000\n",
      "Train Epoch: 312 [160/869 (18%)]\tLoss: 60913628.000000\n",
      "Train Epoch: 312 [320/869 (36%)]\tLoss: 180501440.000000\n",
      "Train Epoch: 312 [480/869 (55%)]\tLoss: 231218000.000000\n",
      "Train Epoch: 312 [640/869 (73%)]\tLoss: 99355528.000000\n",
      "Train Epoch: 312 [800/869 (91%)]\tLoss: 33952564.000000\n",
      "\n",
      "Test set: Avg. loss: 7650597.0618, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 313 [0/869 (0%)]\tLoss: 100975720.000000\n",
      "Train Epoch: 313 [160/869 (18%)]\tLoss: 233144976.000000\n",
      "Train Epoch: 313 [320/869 (36%)]\tLoss: 110630464.000000\n",
      "Train Epoch: 313 [480/869 (55%)]\tLoss: 131189888.000000\n",
      "Train Epoch: 313 [640/869 (73%)]\tLoss: 91185144.000000\n",
      "Train Epoch: 313 [800/869 (91%)]\tLoss: 69639136.000000\n",
      "\n",
      "Test set: Avg. loss: 7506782.9765, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 314 [0/869 (0%)]\tLoss: 40121984.000000\n",
      "Train Epoch: 314 [160/869 (18%)]\tLoss: 236567040.000000\n",
      "Train Epoch: 314 [320/869 (36%)]\tLoss: 161817120.000000\n",
      "Train Epoch: 314 [480/869 (55%)]\tLoss: 68714872.000000\n",
      "Train Epoch: 314 [640/869 (73%)]\tLoss: 128607576.000000\n",
      "Train Epoch: 314 [800/869 (91%)]\tLoss: 67022264.000000\n",
      "\n",
      "Test set: Avg. loss: 7805630.6397, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 315 [0/869 (0%)]\tLoss: 22907744.000000\n",
      "Train Epoch: 315 [160/869 (18%)]\tLoss: 134629376.000000\n",
      "Train Epoch: 315 [320/869 (36%)]\tLoss: 115813496.000000\n",
      "Train Epoch: 315 [480/869 (55%)]\tLoss: 141791040.000000\n",
      "Train Epoch: 315 [640/869 (73%)]\tLoss: 266771040.000000\n",
      "Train Epoch: 315 [800/869 (91%)]\tLoss: 188279952.000000\n",
      "\n",
      "Test set: Avg. loss: 8311001.3390, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 316 [0/869 (0%)]\tLoss: 202600640.000000\n",
      "Train Epoch: 316 [160/869 (18%)]\tLoss: 77024824.000000\n",
      "Train Epoch: 316 [320/869 (36%)]\tLoss: 183815344.000000\n",
      "Train Epoch: 316 [480/869 (55%)]\tLoss: 276062368.000000\n",
      "Train Epoch: 316 [640/869 (73%)]\tLoss: 126986296.000000\n",
      "Train Epoch: 316 [800/869 (91%)]\tLoss: 150096864.000000\n",
      "\n",
      "Test set: Avg. loss: 7421227.1983, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 317 [0/869 (0%)]\tLoss: 184207120.000000\n",
      "Train Epoch: 317 [160/869 (18%)]\tLoss: 35943308.000000\n",
      "Train Epoch: 317 [320/869 (36%)]\tLoss: 161596512.000000\n",
      "Train Epoch: 317 [480/869 (55%)]\tLoss: 159328736.000000\n",
      "Train Epoch: 317 [640/869 (73%)]\tLoss: 49533996.000000\n",
      "Train Epoch: 317 [800/869 (91%)]\tLoss: 76510488.000000\n",
      "\n",
      "Test set: Avg. loss: 7733538.0554, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 318 [0/869 (0%)]\tLoss: 175629792.000000\n",
      "Train Epoch: 318 [160/869 (18%)]\tLoss: 123078176.000000\n",
      "Train Epoch: 318 [320/869 (36%)]\tLoss: 125918504.000000\n",
      "Train Epoch: 318 [480/869 (55%)]\tLoss: 197167072.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 318 [640/869 (73%)]\tLoss: 141352832.000000\n",
      "Train Epoch: 318 [800/869 (91%)]\tLoss: 312740320.000000\n",
      "\n",
      "Test set: Avg. loss: 7399130.7548, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 319 [0/869 (0%)]\tLoss: 89305448.000000\n",
      "Train Epoch: 319 [160/869 (18%)]\tLoss: 175907728.000000\n",
      "Train Epoch: 319 [320/869 (36%)]\tLoss: 253694464.000000\n",
      "Train Epoch: 319 [480/869 (55%)]\tLoss: 126685648.000000\n",
      "Train Epoch: 319 [640/869 (73%)]\tLoss: 196436160.000000\n",
      "Train Epoch: 319 [800/869 (91%)]\tLoss: 42241448.000000\n",
      "\n",
      "Test set: Avg. loss: 7482902.4307, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 320 [0/869 (0%)]\tLoss: 75193264.000000\n",
      "Train Epoch: 320 [160/869 (18%)]\tLoss: 89593624.000000\n",
      "Train Epoch: 320 [320/869 (36%)]\tLoss: 150030192.000000\n",
      "Train Epoch: 320 [480/869 (55%)]\tLoss: 183073600.000000\n",
      "Train Epoch: 320 [640/869 (73%)]\tLoss: 80130472.000000\n",
      "Train Epoch: 320 [800/869 (91%)]\tLoss: 147093920.000000\n",
      "\n",
      "Test set: Avg. loss: 7533004.9382, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 321 [0/869 (0%)]\tLoss: 86819608.000000\n",
      "Train Epoch: 321 [160/869 (18%)]\tLoss: 143857184.000000\n",
      "Train Epoch: 321 [320/869 (36%)]\tLoss: 179552928.000000\n",
      "Train Epoch: 321 [480/869 (55%)]\tLoss: 140047776.000000\n",
      "Train Epoch: 321 [640/869 (73%)]\tLoss: 55806752.000000\n",
      "Train Epoch: 321 [800/869 (91%)]\tLoss: 148243360.000000\n",
      "\n",
      "Test set: Avg. loss: 7536198.2260, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 322 [0/869 (0%)]\tLoss: 142503760.000000\n",
      "Train Epoch: 322 [160/869 (18%)]\tLoss: 66814384.000000\n",
      "Train Epoch: 322 [320/869 (36%)]\tLoss: 219822400.000000\n",
      "Train Epoch: 322 [480/869 (55%)]\tLoss: 136259456.000000\n",
      "Train Epoch: 322 [640/869 (73%)]\tLoss: 100778432.000000\n",
      "Train Epoch: 322 [800/869 (91%)]\tLoss: 96112832.000000\n",
      "\n",
      "Test set: Avg. loss: 7552847.7441, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 323 [0/869 (0%)]\tLoss: 139828400.000000\n",
      "Train Epoch: 323 [160/869 (18%)]\tLoss: 38961280.000000\n",
      "Train Epoch: 323 [320/869 (36%)]\tLoss: 30205618.000000\n",
      "Train Epoch: 323 [480/869 (55%)]\tLoss: 126193632.000000\n",
      "Train Epoch: 323 [640/869 (73%)]\tLoss: 57175952.000000\n",
      "Train Epoch: 323 [800/869 (91%)]\tLoss: 138174880.000000\n",
      "\n",
      "Test set: Avg. loss: 7712568.3753, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 324 [0/869 (0%)]\tLoss: 62886932.000000\n",
      "Train Epoch: 324 [160/869 (18%)]\tLoss: 278740608.000000\n",
      "Train Epoch: 324 [320/869 (36%)]\tLoss: 134861728.000000\n",
      "Train Epoch: 324 [480/869 (55%)]\tLoss: 159226880.000000\n",
      "Train Epoch: 324 [640/869 (73%)]\tLoss: 126726024.000000\n",
      "Train Epoch: 324 [800/869 (91%)]\tLoss: 98527344.000000\n",
      "\n",
      "Test set: Avg. loss: 7354863.8294, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 325 [0/869 (0%)]\tLoss: 179406144.000000\n",
      "Train Epoch: 325 [160/869 (18%)]\tLoss: 85090704.000000\n",
      "Train Epoch: 325 [320/869 (36%)]\tLoss: 41112500.000000\n",
      "Train Epoch: 325 [480/869 (55%)]\tLoss: 53988056.000000\n",
      "Train Epoch: 325 [640/869 (73%)]\tLoss: 58435872.000000\n",
      "Train Epoch: 325 [800/869 (91%)]\tLoss: 156277760.000000\n",
      "\n",
      "Test set: Avg. loss: 7647697.0832, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 326 [0/869 (0%)]\tLoss: 120816336.000000\n",
      "Train Epoch: 326 [160/869 (18%)]\tLoss: 144906752.000000\n",
      "Train Epoch: 326 [320/869 (36%)]\tLoss: 93554192.000000\n",
      "Train Epoch: 326 [480/869 (55%)]\tLoss: 279700800.000000\n",
      "Train Epoch: 326 [640/869 (73%)]\tLoss: 187581776.000000\n",
      "Train Epoch: 326 [800/869 (91%)]\tLoss: 178840160.000000\n",
      "\n",
      "Test set: Avg. loss: 7310704.4520, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 327 [0/869 (0%)]\tLoss: 189315712.000000\n",
      "Train Epoch: 327 [160/869 (18%)]\tLoss: 91901336.000000\n",
      "Train Epoch: 327 [320/869 (36%)]\tLoss: 118708328.000000\n",
      "Train Epoch: 327 [480/869 (55%)]\tLoss: 163750240.000000\n",
      "Train Epoch: 327 [640/869 (73%)]\tLoss: 97365624.000000\n",
      "Train Epoch: 327 [800/869 (91%)]\tLoss: 24126546.000000\n",
      "\n",
      "Test set: Avg. loss: 7600450.5416, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 328 [0/869 (0%)]\tLoss: 59625736.000000\n",
      "Train Epoch: 328 [160/869 (18%)]\tLoss: 169348224.000000\n",
      "Train Epoch: 328 [320/869 (36%)]\tLoss: 93100888.000000\n",
      "Train Epoch: 328 [480/869 (55%)]\tLoss: 52587036.000000\n",
      "Train Epoch: 328 [640/869 (73%)]\tLoss: 134014864.000000\n",
      "Train Epoch: 328 [800/869 (91%)]\tLoss: 105985920.000000\n",
      "\n",
      "Test set: Avg. loss: 7903502.4478, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 329 [0/869 (0%)]\tLoss: 84791584.000000\n",
      "Train Epoch: 329 [160/869 (18%)]\tLoss: 159266448.000000\n",
      "Train Epoch: 329 [320/869 (36%)]\tLoss: 134454288.000000\n",
      "Train Epoch: 329 [480/869 (55%)]\tLoss: 247339408.000000\n",
      "Train Epoch: 329 [640/869 (73%)]\tLoss: 126042392.000000\n",
      "Train Epoch: 329 [800/869 (91%)]\tLoss: 145148928.000000\n",
      "\n",
      "Test set: Avg. loss: 8278158.1578, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 330 [0/869 (0%)]\tLoss: 120208456.000000\n",
      "Train Epoch: 330 [160/869 (18%)]\tLoss: 83162072.000000\n",
      "Train Epoch: 330 [320/869 (36%)]\tLoss: 108426432.000000\n",
      "Train Epoch: 330 [480/869 (55%)]\tLoss: 185372016.000000\n",
      "Train Epoch: 330 [640/869 (73%)]\tLoss: 137285520.000000\n",
      "Train Epoch: 330 [800/869 (91%)]\tLoss: 34013328.000000\n",
      "\n",
      "Test set: Avg. loss: 7399491.3348, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 331 [0/869 (0%)]\tLoss: 132856368.000000\n",
      "Train Epoch: 331 [160/869 (18%)]\tLoss: 103806784.000000\n",
      "Train Epoch: 331 [320/869 (36%)]\tLoss: 87196832.000000\n",
      "Train Epoch: 331 [480/869 (55%)]\tLoss: 33062784.000000\n",
      "Train Epoch: 331 [640/869 (73%)]\tLoss: 174937728.000000\n",
      "Train Epoch: 331 [800/869 (91%)]\tLoss: 63049996.000000\n",
      "\n",
      "Test set: Avg. loss: 7844119.5991, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 332 [0/869 (0%)]\tLoss: 109761072.000000\n",
      "Train Epoch: 332 [160/869 (18%)]\tLoss: 85484176.000000\n",
      "Train Epoch: 332 [320/869 (36%)]\tLoss: 115277384.000000\n",
      "Train Epoch: 332 [480/869 (55%)]\tLoss: 229939008.000000\n",
      "Train Epoch: 332 [640/869 (73%)]\tLoss: 197413824.000000\n",
      "Train Epoch: 332 [800/869 (91%)]\tLoss: 135315040.000000\n",
      "\n",
      "Test set: Avg. loss: 8078411.7569, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 333 [0/869 (0%)]\tLoss: 23852770.000000\n",
      "Train Epoch: 333 [160/869 (18%)]\tLoss: 137958064.000000\n",
      "Train Epoch: 333 [320/869 (36%)]\tLoss: 156574512.000000\n",
      "Train Epoch: 333 [480/869 (55%)]\tLoss: 134574192.000000\n",
      "Train Epoch: 333 [640/869 (73%)]\tLoss: 162831376.000000\n",
      "Train Epoch: 333 [800/869 (91%)]\tLoss: 103627840.000000\n",
      "\n",
      "Test set: Avg. loss: 7451566.7548, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 334 [0/869 (0%)]\tLoss: 139303200.000000\n",
      "Train Epoch: 334 [160/869 (18%)]\tLoss: 117027256.000000\n",
      "Train Epoch: 334 [320/869 (36%)]\tLoss: 103531376.000000\n",
      "Train Epoch: 334 [480/869 (55%)]\tLoss: 138837872.000000\n",
      "Train Epoch: 334 [640/869 (73%)]\tLoss: 123269816.000000\n",
      "Train Epoch: 334 [800/869 (91%)]\tLoss: 16382093.000000\n",
      "\n",
      "Test set: Avg. loss: 7662773.9616, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 335 [0/869 (0%)]\tLoss: 283418912.000000\n",
      "Train Epoch: 335 [160/869 (18%)]\tLoss: 32800402.000000\n",
      "Train Epoch: 335 [320/869 (36%)]\tLoss: 34939032.000000\n",
      "Train Epoch: 335 [480/869 (55%)]\tLoss: 148460448.000000\n",
      "Train Epoch: 335 [640/869 (73%)]\tLoss: 125114200.000000\n",
      "Train Epoch: 335 [800/869 (91%)]\tLoss: 234071168.000000\n",
      "\n",
      "Test set: Avg. loss: 7657553.5181, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 336 [0/869 (0%)]\tLoss: 136247728.000000\n",
      "Train Epoch: 336 [160/869 (18%)]\tLoss: 181643024.000000\n",
      "Train Epoch: 336 [320/869 (36%)]\tLoss: 202492000.000000\n",
      "Train Epoch: 336 [480/869 (55%)]\tLoss: 38512516.000000\n",
      "Train Epoch: 336 [640/869 (73%)]\tLoss: 37983976.000000\n",
      "Train Epoch: 336 [800/869 (91%)]\tLoss: 127641760.000000\n",
      "\n",
      "Test set: Avg. loss: 7395000.1791, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 337 [0/869 (0%)]\tLoss: 148766944.000000\n",
      "Train Epoch: 337 [160/869 (18%)]\tLoss: 151034816.000000\n",
      "Train Epoch: 337 [320/869 (36%)]\tLoss: 132891440.000000\n",
      "Train Epoch: 337 [480/869 (55%)]\tLoss: 239539984.000000\n",
      "Train Epoch: 337 [640/869 (73%)]\tLoss: 57878664.000000\n",
      "Train Epoch: 337 [800/869 (91%)]\tLoss: 33584000.000000\n",
      "\n",
      "Test set: Avg. loss: 7596166.9424, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 338 [0/869 (0%)]\tLoss: 144621344.000000\n",
      "Train Epoch: 338 [160/869 (18%)]\tLoss: 236311088.000000\n",
      "Train Epoch: 338 [320/869 (36%)]\tLoss: 273005344.000000\n",
      "Train Epoch: 338 [480/869 (55%)]\tLoss: 148002208.000000\n",
      "Train Epoch: 338 [640/869 (73%)]\tLoss: 121259680.000000\n",
      "Train Epoch: 338 [800/869 (91%)]\tLoss: 94861568.000000\n",
      "\n",
      "Test set: Avg. loss: 7778596.3241, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 339 [0/869 (0%)]\tLoss: 74302088.000000\n",
      "Train Epoch: 339 [160/869 (18%)]\tLoss: 154087184.000000\n",
      "Train Epoch: 339 [320/869 (36%)]\tLoss: 89672176.000000\n",
      "Train Epoch: 339 [480/869 (55%)]\tLoss: 202979968.000000\n",
      "Train Epoch: 339 [640/869 (73%)]\tLoss: 201499152.000000\n",
      "Train Epoch: 339 [800/869 (91%)]\tLoss: 92593712.000000\n",
      "\n",
      "Test set: Avg. loss: 7403083.7015, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 340 [0/869 (0%)]\tLoss: 107878008.000000\n",
      "Train Epoch: 340 [160/869 (18%)]\tLoss: 221864112.000000\n",
      "Train Epoch: 340 [320/869 (36%)]\tLoss: 148935248.000000\n",
      "Train Epoch: 340 [480/869 (55%)]\tLoss: 44128588.000000\n",
      "Train Epoch: 340 [640/869 (73%)]\tLoss: 117327664.000000\n",
      "Train Epoch: 340 [800/869 (91%)]\tLoss: 79123320.000000\n",
      "\n",
      "Test set: Avg. loss: 7799600.5629, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 341 [0/869 (0%)]\tLoss: 186146016.000000\n",
      "Train Epoch: 341 [160/869 (18%)]\tLoss: 127452488.000000\n",
      "Train Epoch: 341 [320/869 (36%)]\tLoss: 157881904.000000\n",
      "Train Epoch: 341 [480/869 (55%)]\tLoss: 97560944.000000\n",
      "Train Epoch: 341 [640/869 (73%)]\tLoss: 179265456.000000\n",
      "Train Epoch: 341 [800/869 (91%)]\tLoss: 63132144.000000\n",
      "\n",
      "Test set: Avg. loss: 7601744.2260, Accuracy: 0/469 (0%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 342 [0/869 (0%)]\tLoss: 190046496.000000\n",
      "Train Epoch: 342 [160/869 (18%)]\tLoss: 80037992.000000\n",
      "Train Epoch: 342 [320/869 (36%)]\tLoss: 194041792.000000\n",
      "Train Epoch: 342 [480/869 (55%)]\tLoss: 153987552.000000\n",
      "Train Epoch: 342 [640/869 (73%)]\tLoss: 138800000.000000\n",
      "Train Epoch: 342 [800/869 (91%)]\tLoss: 98928176.000000\n",
      "\n",
      "Test set: Avg. loss: 7570170.5757, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 343 [0/869 (0%)]\tLoss: 97855960.000000\n",
      "Train Epoch: 343 [160/869 (18%)]\tLoss: 162840480.000000\n",
      "Train Epoch: 343 [320/869 (36%)]\tLoss: 80700128.000000\n",
      "Train Epoch: 343 [480/869 (55%)]\tLoss: 221190528.000000\n",
      "Train Epoch: 343 [640/869 (73%)]\tLoss: 224855008.000000\n",
      "Train Epoch: 343 [800/869 (91%)]\tLoss: 58156288.000000\n",
      "\n",
      "Test set: Avg. loss: 7803956.5117, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 344 [0/869 (0%)]\tLoss: 51621680.000000\n",
      "Train Epoch: 344 [160/869 (18%)]\tLoss: 82397984.000000\n",
      "Train Epoch: 344 [320/869 (36%)]\tLoss: 161261424.000000\n",
      "Train Epoch: 344 [480/869 (55%)]\tLoss: 103509592.000000\n",
      "Train Epoch: 344 [640/869 (73%)]\tLoss: 117511296.000000\n",
      "Train Epoch: 344 [800/869 (91%)]\tLoss: 152116704.000000\n",
      "\n",
      "Test set: Avg. loss: 7492901.6802, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 345 [0/869 (0%)]\tLoss: 155675504.000000\n",
      "Train Epoch: 345 [160/869 (18%)]\tLoss: 87934792.000000\n",
      "Train Epoch: 345 [320/869 (36%)]\tLoss: 89708056.000000\n",
      "Train Epoch: 345 [480/869 (55%)]\tLoss: 188294048.000000\n",
      "Train Epoch: 345 [640/869 (73%)]\tLoss: 53179780.000000\n",
      "Train Epoch: 345 [800/869 (91%)]\tLoss: 98437160.000000\n",
      "\n",
      "Test set: Avg. loss: 7453379.4968, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 346 [0/869 (0%)]\tLoss: 71018712.000000\n",
      "Train Epoch: 346 [160/869 (18%)]\tLoss: 228786528.000000\n",
      "Train Epoch: 346 [320/869 (36%)]\tLoss: 127589872.000000\n",
      "Train Epoch: 346 [480/869 (55%)]\tLoss: 80188512.000000\n",
      "Train Epoch: 346 [640/869 (73%)]\tLoss: 139065856.000000\n",
      "Train Epoch: 346 [800/869 (91%)]\tLoss: 97905472.000000\n",
      "\n",
      "Test set: Avg. loss: 7735047.9744, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 347 [0/869 (0%)]\tLoss: 96363688.000000\n",
      "Train Epoch: 347 [160/869 (18%)]\tLoss: 252486496.000000\n",
      "Train Epoch: 347 [320/869 (36%)]\tLoss: 118854496.000000\n",
      "Train Epoch: 347 [480/869 (55%)]\tLoss: 216036560.000000\n",
      "Train Epoch: 347 [640/869 (73%)]\tLoss: 165292896.000000\n",
      "Train Epoch: 347 [800/869 (91%)]\tLoss: 70650408.000000\n",
      "\n",
      "Test set: Avg. loss: 7538492.5885, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 348 [0/869 (0%)]\tLoss: 108687072.000000\n",
      "Train Epoch: 348 [160/869 (18%)]\tLoss: 176249904.000000\n",
      "Train Epoch: 348 [320/869 (36%)]\tLoss: 96411280.000000\n",
      "Train Epoch: 348 [480/869 (55%)]\tLoss: 101883952.000000\n",
      "Train Epoch: 348 [640/869 (73%)]\tLoss: 77525936.000000\n",
      "Train Epoch: 348 [800/869 (91%)]\tLoss: 182709840.000000\n",
      "\n",
      "Test set: Avg. loss: 7551421.0917, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 349 [0/869 (0%)]\tLoss: 96489992.000000\n",
      "Train Epoch: 349 [160/869 (18%)]\tLoss: 136665568.000000\n",
      "Train Epoch: 349 [320/869 (36%)]\tLoss: 98952816.000000\n",
      "Train Epoch: 349 [480/869 (55%)]\tLoss: 141967344.000000\n",
      "Train Epoch: 349 [640/869 (73%)]\tLoss: 57429188.000000\n",
      "Train Epoch: 349 [800/869 (91%)]\tLoss: 114351616.000000\n",
      "\n",
      "Test set: Avg. loss: 7425851.0448, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 350 [0/869 (0%)]\tLoss: 102013032.000000\n",
      "Train Epoch: 350 [160/869 (18%)]\tLoss: 126181376.000000\n",
      "Train Epoch: 350 [320/869 (36%)]\tLoss: 59427064.000000\n",
      "Train Epoch: 350 [480/869 (55%)]\tLoss: 62163392.000000\n",
      "Train Epoch: 350 [640/869 (73%)]\tLoss: 262461536.000000\n",
      "Train Epoch: 350 [800/869 (91%)]\tLoss: 139238048.000000\n",
      "\n",
      "Test set: Avg. loss: 7470439.7953, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 351 [0/869 (0%)]\tLoss: 97454192.000000\n",
      "Train Epoch: 351 [160/869 (18%)]\tLoss: 133706568.000000\n",
      "Train Epoch: 351 [320/869 (36%)]\tLoss: 79157408.000000\n",
      "Train Epoch: 351 [480/869 (55%)]\tLoss: 94600120.000000\n",
      "Train Epoch: 351 [640/869 (73%)]\tLoss: 100493888.000000\n",
      "Train Epoch: 351 [800/869 (91%)]\tLoss: 254133824.000000\n",
      "\n",
      "Test set: Avg. loss: 7378038.7036, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 352 [0/869 (0%)]\tLoss: 101202256.000000\n",
      "Train Epoch: 352 [160/869 (18%)]\tLoss: 101474224.000000\n",
      "Train Epoch: 352 [320/869 (36%)]\tLoss: 150241008.000000\n",
      "Train Epoch: 352 [480/869 (55%)]\tLoss: 153407168.000000\n",
      "Train Epoch: 352 [640/869 (73%)]\tLoss: 86219144.000000\n",
      "Train Epoch: 352 [800/869 (91%)]\tLoss: 187861504.000000\n",
      "\n",
      "Test set: Avg. loss: 7362061.3475, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 353 [0/869 (0%)]\tLoss: 144698128.000000\n",
      "Train Epoch: 353 [160/869 (18%)]\tLoss: 158092656.000000\n",
      "Train Epoch: 353 [320/869 (36%)]\tLoss: 45831876.000000\n",
      "Train Epoch: 353 [480/869 (55%)]\tLoss: 106859856.000000\n",
      "Train Epoch: 353 [640/869 (73%)]\tLoss: 161427632.000000\n",
      "Train Epoch: 353 [800/869 (91%)]\tLoss: 168123392.000000\n",
      "\n",
      "Test set: Avg. loss: 7337880.0853, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 354 [0/869 (0%)]\tLoss: 180772544.000000\n",
      "Train Epoch: 354 [160/869 (18%)]\tLoss: 117212144.000000\n",
      "Train Epoch: 354 [320/869 (36%)]\tLoss: 147334688.000000\n",
      "Train Epoch: 354 [480/869 (55%)]\tLoss: 116031424.000000\n",
      "Train Epoch: 354 [640/869 (73%)]\tLoss: 86164560.000000\n",
      "Train Epoch: 354 [800/869 (91%)]\tLoss: 146262256.000000\n",
      "\n",
      "Test set: Avg. loss: 7565229.1429, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 355 [0/869 (0%)]\tLoss: 168597424.000000\n",
      "Train Epoch: 355 [160/869 (18%)]\tLoss: 134564416.000000\n",
      "Train Epoch: 355 [320/869 (36%)]\tLoss: 169458784.000000\n",
      "Train Epoch: 355 [480/869 (55%)]\tLoss: 155698176.000000\n",
      "Train Epoch: 355 [640/869 (73%)]\tLoss: 108580408.000000\n",
      "Train Epoch: 355 [800/869 (91%)]\tLoss: 240606208.000000\n",
      "\n",
      "Test set: Avg. loss: 7608208.8017, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 356 [0/869 (0%)]\tLoss: 155394832.000000\n",
      "Train Epoch: 356 [160/869 (18%)]\tLoss: 22366724.000000\n",
      "Train Epoch: 356 [320/869 (36%)]\tLoss: 104594808.000000\n",
      "Train Epoch: 356 [480/869 (55%)]\tLoss: 206601872.000000\n",
      "Train Epoch: 356 [640/869 (73%)]\tLoss: 172011440.000000\n",
      "Train Epoch: 356 [800/869 (91%)]\tLoss: 89423392.000000\n",
      "\n",
      "Test set: Avg. loss: 7473102.2090, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 357 [0/869 (0%)]\tLoss: 204143392.000000\n",
      "Train Epoch: 357 [160/869 (18%)]\tLoss: 184895136.000000\n",
      "Train Epoch: 357 [320/869 (36%)]\tLoss: 201154192.000000\n",
      "Train Epoch: 357 [480/869 (55%)]\tLoss: 96746944.000000\n",
      "Train Epoch: 357 [640/869 (73%)]\tLoss: 143660080.000000\n",
      "Train Epoch: 357 [800/869 (91%)]\tLoss: 160478848.000000\n",
      "\n",
      "Test set: Avg. loss: 7413303.2324, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 358 [0/869 (0%)]\tLoss: 331070112.000000\n",
      "Train Epoch: 358 [160/869 (18%)]\tLoss: 153615600.000000\n",
      "Train Epoch: 358 [320/869 (36%)]\tLoss: 224208464.000000\n",
      "Train Epoch: 358 [480/869 (55%)]\tLoss: 194116112.000000\n",
      "Train Epoch: 358 [640/869 (73%)]\tLoss: 95268584.000000\n",
      "Train Epoch: 358 [800/869 (91%)]\tLoss: 150033792.000000\n",
      "\n",
      "Test set: Avg. loss: 7494674.1407, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 359 [0/869 (0%)]\tLoss: 128980408.000000\n",
      "Train Epoch: 359 [160/869 (18%)]\tLoss: 82212168.000000\n",
      "Train Epoch: 359 [320/869 (36%)]\tLoss: 120847440.000000\n",
      "Train Epoch: 359 [480/869 (55%)]\tLoss: 141770432.000000\n",
      "Train Epoch: 359 [640/869 (73%)]\tLoss: 61949936.000000\n",
      "Train Epoch: 359 [800/869 (91%)]\tLoss: 191242336.000000\n",
      "\n",
      "Test set: Avg. loss: 7701286.8998, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 360 [0/869 (0%)]\tLoss: 111763384.000000\n",
      "Train Epoch: 360 [160/869 (18%)]\tLoss: 129202104.000000\n",
      "Train Epoch: 360 [320/869 (36%)]\tLoss: 96940672.000000\n",
      "Train Epoch: 360 [480/869 (55%)]\tLoss: 42213528.000000\n",
      "Train Epoch: 360 [640/869 (73%)]\tLoss: 93531352.000000\n",
      "Train Epoch: 360 [800/869 (91%)]\tLoss: 75851032.000000\n",
      "\n",
      "Test set: Avg. loss: 7568758.4392, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 361 [0/869 (0%)]\tLoss: 197675184.000000\n",
      "Train Epoch: 361 [160/869 (18%)]\tLoss: 199098080.000000\n",
      "Train Epoch: 361 [320/869 (36%)]\tLoss: 42355460.000000\n",
      "Train Epoch: 361 [480/869 (55%)]\tLoss: 135360912.000000\n",
      "Train Epoch: 361 [640/869 (73%)]\tLoss: 36269416.000000\n",
      "Train Epoch: 361 [800/869 (91%)]\tLoss: 118018960.000000\n",
      "\n",
      "Test set: Avg. loss: 7503652.4819, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 362 [0/869 (0%)]\tLoss: 117660744.000000\n",
      "Train Epoch: 362 [160/869 (18%)]\tLoss: 161206480.000000\n",
      "Train Epoch: 362 [320/869 (36%)]\tLoss: 214439856.000000\n",
      "Train Epoch: 362 [480/869 (55%)]\tLoss: 147655024.000000\n",
      "Train Epoch: 362 [640/869 (73%)]\tLoss: 238870208.000000\n",
      "Train Epoch: 362 [800/869 (91%)]\tLoss: 97122528.000000\n",
      "\n",
      "Test set: Avg. loss: 7544549.9446, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 363 [0/869 (0%)]\tLoss: 88234128.000000\n",
      "Train Epoch: 363 [160/869 (18%)]\tLoss: 155827312.000000\n",
      "Train Epoch: 363 [320/869 (36%)]\tLoss: 95376008.000000\n",
      "Train Epoch: 363 [480/869 (55%)]\tLoss: 48383560.000000\n",
      "Train Epoch: 363 [640/869 (73%)]\tLoss: 113268176.000000\n",
      "Train Epoch: 363 [800/869 (91%)]\tLoss: 97830928.000000\n",
      "\n",
      "Test set: Avg. loss: 7647585.4499, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 364 [0/869 (0%)]\tLoss: 143284064.000000\n",
      "Train Epoch: 364 [160/869 (18%)]\tLoss: 193594912.000000\n",
      "Train Epoch: 364 [320/869 (36%)]\tLoss: 123838408.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 364 [480/869 (55%)]\tLoss: 138424256.000000\n",
      "Train Epoch: 364 [640/869 (73%)]\tLoss: 138182432.000000\n",
      "Train Epoch: 364 [800/869 (91%)]\tLoss: 101756160.000000\n",
      "\n",
      "Test set: Avg. loss: 7323087.4968, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 365 [0/869 (0%)]\tLoss: 119161760.000000\n",
      "Train Epoch: 365 [160/869 (18%)]\tLoss: 77251904.000000\n",
      "Train Epoch: 365 [320/869 (36%)]\tLoss: 121519984.000000\n",
      "Train Epoch: 365 [480/869 (55%)]\tLoss: 298502336.000000\n",
      "Train Epoch: 365 [640/869 (73%)]\tLoss: 39220356.000000\n",
      "Train Epoch: 365 [800/869 (91%)]\tLoss: 129998784.000000\n",
      "\n",
      "Test set: Avg. loss: 7336844.7676, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 366 [0/869 (0%)]\tLoss: 158141584.000000\n",
      "Train Epoch: 366 [160/869 (18%)]\tLoss: 183116208.000000\n",
      "Train Epoch: 366 [320/869 (36%)]\tLoss: 171936832.000000\n",
      "Train Epoch: 366 [480/869 (55%)]\tLoss: 45410872.000000\n",
      "Train Epoch: 366 [640/869 (73%)]\tLoss: 156535392.000000\n",
      "Train Epoch: 366 [800/869 (91%)]\tLoss: 36951768.000000\n",
      "\n",
      "Test set: Avg. loss: 7693605.1684, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 367 [0/869 (0%)]\tLoss: 197063648.000000\n",
      "Train Epoch: 367 [160/869 (18%)]\tLoss: 149249440.000000\n",
      "Train Epoch: 367 [320/869 (36%)]\tLoss: 87637624.000000\n",
      "Train Epoch: 367 [480/869 (55%)]\tLoss: 106999152.000000\n",
      "Train Epoch: 367 [640/869 (73%)]\tLoss: 146979776.000000\n",
      "Train Epoch: 367 [800/869 (91%)]\tLoss: 79960472.000000\n",
      "\n",
      "Test set: Avg. loss: 7432441.5011, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 368 [0/869 (0%)]\tLoss: 176138960.000000\n",
      "Train Epoch: 368 [160/869 (18%)]\tLoss: 219876352.000000\n",
      "Train Epoch: 368 [320/869 (36%)]\tLoss: 52992832.000000\n",
      "Train Epoch: 368 [480/869 (55%)]\tLoss: 103469384.000000\n",
      "Train Epoch: 368 [640/869 (73%)]\tLoss: 27120534.000000\n",
      "Train Epoch: 368 [800/869 (91%)]\tLoss: 126049992.000000\n",
      "\n",
      "Test set: Avg. loss: 7444227.8465, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 369 [0/869 (0%)]\tLoss: 144818416.000000\n",
      "Train Epoch: 369 [160/869 (18%)]\tLoss: 80959416.000000\n",
      "Train Epoch: 369 [320/869 (36%)]\tLoss: 212520416.000000\n",
      "Train Epoch: 369 [480/869 (55%)]\tLoss: 222258768.000000\n",
      "Train Epoch: 369 [640/869 (73%)]\tLoss: 170167136.000000\n",
      "Train Epoch: 369 [800/869 (91%)]\tLoss: 172035216.000000\n",
      "\n",
      "Test set: Avg. loss: 7716887.1898, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 370 [0/869 (0%)]\tLoss: 212120048.000000\n",
      "Train Epoch: 370 [160/869 (18%)]\tLoss: 111388712.000000\n",
      "Train Epoch: 370 [320/869 (36%)]\tLoss: 123644656.000000\n",
      "Train Epoch: 370 [480/869 (55%)]\tLoss: 104693648.000000\n",
      "Train Epoch: 370 [640/869 (73%)]\tLoss: 185694992.000000\n",
      "Train Epoch: 370 [800/869 (91%)]\tLoss: 81616824.000000\n",
      "\n",
      "Test set: Avg. loss: 7433251.1898, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 371 [0/869 (0%)]\tLoss: 212061376.000000\n",
      "Train Epoch: 371 [160/869 (18%)]\tLoss: 161434016.000000\n",
      "Train Epoch: 371 [320/869 (36%)]\tLoss: 342611488.000000\n",
      "Train Epoch: 371 [480/869 (55%)]\tLoss: 97303248.000000\n",
      "Train Epoch: 371 [640/869 (73%)]\tLoss: 184079728.000000\n",
      "Train Epoch: 371 [800/869 (91%)]\tLoss: 103631104.000000\n",
      "\n",
      "Test set: Avg. loss: 7574070.6141, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 372 [0/869 (0%)]\tLoss: 130643384.000000\n",
      "Train Epoch: 372 [160/869 (18%)]\tLoss: 178555200.000000\n",
      "Train Epoch: 372 [320/869 (36%)]\tLoss: 97705192.000000\n",
      "Train Epoch: 372 [480/869 (55%)]\tLoss: 126266656.000000\n",
      "Train Epoch: 372 [640/869 (73%)]\tLoss: 82498232.000000\n",
      "Train Epoch: 372 [800/869 (91%)]\tLoss: 110356624.000000\n",
      "\n",
      "Test set: Avg. loss: 7516378.0640, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 373 [0/869 (0%)]\tLoss: 185365792.000000\n",
      "Train Epoch: 373 [160/869 (18%)]\tLoss: 231646320.000000\n",
      "Train Epoch: 373 [320/869 (36%)]\tLoss: 63512544.000000\n",
      "Train Epoch: 373 [480/869 (55%)]\tLoss: 76796408.000000\n",
      "Train Epoch: 373 [640/869 (73%)]\tLoss: 62000012.000000\n",
      "Train Epoch: 373 [800/869 (91%)]\tLoss: 151202368.000000\n",
      "\n",
      "Test set: Avg. loss: 7789815.4925, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 374 [0/869 (0%)]\tLoss: 103190624.000000\n",
      "Train Epoch: 374 [160/869 (18%)]\tLoss: 105888536.000000\n",
      "Train Epoch: 374 [320/869 (36%)]\tLoss: 89133032.000000\n",
      "Train Epoch: 374 [480/869 (55%)]\tLoss: 167201440.000000\n",
      "Train Epoch: 374 [640/869 (73%)]\tLoss: 78756440.000000\n",
      "Train Epoch: 374 [800/869 (91%)]\tLoss: 41135584.000000\n",
      "\n",
      "Test set: Avg. loss: 7456189.0490, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 375 [0/869 (0%)]\tLoss: 118331568.000000\n",
      "Train Epoch: 375 [160/869 (18%)]\tLoss: 227121984.000000\n",
      "Train Epoch: 375 [320/869 (36%)]\tLoss: 118840896.000000\n",
      "Train Epoch: 375 [480/869 (55%)]\tLoss: 73969992.000000\n",
      "Train Epoch: 375 [640/869 (73%)]\tLoss: 39534252.000000\n",
      "Train Epoch: 375 [800/869 (91%)]\tLoss: 89007728.000000\n",
      "\n",
      "Test set: Avg. loss: 7835585.9616, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 376 [0/869 (0%)]\tLoss: 136280672.000000\n",
      "Train Epoch: 376 [160/869 (18%)]\tLoss: 92295832.000000\n",
      "Train Epoch: 376 [320/869 (36%)]\tLoss: 127117872.000000\n",
      "Train Epoch: 376 [480/869 (55%)]\tLoss: 110307168.000000\n",
      "Train Epoch: 376 [640/869 (73%)]\tLoss: 51927028.000000\n",
      "Train Epoch: 376 [800/869 (91%)]\tLoss: 50906896.000000\n",
      "\n",
      "Test set: Avg. loss: 7554913.2537, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 377 [0/869 (0%)]\tLoss: 228741424.000000\n",
      "Train Epoch: 377 [160/869 (18%)]\tLoss: 109752584.000000\n",
      "Train Epoch: 377 [320/869 (36%)]\tLoss: 152604208.000000\n",
      "Train Epoch: 377 [480/869 (55%)]\tLoss: 129752576.000000\n",
      "Train Epoch: 377 [640/869 (73%)]\tLoss: 106117288.000000\n",
      "Train Epoch: 377 [800/869 (91%)]\tLoss: 85743432.000000\n",
      "\n",
      "Test set: Avg. loss: 7502222.7889, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 378 [0/869 (0%)]\tLoss: 62234116.000000\n",
      "Train Epoch: 378 [160/869 (18%)]\tLoss: 199193440.000000\n",
      "Train Epoch: 378 [320/869 (36%)]\tLoss: 180612224.000000\n",
      "Train Epoch: 378 [480/869 (55%)]\tLoss: 190837984.000000\n",
      "Train Epoch: 378 [640/869 (73%)]\tLoss: 140454192.000000\n",
      "Train Epoch: 378 [800/869 (91%)]\tLoss: 104136984.000000\n",
      "\n",
      "Test set: Avg. loss: 7314228.2388, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 379 [0/869 (0%)]\tLoss: 126586264.000000\n",
      "Train Epoch: 379 [160/869 (18%)]\tLoss: 172153616.000000\n",
      "Train Epoch: 379 [320/869 (36%)]\tLoss: 256457792.000000\n",
      "Train Epoch: 379 [480/869 (55%)]\tLoss: 40938652.000000\n",
      "Train Epoch: 379 [640/869 (73%)]\tLoss: 48613136.000000\n",
      "Train Epoch: 379 [800/869 (91%)]\tLoss: 229386032.000000\n",
      "\n",
      "Test set: Avg. loss: 7948553.6802, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 380 [0/869 (0%)]\tLoss: 268880768.000000\n",
      "Train Epoch: 380 [160/869 (18%)]\tLoss: 95719896.000000\n",
      "Train Epoch: 380 [320/869 (36%)]\tLoss: 38080316.000000\n",
      "Train Epoch: 380 [480/869 (55%)]\tLoss: 57618836.000000\n",
      "Train Epoch: 380 [640/869 (73%)]\tLoss: 99124792.000000\n",
      "Train Epoch: 380 [800/869 (91%)]\tLoss: 186624464.000000\n",
      "\n",
      "Test set: Avg. loss: 7523816.4947, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 381 [0/869 (0%)]\tLoss: 70043280.000000\n",
      "Train Epoch: 381 [160/869 (18%)]\tLoss: 158094240.000000\n",
      "Train Epoch: 381 [320/869 (36%)]\tLoss: 90375344.000000\n",
      "Train Epoch: 381 [480/869 (55%)]\tLoss: 168644112.000000\n",
      "Train Epoch: 381 [640/869 (73%)]\tLoss: 180077280.000000\n",
      "Train Epoch: 381 [800/869 (91%)]\tLoss: 121881400.000000\n",
      "\n",
      "Test set: Avg. loss: 7689157.5096, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 382 [0/869 (0%)]\tLoss: 131733632.000000\n",
      "Train Epoch: 382 [160/869 (18%)]\tLoss: 105569784.000000\n",
      "Train Epoch: 382 [320/869 (36%)]\tLoss: 44806808.000000\n",
      "Train Epoch: 382 [480/869 (55%)]\tLoss: 113665696.000000\n",
      "Train Epoch: 382 [640/869 (73%)]\tLoss: 94075608.000000\n",
      "Train Epoch: 382 [800/869 (91%)]\tLoss: 131072440.000000\n",
      "\n",
      "Test set: Avg. loss: 7459085.6716, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 383 [0/869 (0%)]\tLoss: 56425324.000000\n",
      "Train Epoch: 383 [160/869 (18%)]\tLoss: 155372736.000000\n",
      "Train Epoch: 383 [320/869 (36%)]\tLoss: 114972760.000000\n",
      "Train Epoch: 383 [480/869 (55%)]\tLoss: 80644648.000000\n",
      "Train Epoch: 383 [640/869 (73%)]\tLoss: 116245992.000000\n",
      "Train Epoch: 383 [800/869 (91%)]\tLoss: 120064632.000000\n",
      "\n",
      "Test set: Avg. loss: 7414111.7868, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 384 [0/869 (0%)]\tLoss: 159110176.000000\n",
      "Train Epoch: 384 [160/869 (18%)]\tLoss: 102965392.000000\n",
      "Train Epoch: 384 [320/869 (36%)]\tLoss: 53491496.000000\n",
      "Train Epoch: 384 [480/869 (55%)]\tLoss: 134928048.000000\n",
      "Train Epoch: 384 [640/869 (73%)]\tLoss: 165369136.000000\n",
      "Train Epoch: 384 [800/869 (91%)]\tLoss: 40548792.000000\n",
      "\n",
      "Test set: Avg. loss: 7580293.3220, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 385 [0/869 (0%)]\tLoss: 79190160.000000\n",
      "Train Epoch: 385 [160/869 (18%)]\tLoss: 31939992.000000\n",
      "Train Epoch: 385 [320/869 (36%)]\tLoss: 146161120.000000\n",
      "Train Epoch: 385 [480/869 (55%)]\tLoss: 109660752.000000\n",
      "Train Epoch: 385 [640/869 (73%)]\tLoss: 214633488.000000\n",
      "Train Epoch: 385 [800/869 (91%)]\tLoss: 139537104.000000\n",
      "\n",
      "Test set: Avg. loss: 7343319.0448, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 386 [0/869 (0%)]\tLoss: 143943424.000000\n",
      "Train Epoch: 386 [160/869 (18%)]\tLoss: 76953752.000000\n",
      "Train Epoch: 386 [320/869 (36%)]\tLoss: 82997944.000000\n",
      "Train Epoch: 386 [480/869 (55%)]\tLoss: 122139824.000000\n",
      "Train Epoch: 386 [640/869 (73%)]\tLoss: 99898616.000000\n",
      "Train Epoch: 386 [800/869 (91%)]\tLoss: 235472096.000000\n",
      "\n",
      "Test set: Avg. loss: 7586536.8998, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 387 [0/869 (0%)]\tLoss: 91481760.000000\n",
      "Train Epoch: 387 [160/869 (18%)]\tLoss: 71679368.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 387 [320/869 (36%)]\tLoss: 206615376.000000\n",
      "Train Epoch: 387 [480/869 (55%)]\tLoss: 170976768.000000\n",
      "Train Epoch: 387 [640/869 (73%)]\tLoss: 189827232.000000\n",
      "Train Epoch: 387 [800/869 (91%)]\tLoss: 273563968.000000\n",
      "\n",
      "Test set: Avg. loss: 7646345.3987, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 388 [0/869 (0%)]\tLoss: 160772000.000000\n",
      "Train Epoch: 388 [160/869 (18%)]\tLoss: 129131824.000000\n",
      "Train Epoch: 388 [320/869 (36%)]\tLoss: 156827376.000000\n",
      "Train Epoch: 388 [480/869 (55%)]\tLoss: 268801600.000000\n",
      "Train Epoch: 388 [640/869 (73%)]\tLoss: 111817456.000000\n",
      "Train Epoch: 388 [800/869 (91%)]\tLoss: 168685376.000000\n",
      "\n",
      "Test set: Avg. loss: 7589164.6397, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 389 [0/869 (0%)]\tLoss: 169595088.000000\n",
      "Train Epoch: 389 [160/869 (18%)]\tLoss: 82837192.000000\n",
      "Train Epoch: 389 [320/869 (36%)]\tLoss: 174843488.000000\n",
      "Train Epoch: 389 [480/869 (55%)]\tLoss: 162169152.000000\n",
      "Train Epoch: 389 [640/869 (73%)]\tLoss: 148015936.000000\n",
      "Train Epoch: 389 [800/869 (91%)]\tLoss: 122520608.000000\n",
      "\n",
      "Test set: Avg. loss: 7598459.7484, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 390 [0/869 (0%)]\tLoss: 124705488.000000\n",
      "Train Epoch: 390 [160/869 (18%)]\tLoss: 218653024.000000\n",
      "Train Epoch: 390 [320/869 (36%)]\tLoss: 83851192.000000\n",
      "Train Epoch: 390 [480/869 (55%)]\tLoss: 121711864.000000\n",
      "Train Epoch: 390 [640/869 (73%)]\tLoss: 86542064.000000\n",
      "Train Epoch: 390 [800/869 (91%)]\tLoss: 82390400.000000\n",
      "\n",
      "Test set: Avg. loss: 7361291.5394, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 391 [0/869 (0%)]\tLoss: 138470000.000000\n",
      "Train Epoch: 391 [160/869 (18%)]\tLoss: 75483352.000000\n",
      "Train Epoch: 391 [320/869 (36%)]\tLoss: 121398000.000000\n",
      "Train Epoch: 391 [480/869 (55%)]\tLoss: 109582184.000000\n",
      "Train Epoch: 391 [640/869 (73%)]\tLoss: 207466496.000000\n",
      "Train Epoch: 391 [800/869 (91%)]\tLoss: 35782740.000000\n",
      "\n",
      "Test set: Avg. loss: 7596352.6482, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 392 [0/869 (0%)]\tLoss: 186597536.000000\n",
      "Train Epoch: 392 [160/869 (18%)]\tLoss: 103929480.000000\n",
      "Train Epoch: 392 [320/869 (36%)]\tLoss: 203456720.000000\n",
      "Train Epoch: 392 [480/869 (55%)]\tLoss: 147952256.000000\n",
      "Train Epoch: 392 [640/869 (73%)]\tLoss: 44201900.000000\n",
      "Train Epoch: 392 [800/869 (91%)]\tLoss: 91944200.000000\n",
      "\n",
      "Test set: Avg. loss: 7530064.4179, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 393 [0/869 (0%)]\tLoss: 41996116.000000\n",
      "Train Epoch: 393 [160/869 (18%)]\tLoss: 125922704.000000\n",
      "Train Epoch: 393 [320/869 (36%)]\tLoss: 203836224.000000\n",
      "Train Epoch: 393 [480/869 (55%)]\tLoss: 276828096.000000\n",
      "Train Epoch: 393 [640/869 (73%)]\tLoss: 130279640.000000\n",
      "Train Epoch: 393 [800/869 (91%)]\tLoss: 233174832.000000\n",
      "\n",
      "Test set: Avg. loss: 7468681.5949, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 394 [0/869 (0%)]\tLoss: 123826176.000000\n",
      "Train Epoch: 394 [160/869 (18%)]\tLoss: 92213608.000000\n",
      "Train Epoch: 394 [320/869 (36%)]\tLoss: 188584688.000000\n",
      "Train Epoch: 394 [480/869 (55%)]\tLoss: 227832496.000000\n",
      "Train Epoch: 394 [640/869 (73%)]\tLoss: 169010656.000000\n",
      "Train Epoch: 394 [800/869 (91%)]\tLoss: 144275616.000000\n",
      "\n",
      "Test set: Avg. loss: 7543059.1215, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 395 [0/869 (0%)]\tLoss: 161023456.000000\n",
      "Train Epoch: 395 [160/869 (18%)]\tLoss: 231466624.000000\n",
      "Train Epoch: 395 [320/869 (36%)]\tLoss: 184862896.000000\n",
      "Train Epoch: 395 [480/869 (55%)]\tLoss: 151297888.000000\n",
      "Train Epoch: 395 [640/869 (73%)]\tLoss: 49568736.000000\n",
      "Train Epoch: 395 [800/869 (91%)]\tLoss: 183524416.000000\n",
      "\n",
      "Test set: Avg. loss: 7529754.5928, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 396 [0/869 (0%)]\tLoss: 87725248.000000\n",
      "Train Epoch: 396 [160/869 (18%)]\tLoss: 37380440.000000\n",
      "Train Epoch: 396 [320/869 (36%)]\tLoss: 179256416.000000\n",
      "Train Epoch: 396 [480/869 (55%)]\tLoss: 212846064.000000\n",
      "Train Epoch: 396 [640/869 (73%)]\tLoss: 91093960.000000\n",
      "Train Epoch: 396 [800/869 (91%)]\tLoss: 72471128.000000\n",
      "\n",
      "Test set: Avg. loss: 7532055.0362, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 397 [0/869 (0%)]\tLoss: 124880936.000000\n",
      "Train Epoch: 397 [160/869 (18%)]\tLoss: 133914104.000000\n",
      "Train Epoch: 397 [320/869 (36%)]\tLoss: 175511184.000000\n",
      "Train Epoch: 397 [480/869 (55%)]\tLoss: 100539096.000000\n",
      "Train Epoch: 397 [640/869 (73%)]\tLoss: 76554168.000000\n",
      "Train Epoch: 397 [800/869 (91%)]\tLoss: 96938048.000000\n",
      "\n",
      "Test set: Avg. loss: 7576571.2751, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 398 [0/869 (0%)]\tLoss: 323225792.000000\n",
      "Train Epoch: 398 [160/869 (18%)]\tLoss: 67203112.000000\n",
      "Train Epoch: 398 [320/869 (36%)]\tLoss: 100878832.000000\n",
      "Train Epoch: 398 [480/869 (55%)]\tLoss: 122966976.000000\n",
      "Train Epoch: 398 [640/869 (73%)]\tLoss: 42580840.000000\n",
      "Train Epoch: 398 [800/869 (91%)]\tLoss: 74170352.000000\n",
      "\n",
      "Test set: Avg. loss: 7583096.2644, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 399 [0/869 (0%)]\tLoss: 249362880.000000\n",
      "Train Epoch: 399 [160/869 (18%)]\tLoss: 133581664.000000\n",
      "Train Epoch: 399 [320/869 (36%)]\tLoss: 186248208.000000\n",
      "Train Epoch: 399 [480/869 (55%)]\tLoss: 48344752.000000\n",
      "Train Epoch: 399 [640/869 (73%)]\tLoss: 208440768.000000\n",
      "Train Epoch: 399 [800/869 (91%)]\tLoss: 123380624.000000\n",
      "\n",
      "Test set: Avg. loss: 7538991.5736, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 400 [0/869 (0%)]\tLoss: 238267840.000000\n",
      "Train Epoch: 400 [160/869 (18%)]\tLoss: 128705608.000000\n",
      "Train Epoch: 400 [320/869 (36%)]\tLoss: 42182944.000000\n",
      "Train Epoch: 400 [480/869 (55%)]\tLoss: 29261148.000000\n",
      "Train Epoch: 400 [640/869 (73%)]\tLoss: 239054832.000000\n",
      "Train Epoch: 400 [800/869 (91%)]\tLoss: 83673272.000000\n",
      "\n",
      "Test set: Avg. loss: 7677878.3667, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 401 [0/869 (0%)]\tLoss: 43558560.000000\n",
      "Train Epoch: 401 [160/869 (18%)]\tLoss: 103533832.000000\n",
      "Train Epoch: 401 [320/869 (36%)]\tLoss: 86279464.000000\n",
      "Train Epoch: 401 [480/869 (55%)]\tLoss: 204157760.000000\n",
      "Train Epoch: 401 [640/869 (73%)]\tLoss: 201411600.000000\n",
      "Train Epoch: 401 [800/869 (91%)]\tLoss: 90528048.000000\n",
      "\n",
      "Test set: Avg. loss: 7733891.7484, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 402 [0/869 (0%)]\tLoss: 27739734.000000\n",
      "Train Epoch: 402 [160/869 (18%)]\tLoss: 94576624.000000\n",
      "Train Epoch: 402 [320/869 (36%)]\tLoss: 116731808.000000\n",
      "Train Epoch: 402 [480/869 (55%)]\tLoss: 193265296.000000\n",
      "Train Epoch: 402 [640/869 (73%)]\tLoss: 44772908.000000\n",
      "Train Epoch: 402 [800/869 (91%)]\tLoss: 210960848.000000\n",
      "\n",
      "Test set: Avg. loss: 7672402.1834, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 403 [0/869 (0%)]\tLoss: 71705648.000000\n",
      "Train Epoch: 403 [160/869 (18%)]\tLoss: 152944880.000000\n",
      "Train Epoch: 403 [320/869 (36%)]\tLoss: 165134496.000000\n",
      "Train Epoch: 403 [480/869 (55%)]\tLoss: 142506400.000000\n",
      "Train Epoch: 403 [640/869 (73%)]\tLoss: 147741040.000000\n",
      "Train Epoch: 403 [800/869 (91%)]\tLoss: 77297344.000000\n",
      "\n",
      "Test set: Avg. loss: 7757236.3412, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 404 [0/869 (0%)]\tLoss: 114380952.000000\n",
      "Train Epoch: 404 [160/869 (18%)]\tLoss: 177892464.000000\n",
      "Train Epoch: 404 [320/869 (36%)]\tLoss: 218996752.000000\n",
      "Train Epoch: 404 [480/869 (55%)]\tLoss: 222077248.000000\n",
      "Train Epoch: 404 [640/869 (73%)]\tLoss: 90026016.000000\n",
      "Train Epoch: 404 [800/869 (91%)]\tLoss: 254455936.000000\n",
      "\n",
      "Test set: Avg. loss: 7501467.3092, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 405 [0/869 (0%)]\tLoss: 142540512.000000\n",
      "Train Epoch: 405 [160/869 (18%)]\tLoss: 211445440.000000\n",
      "Train Epoch: 405 [320/869 (36%)]\tLoss: 136849360.000000\n",
      "Train Epoch: 405 [480/869 (55%)]\tLoss: 93820800.000000\n",
      "Train Epoch: 405 [640/869 (73%)]\tLoss: 164137216.000000\n",
      "Train Epoch: 405 [800/869 (91%)]\tLoss: 125422352.000000\n",
      "\n",
      "Test set: Avg. loss: 7491416.3497, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 406 [0/869 (0%)]\tLoss: 37517204.000000\n",
      "Train Epoch: 406 [160/869 (18%)]\tLoss: 89413776.000000\n",
      "Train Epoch: 406 [320/869 (36%)]\tLoss: 135991024.000000\n",
      "Train Epoch: 406 [480/869 (55%)]\tLoss: 88715248.000000\n",
      "Train Epoch: 406 [640/869 (73%)]\tLoss: 132930488.000000\n",
      "Train Epoch: 406 [800/869 (91%)]\tLoss: 39634976.000000\n",
      "\n",
      "Test set: Avg. loss: 7849454.5160, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 407 [0/869 (0%)]\tLoss: 215083200.000000\n",
      "Train Epoch: 407 [160/869 (18%)]\tLoss: 131684976.000000\n",
      "Train Epoch: 407 [320/869 (36%)]\tLoss: 34803948.000000\n",
      "Train Epoch: 407 [480/869 (55%)]\tLoss: 91053072.000000\n",
      "Train Epoch: 407 [640/869 (73%)]\tLoss: 88324624.000000\n",
      "Train Epoch: 407 [800/869 (91%)]\tLoss: 102179016.000000\n",
      "\n",
      "Test set: Avg. loss: 7564170.1407, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 408 [0/869 (0%)]\tLoss: 42059128.000000\n",
      "Train Epoch: 408 [160/869 (18%)]\tLoss: 187678656.000000\n",
      "Train Epoch: 408 [320/869 (36%)]\tLoss: 39927260.000000\n",
      "Train Epoch: 408 [480/869 (55%)]\tLoss: 93760392.000000\n",
      "Train Epoch: 408 [640/869 (73%)]\tLoss: 174158480.000000\n",
      "Train Epoch: 408 [800/869 (91%)]\tLoss: 95106992.000000\n",
      "\n",
      "Test set: Avg. loss: 7867801.2111, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 409 [0/869 (0%)]\tLoss: 79355520.000000\n",
      "Train Epoch: 409 [160/869 (18%)]\tLoss: 105919536.000000\n",
      "Train Epoch: 409 [320/869 (36%)]\tLoss: 144553312.000000\n",
      "Train Epoch: 409 [480/869 (55%)]\tLoss: 88250832.000000\n",
      "Train Epoch: 409 [640/869 (73%)]\tLoss: 47953464.000000\n",
      "Train Epoch: 409 [800/869 (91%)]\tLoss: 57829808.000000\n",
      "\n",
      "Test set: Avg. loss: 7312098.4307, Accuracy: 0/469 (0%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 410 [0/869 (0%)]\tLoss: 82750096.000000\n",
      "Train Epoch: 410 [160/869 (18%)]\tLoss: 85003368.000000\n",
      "Train Epoch: 410 [320/869 (36%)]\tLoss: 99350976.000000\n",
      "Train Epoch: 410 [480/869 (55%)]\tLoss: 117766992.000000\n",
      "Train Epoch: 410 [640/869 (73%)]\tLoss: 134726016.000000\n",
      "Train Epoch: 410 [800/869 (91%)]\tLoss: 82939928.000000\n",
      "\n",
      "Test set: Avg. loss: 7624217.4584, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 411 [0/869 (0%)]\tLoss: 185545072.000000\n",
      "Train Epoch: 411 [160/869 (18%)]\tLoss: 96694704.000000\n",
      "Train Epoch: 411 [320/869 (36%)]\tLoss: 162082000.000000\n",
      "Train Epoch: 411 [480/869 (55%)]\tLoss: 102392224.000000\n",
      "Train Epoch: 411 [640/869 (73%)]\tLoss: 121169912.000000\n",
      "Train Epoch: 411 [800/869 (91%)]\tLoss: 160454656.000000\n",
      "\n",
      "Test set: Avg. loss: 7834712.9041, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 412 [0/869 (0%)]\tLoss: 32195726.000000\n",
      "Train Epoch: 412 [160/869 (18%)]\tLoss: 66362060.000000\n",
      "Train Epoch: 412 [320/869 (36%)]\tLoss: 109148128.000000\n",
      "Train Epoch: 412 [480/869 (55%)]\tLoss: 92462056.000000\n",
      "Train Epoch: 412 [640/869 (73%)]\tLoss: 56567812.000000\n",
      "Train Epoch: 412 [800/869 (91%)]\tLoss: 92884424.000000\n",
      "\n",
      "Test set: Avg. loss: 7564465.7058, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 413 [0/869 (0%)]\tLoss: 66701512.000000\n",
      "Train Epoch: 413 [160/869 (18%)]\tLoss: 173901680.000000\n",
      "Train Epoch: 413 [320/869 (36%)]\tLoss: 46543164.000000\n",
      "Train Epoch: 413 [480/869 (55%)]\tLoss: 84368696.000000\n",
      "Train Epoch: 413 [640/869 (73%)]\tLoss: 125025024.000000\n",
      "Train Epoch: 413 [800/869 (91%)]\tLoss: 45663536.000000\n",
      "\n",
      "Test set: Avg. loss: 8372405.2367, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 414 [0/869 (0%)]\tLoss: 209515088.000000\n",
      "Train Epoch: 414 [160/869 (18%)]\tLoss: 52309276.000000\n",
      "Train Epoch: 414 [320/869 (36%)]\tLoss: 331032096.000000\n",
      "Train Epoch: 414 [480/869 (55%)]\tLoss: 128349704.000000\n",
      "Train Epoch: 414 [640/869 (73%)]\tLoss: 42360920.000000\n",
      "Train Epoch: 414 [800/869 (91%)]\tLoss: 109503968.000000\n",
      "\n",
      "Test set: Avg. loss: 7567070.5373, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 415 [0/869 (0%)]\tLoss: 94586752.000000\n",
      "Train Epoch: 415 [160/869 (18%)]\tLoss: 130433864.000000\n",
      "Train Epoch: 415 [320/869 (36%)]\tLoss: 171466624.000000\n",
      "Train Epoch: 415 [480/869 (55%)]\tLoss: 162706560.000000\n",
      "Train Epoch: 415 [640/869 (73%)]\tLoss: 87678784.000000\n",
      "Train Epoch: 415 [800/869 (91%)]\tLoss: 159890896.000000\n",
      "\n",
      "Test set: Avg. loss: 7699128.7036, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 416 [0/869 (0%)]\tLoss: 156110976.000000\n",
      "Train Epoch: 416 [160/869 (18%)]\tLoss: 164813952.000000\n",
      "Train Epoch: 416 [320/869 (36%)]\tLoss: 55365620.000000\n",
      "Train Epoch: 416 [480/869 (55%)]\tLoss: 102577320.000000\n",
      "Train Epoch: 416 [640/869 (73%)]\tLoss: 181955328.000000\n",
      "Train Epoch: 416 [800/869 (91%)]\tLoss: 97292896.000000\n",
      "\n",
      "Test set: Avg. loss: 8056007.0192, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 417 [0/869 (0%)]\tLoss: 115105544.000000\n",
      "Train Epoch: 417 [160/869 (18%)]\tLoss: 218235344.000000\n",
      "Train Epoch: 417 [320/869 (36%)]\tLoss: 215656288.000000\n",
      "Train Epoch: 417 [480/869 (55%)]\tLoss: 131645248.000000\n",
      "Train Epoch: 417 [640/869 (73%)]\tLoss: 153084752.000000\n",
      "Train Epoch: 417 [800/869 (91%)]\tLoss: 50305092.000000\n",
      "\n",
      "Test set: Avg. loss: 7538363.8380, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 418 [0/869 (0%)]\tLoss: 114003488.000000\n",
      "Train Epoch: 418 [160/869 (18%)]\tLoss: 121441024.000000\n",
      "Train Epoch: 418 [320/869 (36%)]\tLoss: 252835072.000000\n",
      "Train Epoch: 418 [480/869 (55%)]\tLoss: 171493296.000000\n",
      "Train Epoch: 418 [640/869 (73%)]\tLoss: 80658136.000000\n",
      "Train Epoch: 418 [800/869 (91%)]\tLoss: 115885912.000000\n",
      "\n",
      "Test set: Avg. loss: 7587537.7313, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 419 [0/869 (0%)]\tLoss: 104963616.000000\n",
      "Train Epoch: 419 [160/869 (18%)]\tLoss: 42405868.000000\n",
      "Train Epoch: 419 [320/869 (36%)]\tLoss: 211118544.000000\n",
      "Train Epoch: 419 [480/869 (55%)]\tLoss: 167640016.000000\n",
      "Train Epoch: 419 [640/869 (73%)]\tLoss: 99745600.000000\n",
      "Train Epoch: 419 [800/869 (91%)]\tLoss: 72535320.000000\n",
      "\n",
      "Test set: Avg. loss: 7481905.0576, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 420 [0/869 (0%)]\tLoss: 177455248.000000\n",
      "Train Epoch: 420 [160/869 (18%)]\tLoss: 146749072.000000\n",
      "Train Epoch: 420 [320/869 (36%)]\tLoss: 104749056.000000\n",
      "Train Epoch: 420 [480/869 (55%)]\tLoss: 195436176.000000\n",
      "Train Epoch: 420 [640/869 (73%)]\tLoss: 251733472.000000\n",
      "Train Epoch: 420 [800/869 (91%)]\tLoss: 48013576.000000\n",
      "\n",
      "Test set: Avg. loss: 7760353.9616, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 421 [0/869 (0%)]\tLoss: 355591936.000000\n",
      "Train Epoch: 421 [160/869 (18%)]\tLoss: 93727384.000000\n",
      "Train Epoch: 421 [320/869 (36%)]\tLoss: 47408472.000000\n",
      "Train Epoch: 421 [480/869 (55%)]\tLoss: 83015984.000000\n",
      "Train Epoch: 421 [640/869 (73%)]\tLoss: 124673672.000000\n",
      "Train Epoch: 421 [800/869 (91%)]\tLoss: 59742964.000000\n",
      "\n",
      "Test set: Avg. loss: 7504247.8380, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 422 [0/869 (0%)]\tLoss: 41610048.000000\n",
      "Train Epoch: 422 [160/869 (18%)]\tLoss: 82818480.000000\n",
      "Train Epoch: 422 [320/869 (36%)]\tLoss: 76673592.000000\n",
      "Train Epoch: 422 [480/869 (55%)]\tLoss: 137122400.000000\n",
      "Train Epoch: 422 [640/869 (73%)]\tLoss: 70545264.000000\n",
      "Train Epoch: 422 [800/869 (91%)]\tLoss: 91391496.000000\n",
      "\n",
      "Test set: Avg. loss: 7594642.1748, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 423 [0/869 (0%)]\tLoss: 42514504.000000\n",
      "Train Epoch: 423 [160/869 (18%)]\tLoss: 111125992.000000\n",
      "Train Epoch: 423 [320/869 (36%)]\tLoss: 169550048.000000\n",
      "Train Epoch: 423 [480/869 (55%)]\tLoss: 70109016.000000\n",
      "Train Epoch: 423 [640/869 (73%)]\tLoss: 57148424.000000\n",
      "Train Epoch: 423 [800/869 (91%)]\tLoss: 402192928.000000\n",
      "\n",
      "Test set: Avg. loss: 7890563.8891, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 424 [0/869 (0%)]\tLoss: 201456032.000000\n",
      "Train Epoch: 424 [160/869 (18%)]\tLoss: 83056600.000000\n",
      "Train Epoch: 424 [320/869 (36%)]\tLoss: 94203816.000000\n",
      "Train Epoch: 424 [480/869 (55%)]\tLoss: 170225312.000000\n",
      "Train Epoch: 424 [640/869 (73%)]\tLoss: 28473798.000000\n",
      "Train Epoch: 424 [800/869 (91%)]\tLoss: 55228640.000000\n",
      "\n",
      "Test set: Avg. loss: 7541806.6013, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 425 [0/869 (0%)]\tLoss: 155128896.000000\n",
      "Train Epoch: 425 [160/869 (18%)]\tLoss: 48496640.000000\n",
      "Train Epoch: 425 [320/869 (36%)]\tLoss: 286974816.000000\n",
      "Train Epoch: 425 [480/869 (55%)]\tLoss: 98551536.000000\n",
      "Train Epoch: 425 [640/869 (73%)]\tLoss: 202525536.000000\n",
      "Train Epoch: 425 [800/869 (91%)]\tLoss: 107073048.000000\n",
      "\n",
      "Test set: Avg. loss: 7830129.3518, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 426 [0/869 (0%)]\tLoss: 280021760.000000\n",
      "Train Epoch: 426 [160/869 (18%)]\tLoss: 128959824.000000\n",
      "Train Epoch: 426 [320/869 (36%)]\tLoss: 82584464.000000\n",
      "Train Epoch: 426 [480/869 (55%)]\tLoss: 84073552.000000\n",
      "Train Epoch: 426 [640/869 (73%)]\tLoss: 87474656.000000\n",
      "Train Epoch: 426 [800/869 (91%)]\tLoss: 100548032.000000\n",
      "\n",
      "Test set: Avg. loss: 7476993.2964, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 427 [0/869 (0%)]\tLoss: 230582560.000000\n",
      "Train Epoch: 427 [160/869 (18%)]\tLoss: 128402128.000000\n",
      "Train Epoch: 427 [320/869 (36%)]\tLoss: 93102336.000000\n",
      "Train Epoch: 427 [480/869 (55%)]\tLoss: 191331776.000000\n",
      "Train Epoch: 427 [640/869 (73%)]\tLoss: 49320972.000000\n",
      "Train Epoch: 427 [800/869 (91%)]\tLoss: 135066496.000000\n",
      "\n",
      "Test set: Avg. loss: 7824104.1663, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 428 [0/869 (0%)]\tLoss: 37672096.000000\n",
      "Train Epoch: 428 [160/869 (18%)]\tLoss: 69783712.000000\n",
      "Train Epoch: 428 [320/869 (36%)]\tLoss: 102387440.000000\n",
      "Train Epoch: 428 [480/869 (55%)]\tLoss: 336507264.000000\n",
      "Train Epoch: 428 [640/869 (73%)]\tLoss: 354841632.000000\n",
      "Train Epoch: 428 [800/869 (91%)]\tLoss: 121188784.000000\n",
      "\n",
      "Test set: Avg. loss: 7337486.9168, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 429 [0/869 (0%)]\tLoss: 96895176.000000\n",
      "Train Epoch: 429 [160/869 (18%)]\tLoss: 50043368.000000\n",
      "Train Epoch: 429 [320/869 (36%)]\tLoss: 176633600.000000\n",
      "Train Epoch: 429 [480/869 (55%)]\tLoss: 66730272.000000\n",
      "Train Epoch: 429 [640/869 (73%)]\tLoss: 185688944.000000\n",
      "Train Epoch: 429 [800/869 (91%)]\tLoss: 100786464.000000\n",
      "\n",
      "Test set: Avg. loss: 7536719.5565, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 430 [0/869 (0%)]\tLoss: 188765104.000000\n",
      "Train Epoch: 430 [160/869 (18%)]\tLoss: 60307392.000000\n",
      "Train Epoch: 430 [320/869 (36%)]\tLoss: 218496608.000000\n",
      "Train Epoch: 430 [480/869 (55%)]\tLoss: 181193936.000000\n",
      "Train Epoch: 430 [640/869 (73%)]\tLoss: 179257120.000000\n",
      "Train Epoch: 430 [800/869 (91%)]\tLoss: 130126680.000000\n",
      "\n",
      "Test set: Avg. loss: 7542547.3774, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 431 [0/869 (0%)]\tLoss: 85493088.000000\n",
      "Train Epoch: 431 [160/869 (18%)]\tLoss: 133892264.000000\n",
      "Train Epoch: 431 [320/869 (36%)]\tLoss: 134897696.000000\n",
      "Train Epoch: 431 [480/869 (55%)]\tLoss: 273904576.000000\n",
      "Train Epoch: 431 [640/869 (73%)]\tLoss: 192420176.000000\n",
      "Train Epoch: 431 [800/869 (91%)]\tLoss: 39417120.000000\n",
      "\n",
      "Test set: Avg. loss: 7609675.2580, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 432 [0/869 (0%)]\tLoss: 93689768.000000\n",
      "Train Epoch: 432 [160/869 (18%)]\tLoss: 86431856.000000\n",
      "Train Epoch: 432 [320/869 (36%)]\tLoss: 36246704.000000\n",
      "Train Epoch: 432 [480/869 (55%)]\tLoss: 123846672.000000\n",
      "Train Epoch: 432 [640/869 (73%)]\tLoss: 87200400.000000\n",
      "Train Epoch: 432 [800/869 (91%)]\tLoss: 70895088.000000\n",
      "\n",
      "Test set: Avg. loss: 7459040.0426, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 433 [0/869 (0%)]\tLoss: 146989264.000000\n",
      "Train Epoch: 433 [160/869 (18%)]\tLoss: 179895200.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 433 [320/869 (36%)]\tLoss: 29250192.000000\n",
      "Train Epoch: 433 [480/869 (55%)]\tLoss: 207106160.000000\n",
      "Train Epoch: 433 [640/869 (73%)]\tLoss: 219741008.000000\n",
      "Train Epoch: 433 [800/869 (91%)]\tLoss: 63200668.000000\n",
      "\n",
      "Test set: Avg. loss: 7460106.9765, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 434 [0/869 (0%)]\tLoss: 146579264.000000\n",
      "Train Epoch: 434 [160/869 (18%)]\tLoss: 139846208.000000\n",
      "Train Epoch: 434 [320/869 (36%)]\tLoss: 170792512.000000\n",
      "Train Epoch: 434 [480/869 (55%)]\tLoss: 120959976.000000\n",
      "Train Epoch: 434 [640/869 (73%)]\tLoss: 138434208.000000\n",
      "Train Epoch: 434 [800/869 (91%)]\tLoss: 233303008.000000\n",
      "\n",
      "Test set: Avg. loss: 7515412.0426, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 435 [0/869 (0%)]\tLoss: 107483360.000000\n",
      "Train Epoch: 435 [160/869 (18%)]\tLoss: 95582944.000000\n",
      "Train Epoch: 435 [320/869 (36%)]\tLoss: 51820672.000000\n",
      "Train Epoch: 435 [480/869 (55%)]\tLoss: 137222576.000000\n",
      "Train Epoch: 435 [640/869 (73%)]\tLoss: 16257186.000000\n",
      "Train Epoch: 435 [800/869 (91%)]\tLoss: 98903576.000000\n",
      "\n",
      "Test set: Avg. loss: 7823690.4051, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 436 [0/869 (0%)]\tLoss: 91081688.000000\n",
      "Train Epoch: 436 [160/869 (18%)]\tLoss: 171526992.000000\n",
      "Train Epoch: 436 [320/869 (36%)]\tLoss: 83964504.000000\n",
      "Train Epoch: 436 [480/869 (55%)]\tLoss: 82090904.000000\n",
      "Train Epoch: 436 [640/869 (73%)]\tLoss: 32902536.000000\n",
      "Train Epoch: 436 [800/869 (91%)]\tLoss: 102537928.000000\n",
      "\n",
      "Test set: Avg. loss: 7482778.2687, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 437 [0/869 (0%)]\tLoss: 32354664.000000\n",
      "Train Epoch: 437 [160/869 (18%)]\tLoss: 83267096.000000\n",
      "Train Epoch: 437 [320/869 (36%)]\tLoss: 143967360.000000\n",
      "Train Epoch: 437 [480/869 (55%)]\tLoss: 143094928.000000\n",
      "Train Epoch: 437 [640/869 (73%)]\tLoss: 44785856.000000\n",
      "Train Epoch: 437 [800/869 (91%)]\tLoss: 119153336.000000\n",
      "\n",
      "Test set: Avg. loss: 7578363.2665, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 438 [0/869 (0%)]\tLoss: 73077976.000000\n",
      "Train Epoch: 438 [160/869 (18%)]\tLoss: 69347376.000000\n",
      "Train Epoch: 438 [320/869 (36%)]\tLoss: 55332164.000000\n",
      "Train Epoch: 438 [480/869 (55%)]\tLoss: 158164080.000000\n",
      "Train Epoch: 438 [640/869 (73%)]\tLoss: 139854240.000000\n",
      "Train Epoch: 438 [800/869 (91%)]\tLoss: 60358272.000000\n",
      "\n",
      "Test set: Avg. loss: 7770087.5160, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 439 [0/869 (0%)]\tLoss: 119082008.000000\n",
      "Train Epoch: 439 [160/869 (18%)]\tLoss: 118215248.000000\n",
      "Train Epoch: 439 [320/869 (36%)]\tLoss: 333533664.000000\n",
      "Train Epoch: 439 [480/869 (55%)]\tLoss: 76779640.000000\n",
      "Train Epoch: 439 [640/869 (73%)]\tLoss: 46754552.000000\n",
      "Train Epoch: 439 [800/869 (91%)]\tLoss: 130238704.000000\n",
      "\n",
      "Test set: Avg. loss: 7727642.1834, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 440 [0/869 (0%)]\tLoss: 55322204.000000\n",
      "Train Epoch: 440 [160/869 (18%)]\tLoss: 97562600.000000\n",
      "Train Epoch: 440 [320/869 (36%)]\tLoss: 65889704.000000\n",
      "Train Epoch: 440 [480/869 (55%)]\tLoss: 94252016.000000\n",
      "Train Epoch: 440 [640/869 (73%)]\tLoss: 273712064.000000\n",
      "Train Epoch: 440 [800/869 (91%)]\tLoss: 87976016.000000\n",
      "\n",
      "Test set: Avg. loss: 7301413.2367, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 441 [0/869 (0%)]\tLoss: 231397440.000000\n",
      "Train Epoch: 441 [160/869 (18%)]\tLoss: 218209536.000000\n",
      "Train Epoch: 441 [320/869 (36%)]\tLoss: 129603816.000000\n",
      "Train Epoch: 441 [480/869 (55%)]\tLoss: 250166864.000000\n",
      "Train Epoch: 441 [640/869 (73%)]\tLoss: 131028456.000000\n",
      "Train Epoch: 441 [800/869 (91%)]\tLoss: 156893728.000000\n",
      "\n",
      "Test set: Avg. loss: 7351455.7441, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 442 [0/869 (0%)]\tLoss: 92006952.000000\n",
      "Train Epoch: 442 [160/869 (18%)]\tLoss: 144540608.000000\n",
      "Train Epoch: 442 [320/869 (36%)]\tLoss: 95892584.000000\n",
      "Train Epoch: 442 [480/869 (55%)]\tLoss: 82928288.000000\n",
      "Train Epoch: 442 [640/869 (73%)]\tLoss: 125816640.000000\n",
      "Train Epoch: 442 [800/869 (91%)]\tLoss: 71758728.000000\n",
      "\n",
      "Test set: Avg. loss: 8072625.9019, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 443 [0/869 (0%)]\tLoss: 82339248.000000\n",
      "Train Epoch: 443 [160/869 (18%)]\tLoss: 78688960.000000\n",
      "Train Epoch: 443 [320/869 (36%)]\tLoss: 150583632.000000\n",
      "Train Epoch: 443 [480/869 (55%)]\tLoss: 125722856.000000\n",
      "Train Epoch: 443 [640/869 (73%)]\tLoss: 57993312.000000\n",
      "Train Epoch: 443 [800/869 (91%)]\tLoss: 192693536.000000\n",
      "\n",
      "Test set: Avg. loss: 7724996.5970, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 444 [0/869 (0%)]\tLoss: 124052992.000000\n",
      "Train Epoch: 444 [160/869 (18%)]\tLoss: 94864224.000000\n",
      "Train Epoch: 444 [320/869 (36%)]\tLoss: 275528224.000000\n",
      "Train Epoch: 444 [480/869 (55%)]\tLoss: 134468784.000000\n",
      "Train Epoch: 444 [640/869 (73%)]\tLoss: 375808768.000000\n",
      "Train Epoch: 444 [800/869 (91%)]\tLoss: 72359840.000000\n",
      "\n",
      "Test set: Avg. loss: 7854322.9510, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 445 [0/869 (0%)]\tLoss: 379418432.000000\n",
      "Train Epoch: 445 [160/869 (18%)]\tLoss: 140684736.000000\n",
      "Train Epoch: 445 [320/869 (36%)]\tLoss: 120658664.000000\n",
      "Train Epoch: 445 [480/869 (55%)]\tLoss: 56782560.000000\n",
      "Train Epoch: 445 [640/869 (73%)]\tLoss: 247031808.000000\n",
      "Train Epoch: 445 [800/869 (91%)]\tLoss: 88338856.000000\n",
      "\n",
      "Test set: Avg. loss: 7773275.2708, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 446 [0/869 (0%)]\tLoss: 136059056.000000\n",
      "Train Epoch: 446 [160/869 (18%)]\tLoss: 235007504.000000\n",
      "Train Epoch: 446 [320/869 (36%)]\tLoss: 123080464.000000\n",
      "Train Epoch: 446 [480/869 (55%)]\tLoss: 107879936.000000\n",
      "Train Epoch: 446 [640/869 (73%)]\tLoss: 133543136.000000\n",
      "Train Epoch: 446 [800/869 (91%)]\tLoss: 61142300.000000\n",
      "\n",
      "Test set: Avg. loss: 7509788.1876, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 447 [0/869 (0%)]\tLoss: 163591120.000000\n",
      "Train Epoch: 447 [160/869 (18%)]\tLoss: 101270928.000000\n",
      "Train Epoch: 447 [320/869 (36%)]\tLoss: 43689280.000000\n",
      "Train Epoch: 447 [480/869 (55%)]\tLoss: 247501648.000000\n",
      "Train Epoch: 447 [640/869 (73%)]\tLoss: 28996276.000000\n",
      "Train Epoch: 447 [800/869 (91%)]\tLoss: 126989072.000000\n",
      "\n",
      "Test set: Avg. loss: 7598968.5800, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 448 [0/869 (0%)]\tLoss: 223810176.000000\n",
      "Train Epoch: 448 [160/869 (18%)]\tLoss: 369305344.000000\n",
      "Train Epoch: 448 [320/869 (36%)]\tLoss: 93710368.000000\n",
      "Train Epoch: 448 [480/869 (55%)]\tLoss: 133250184.000000\n",
      "Train Epoch: 448 [640/869 (73%)]\tLoss: 82332880.000000\n",
      "Train Epoch: 448 [800/869 (91%)]\tLoss: 70025312.000000\n",
      "\n",
      "Test set: Avg. loss: 7817790.6610, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 449 [0/869 (0%)]\tLoss: 95253120.000000\n",
      "Train Epoch: 449 [160/869 (18%)]\tLoss: 170448928.000000\n",
      "Train Epoch: 449 [320/869 (36%)]\tLoss: 82645848.000000\n",
      "Train Epoch: 449 [480/869 (55%)]\tLoss: 62037492.000000\n",
      "Train Epoch: 449 [640/869 (73%)]\tLoss: 99972496.000000\n",
      "Train Epoch: 449 [800/869 (91%)]\tLoss: 85303088.000000\n",
      "\n",
      "Test set: Avg. loss: 7412167.6930, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 450 [0/869 (0%)]\tLoss: 143466416.000000\n",
      "Train Epoch: 450 [160/869 (18%)]\tLoss: 134172776.000000\n",
      "Train Epoch: 450 [320/869 (36%)]\tLoss: 149137680.000000\n",
      "Train Epoch: 450 [480/869 (55%)]\tLoss: 131947632.000000\n",
      "Train Epoch: 450 [640/869 (73%)]\tLoss: 230831408.000000\n",
      "Train Epoch: 450 [800/869 (91%)]\tLoss: 36521848.000000\n",
      "\n",
      "Test set: Avg. loss: 7407295.8763, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 451 [0/869 (0%)]\tLoss: 116375304.000000\n",
      "Train Epoch: 451 [160/869 (18%)]\tLoss: 83819808.000000\n",
      "Train Epoch: 451 [320/869 (36%)]\tLoss: 86130936.000000\n",
      "Train Epoch: 451 [480/869 (55%)]\tLoss: 217281184.000000\n",
      "Train Epoch: 451 [640/869 (73%)]\tLoss: 197704064.000000\n",
      "Train Epoch: 451 [800/869 (91%)]\tLoss: 49979088.000000\n",
      "\n",
      "Test set: Avg. loss: 7732415.3603, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 452 [0/869 (0%)]\tLoss: 119178472.000000\n",
      "Train Epoch: 452 [160/869 (18%)]\tLoss: 143995200.000000\n",
      "Train Epoch: 452 [320/869 (36%)]\tLoss: 179598992.000000\n",
      "Train Epoch: 452 [480/869 (55%)]\tLoss: 125730832.000000\n",
      "Train Epoch: 452 [640/869 (73%)]\tLoss: 151702944.000000\n",
      "Train Epoch: 452 [800/869 (91%)]\tLoss: 55424624.000000\n",
      "\n",
      "Test set: Avg. loss: 8415407.0021, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 453 [0/869 (0%)]\tLoss: 96976656.000000\n",
      "Train Epoch: 453 [160/869 (18%)]\tLoss: 137069376.000000\n",
      "Train Epoch: 453 [320/869 (36%)]\tLoss: 113847904.000000\n",
      "Train Epoch: 453 [480/869 (55%)]\tLoss: 172202832.000000\n",
      "Train Epoch: 453 [640/869 (73%)]\tLoss: 58466308.000000\n",
      "Train Epoch: 453 [800/869 (91%)]\tLoss: 40209936.000000\n",
      "\n",
      "Test set: Avg. loss: 7417295.2409, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 454 [0/869 (0%)]\tLoss: 170927200.000000\n",
      "Train Epoch: 454 [160/869 (18%)]\tLoss: 83397288.000000\n",
      "Train Epoch: 454 [320/869 (36%)]\tLoss: 274713984.000000\n",
      "Train Epoch: 454 [480/869 (55%)]\tLoss: 72670136.000000\n",
      "Train Epoch: 454 [640/869 (73%)]\tLoss: 196579216.000000\n",
      "Train Epoch: 454 [800/869 (91%)]\tLoss: 133887120.000000\n",
      "\n",
      "Test set: Avg. loss: 7458580.6183, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 455 [0/869 (0%)]\tLoss: 45648740.000000\n",
      "Train Epoch: 455 [160/869 (18%)]\tLoss: 154675040.000000\n",
      "Train Epoch: 455 [320/869 (36%)]\tLoss: 217682080.000000\n",
      "Train Epoch: 455 [480/869 (55%)]\tLoss: 80253496.000000\n",
      "Train Epoch: 455 [640/869 (73%)]\tLoss: 74918760.000000\n",
      "Train Epoch: 455 [800/869 (91%)]\tLoss: 89505640.000000\n",
      "\n",
      "Test set: Avg. loss: 7639658.2090, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 456 [0/869 (0%)]\tLoss: 64876948.000000\n",
      "Train Epoch: 456 [160/869 (18%)]\tLoss: 81732360.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 456 [320/869 (36%)]\tLoss: 134225280.000000\n",
      "Train Epoch: 456 [480/869 (55%)]\tLoss: 163126912.000000\n",
      "Train Epoch: 456 [640/869 (73%)]\tLoss: 111207000.000000\n",
      "Train Epoch: 456 [800/869 (91%)]\tLoss: 144418960.000000\n",
      "\n",
      "Test set: Avg. loss: 7497294.2857, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 457 [0/869 (0%)]\tLoss: 194350400.000000\n",
      "Train Epoch: 457 [160/869 (18%)]\tLoss: 59598860.000000\n",
      "Train Epoch: 457 [320/869 (36%)]\tLoss: 130207496.000000\n",
      "Train Epoch: 457 [480/869 (55%)]\tLoss: 134897888.000000\n",
      "Train Epoch: 457 [640/869 (73%)]\tLoss: 191856048.000000\n",
      "Train Epoch: 457 [800/869 (91%)]\tLoss: 76777120.000000\n",
      "\n",
      "Test set: Avg. loss: 7702600.3582, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 458 [0/869 (0%)]\tLoss: 143326784.000000\n",
      "Train Epoch: 458 [160/869 (18%)]\tLoss: 136555808.000000\n",
      "Train Epoch: 458 [320/869 (36%)]\tLoss: 174476976.000000\n",
      "Train Epoch: 458 [480/869 (55%)]\tLoss: 111267760.000000\n",
      "Train Epoch: 458 [640/869 (73%)]\tLoss: 329045120.000000\n",
      "Train Epoch: 458 [800/869 (91%)]\tLoss: 162957152.000000\n",
      "\n",
      "Test set: Avg. loss: 7325262.7719, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 459 [0/869 (0%)]\tLoss: 96765888.000000\n",
      "Train Epoch: 459 [160/869 (18%)]\tLoss: 153580256.000000\n",
      "Train Epoch: 459 [320/869 (36%)]\tLoss: 73696296.000000\n",
      "Train Epoch: 459 [480/869 (55%)]\tLoss: 162611072.000000\n",
      "Train Epoch: 459 [640/869 (73%)]\tLoss: 200614944.000000\n",
      "Train Epoch: 459 [800/869 (91%)]\tLoss: 165734384.000000\n",
      "\n",
      "Test set: Avg. loss: 7665959.3262, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 460 [0/869 (0%)]\tLoss: 93744936.000000\n",
      "Train Epoch: 460 [160/869 (18%)]\tLoss: 129672344.000000\n",
      "Train Epoch: 460 [320/869 (36%)]\tLoss: 162158624.000000\n",
      "Train Epoch: 460 [480/869 (55%)]\tLoss: 43133880.000000\n",
      "Train Epoch: 460 [640/869 (73%)]\tLoss: 108771616.000000\n",
      "Train Epoch: 460 [800/869 (91%)]\tLoss: 83662112.000000\n",
      "\n",
      "Test set: Avg. loss: 7519249.1770, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 461 [0/869 (0%)]\tLoss: 145489280.000000\n",
      "Train Epoch: 461 [160/869 (18%)]\tLoss: 150293056.000000\n",
      "Train Epoch: 461 [320/869 (36%)]\tLoss: 87498464.000000\n",
      "Train Epoch: 461 [480/869 (55%)]\tLoss: 64043272.000000\n",
      "Train Epoch: 461 [640/869 (73%)]\tLoss: 85529456.000000\n",
      "Train Epoch: 461 [800/869 (91%)]\tLoss: 78844728.000000\n",
      "\n",
      "Test set: Avg. loss: 8338106.5928, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 462 [0/869 (0%)]\tLoss: 198028064.000000\n",
      "Train Epoch: 462 [160/869 (18%)]\tLoss: 119686432.000000\n",
      "Train Epoch: 462 [320/869 (36%)]\tLoss: 184644160.000000\n",
      "Train Epoch: 462 [480/869 (55%)]\tLoss: 243154176.000000\n",
      "Train Epoch: 462 [640/869 (73%)]\tLoss: 95856704.000000\n",
      "Train Epoch: 462 [800/869 (91%)]\tLoss: 75872080.000000\n",
      "\n",
      "Test set: Avg. loss: 7390424.2047, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 463 [0/869 (0%)]\tLoss: 63793516.000000\n",
      "Train Epoch: 463 [160/869 (18%)]\tLoss: 32954268.000000\n",
      "Train Epoch: 463 [320/869 (36%)]\tLoss: 194683632.000000\n",
      "Train Epoch: 463 [480/869 (55%)]\tLoss: 70790144.000000\n",
      "Train Epoch: 463 [640/869 (73%)]\tLoss: 78424016.000000\n",
      "Train Epoch: 463 [800/869 (91%)]\tLoss: 126939392.000000\n",
      "\n",
      "Test set: Avg. loss: 7572909.5608, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 464 [0/869 (0%)]\tLoss: 39643172.000000\n",
      "Train Epoch: 464 [160/869 (18%)]\tLoss: 44188292.000000\n",
      "Train Epoch: 464 [320/869 (36%)]\tLoss: 108112232.000000\n",
      "Train Epoch: 464 [480/869 (55%)]\tLoss: 116168448.000000\n",
      "Train Epoch: 464 [640/869 (73%)]\tLoss: 117020112.000000\n",
      "Train Epoch: 464 [800/869 (91%)]\tLoss: 178567168.000000\n",
      "\n",
      "Test set: Avg. loss: 7501314.4136, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 465 [0/869 (0%)]\tLoss: 85123840.000000\n",
      "Train Epoch: 465 [160/869 (18%)]\tLoss: 129840432.000000\n",
      "Train Epoch: 465 [320/869 (36%)]\tLoss: 125913720.000000\n",
      "Train Epoch: 465 [480/869 (55%)]\tLoss: 234781088.000000\n",
      "Train Epoch: 465 [640/869 (73%)]\tLoss: 79003952.000000\n",
      "Train Epoch: 465 [800/869 (91%)]\tLoss: 117806592.000000\n",
      "\n",
      "Test set: Avg. loss: 7968097.8252, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 466 [0/869 (0%)]\tLoss: 190353040.000000\n",
      "Train Epoch: 466 [160/869 (18%)]\tLoss: 132469552.000000\n",
      "Train Epoch: 466 [320/869 (36%)]\tLoss: 181484416.000000\n",
      "Train Epoch: 466 [480/869 (55%)]\tLoss: 158865568.000000\n",
      "Train Epoch: 466 [640/869 (73%)]\tLoss: 184241360.000000\n",
      "Train Epoch: 466 [800/869 (91%)]\tLoss: 137092048.000000\n",
      "\n",
      "Test set: Avg. loss: 7303316.9979, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 467 [0/869 (0%)]\tLoss: 173028784.000000\n",
      "Train Epoch: 467 [160/869 (18%)]\tLoss: 130853208.000000\n",
      "Train Epoch: 467 [320/869 (36%)]\tLoss: 85120336.000000\n",
      "Train Epoch: 467 [480/869 (55%)]\tLoss: 53249328.000000\n",
      "Train Epoch: 467 [640/869 (73%)]\tLoss: 90645808.000000\n",
      "Train Epoch: 467 [800/869 (91%)]\tLoss: 169877088.000000\n",
      "\n",
      "Test set: Avg. loss: 8005051.1727, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 468 [0/869 (0%)]\tLoss: 166179792.000000\n",
      "Train Epoch: 468 [160/869 (18%)]\tLoss: 126203712.000000\n",
      "Train Epoch: 468 [320/869 (36%)]\tLoss: 141583264.000000\n",
      "Train Epoch: 468 [480/869 (55%)]\tLoss: 153379840.000000\n",
      "Train Epoch: 468 [640/869 (73%)]\tLoss: 96751808.000000\n",
      "Train Epoch: 468 [800/869 (91%)]\tLoss: 156324032.000000\n",
      "\n",
      "Test set: Avg. loss: 7585531.5991, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 469 [0/869 (0%)]\tLoss: 95872984.000000\n",
      "Train Epoch: 469 [160/869 (18%)]\tLoss: 102209024.000000\n",
      "Train Epoch: 469 [320/869 (36%)]\tLoss: 116358720.000000\n",
      "Train Epoch: 469 [480/869 (55%)]\tLoss: 118835792.000000\n",
      "Train Epoch: 469 [640/869 (73%)]\tLoss: 42977900.000000\n",
      "Train Epoch: 469 [800/869 (91%)]\tLoss: 125092136.000000\n",
      "\n",
      "Test set: Avg. loss: 7452830.3198, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 470 [0/869 (0%)]\tLoss: 40547172.000000\n",
      "Train Epoch: 470 [160/869 (18%)]\tLoss: 64723880.000000\n",
      "Train Epoch: 470 [320/869 (36%)]\tLoss: 178790480.000000\n",
      "Train Epoch: 470 [480/869 (55%)]\tLoss: 147382464.000000\n",
      "Train Epoch: 470 [640/869 (73%)]\tLoss: 186113568.000000\n",
      "Train Epoch: 470 [800/869 (91%)]\tLoss: 217780032.000000\n",
      "\n",
      "Test set: Avg. loss: 7424245.0661, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 471 [0/869 (0%)]\tLoss: 167866240.000000\n",
      "Train Epoch: 471 [160/869 (18%)]\tLoss: 224067056.000000\n",
      "Train Epoch: 471 [320/869 (36%)]\tLoss: 91202704.000000\n",
      "Train Epoch: 471 [480/869 (55%)]\tLoss: 136752848.000000\n",
      "Train Epoch: 471 [640/869 (73%)]\tLoss: 69723392.000000\n",
      "Train Epoch: 471 [800/869 (91%)]\tLoss: 228359184.000000\n",
      "\n",
      "Test set: Avg. loss: 7515060.2559, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 472 [0/869 (0%)]\tLoss: 104653336.000000\n",
      "Train Epoch: 472 [160/869 (18%)]\tLoss: 103030432.000000\n",
      "Train Epoch: 472 [320/869 (36%)]\tLoss: 33528674.000000\n",
      "Train Epoch: 472 [480/869 (55%)]\tLoss: 81549000.000000\n",
      "Train Epoch: 472 [640/869 (73%)]\tLoss: 45983468.000000\n",
      "Train Epoch: 472 [800/869 (91%)]\tLoss: 164177056.000000\n",
      "\n",
      "Test set: Avg. loss: 7768201.3646, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 473 [0/869 (0%)]\tLoss: 73062672.000000\n",
      "Train Epoch: 473 [160/869 (18%)]\tLoss: 153476080.000000\n",
      "Train Epoch: 473 [320/869 (36%)]\tLoss: 76134464.000000\n",
      "Train Epoch: 473 [480/869 (55%)]\tLoss: 87655032.000000\n",
      "Train Epoch: 473 [640/869 (73%)]\tLoss: 67230744.000000\n",
      "Train Epoch: 473 [800/869 (91%)]\tLoss: 50915120.000000\n",
      "\n",
      "Test set: Avg. loss: 7323806.5842, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 474 [0/869 (0%)]\tLoss: 101552256.000000\n",
      "Train Epoch: 474 [160/869 (18%)]\tLoss: 202593712.000000\n",
      "Train Epoch: 474 [320/869 (36%)]\tLoss: 94117728.000000\n",
      "Train Epoch: 474 [480/869 (55%)]\tLoss: 102669592.000000\n",
      "Train Epoch: 474 [640/869 (73%)]\tLoss: 82377976.000000\n",
      "Train Epoch: 474 [800/869 (91%)]\tLoss: 80213640.000000\n",
      "\n",
      "Test set: Avg. loss: 7540983.6077, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 475 [0/869 (0%)]\tLoss: 85746016.000000\n",
      "Train Epoch: 475 [160/869 (18%)]\tLoss: 195472416.000000\n",
      "Train Epoch: 475 [320/869 (36%)]\tLoss: 78015368.000000\n",
      "Train Epoch: 475 [480/869 (55%)]\tLoss: 298986048.000000\n",
      "Train Epoch: 475 [640/869 (73%)]\tLoss: 99860128.000000\n",
      "Train Epoch: 475 [800/869 (91%)]\tLoss: 243850512.000000\n",
      "\n",
      "Test set: Avg. loss: 7414913.3987, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 476 [0/869 (0%)]\tLoss: 314334944.000000\n",
      "Train Epoch: 476 [160/869 (18%)]\tLoss: 182884224.000000\n",
      "Train Epoch: 476 [320/869 (36%)]\tLoss: 142553392.000000\n",
      "Train Epoch: 476 [480/869 (55%)]\tLoss: 80737592.000000\n",
      "Train Epoch: 476 [640/869 (73%)]\tLoss: 197175568.000000\n",
      "Train Epoch: 476 [800/869 (91%)]\tLoss: 178964512.000000\n",
      "\n",
      "Test set: Avg. loss: 7574607.0959, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 477 [0/869 (0%)]\tLoss: 70570648.000000\n",
      "Train Epoch: 477 [160/869 (18%)]\tLoss: 162458224.000000\n",
      "Train Epoch: 477 [320/869 (36%)]\tLoss: 82213112.000000\n",
      "Train Epoch: 477 [480/869 (55%)]\tLoss: 274662624.000000\n",
      "Train Epoch: 477 [640/869 (73%)]\tLoss: 103510896.000000\n",
      "Train Epoch: 477 [800/869 (91%)]\tLoss: 145106416.000000\n",
      "\n",
      "Test set: Avg. loss: 7431097.1855, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 478 [0/869 (0%)]\tLoss: 212794784.000000\n",
      "Train Epoch: 478 [160/869 (18%)]\tLoss: 188850560.000000\n",
      "Train Epoch: 478 [320/869 (36%)]\tLoss: 92462880.000000\n",
      "Train Epoch: 478 [480/869 (55%)]\tLoss: 81865392.000000\n",
      "Train Epoch: 478 [640/869 (73%)]\tLoss: 95290552.000000\n",
      "Train Epoch: 478 [800/869 (91%)]\tLoss: 111574704.000000\n",
      "\n",
      "Test set: Avg. loss: 7486478.6354, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 479 [0/869 (0%)]\tLoss: 132815568.000000\n",
      "Train Epoch: 479 [160/869 (18%)]\tLoss: 160166976.000000\n",
      "Train Epoch: 479 [320/869 (36%)]\tLoss: 88639344.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 479 [480/869 (55%)]\tLoss: 42236340.000000\n",
      "Train Epoch: 479 [640/869 (73%)]\tLoss: 124997376.000000\n",
      "Train Epoch: 479 [800/869 (91%)]\tLoss: 169860096.000000\n",
      "\n",
      "Test set: Avg. loss: 7425056.2985, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 480 [0/869 (0%)]\tLoss: 55683300.000000\n",
      "Train Epoch: 480 [160/869 (18%)]\tLoss: 166563568.000000\n",
      "Train Epoch: 480 [320/869 (36%)]\tLoss: 117642688.000000\n",
      "Train Epoch: 480 [480/869 (55%)]\tLoss: 146278912.000000\n",
      "Train Epoch: 480 [640/869 (73%)]\tLoss: 45705288.000000\n",
      "Train Epoch: 480 [800/869 (91%)]\tLoss: 99701680.000000\n",
      "\n",
      "Test set: Avg. loss: 7964528.6610, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 481 [0/869 (0%)]\tLoss: 131348832.000000\n",
      "Train Epoch: 481 [160/869 (18%)]\tLoss: 182780176.000000\n",
      "Train Epoch: 481 [320/869 (36%)]\tLoss: 278779296.000000\n",
      "Train Epoch: 481 [480/869 (55%)]\tLoss: 75767928.000000\n",
      "Train Epoch: 481 [640/869 (73%)]\tLoss: 40438416.000000\n",
      "Train Epoch: 481 [800/869 (91%)]\tLoss: 91021568.000000\n",
      "\n",
      "Test set: Avg. loss: 7373199.6588, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 482 [0/869 (0%)]\tLoss: 120514720.000000\n",
      "Train Epoch: 482 [160/869 (18%)]\tLoss: 92992792.000000\n",
      "Train Epoch: 482 [320/869 (36%)]\tLoss: 167442496.000000\n",
      "Train Epoch: 482 [480/869 (55%)]\tLoss: 82776304.000000\n",
      "Train Epoch: 482 [640/869 (73%)]\tLoss: 139661984.000000\n",
      "Train Epoch: 482 [800/869 (91%)]\tLoss: 103610536.000000\n",
      "\n",
      "Test set: Avg. loss: 7467575.9829, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 483 [0/869 (0%)]\tLoss: 164343040.000000\n",
      "Train Epoch: 483 [160/869 (18%)]\tLoss: 62250896.000000\n",
      "Train Epoch: 483 [320/869 (36%)]\tLoss: 137606144.000000\n",
      "Train Epoch: 483 [480/869 (55%)]\tLoss: 138245648.000000\n",
      "Train Epoch: 483 [640/869 (73%)]\tLoss: 156931840.000000\n",
      "Train Epoch: 483 [800/869 (91%)]\tLoss: 80830768.000000\n",
      "\n",
      "Test set: Avg. loss: 7948255.7612, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 484 [0/869 (0%)]\tLoss: 165831360.000000\n",
      "Train Epoch: 484 [160/869 (18%)]\tLoss: 347598400.000000\n",
      "Train Epoch: 484 [320/869 (36%)]\tLoss: 77939552.000000\n",
      "Train Epoch: 484 [480/869 (55%)]\tLoss: 78644256.000000\n",
      "Train Epoch: 484 [640/869 (73%)]\tLoss: 177364512.000000\n",
      "Train Epoch: 484 [800/869 (91%)]\tLoss: 129908776.000000\n",
      "\n",
      "Test set: Avg. loss: 7766728.6397, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 485 [0/869 (0%)]\tLoss: 189881680.000000\n",
      "Train Epoch: 485 [160/869 (18%)]\tLoss: 67817192.000000\n",
      "Train Epoch: 485 [320/869 (36%)]\tLoss: 50365384.000000\n",
      "Train Epoch: 485 [480/869 (55%)]\tLoss: 104710512.000000\n",
      "Train Epoch: 485 [640/869 (73%)]\tLoss: 139471840.000000\n",
      "Train Epoch: 485 [800/869 (91%)]\tLoss: 53924000.000000\n",
      "\n",
      "Test set: Avg. loss: 7631681.8166, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 486 [0/869 (0%)]\tLoss: 86355320.000000\n",
      "Train Epoch: 486 [160/869 (18%)]\tLoss: 120821200.000000\n",
      "Train Epoch: 486 [320/869 (36%)]\tLoss: 88318664.000000\n",
      "Train Epoch: 486 [480/869 (55%)]\tLoss: 350008704.000000\n",
      "Train Epoch: 486 [640/869 (73%)]\tLoss: 206056464.000000\n",
      "Train Epoch: 486 [800/869 (91%)]\tLoss: 139233952.000000\n",
      "\n",
      "Test set: Avg. loss: 7319474.5160, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 487 [0/869 (0%)]\tLoss: 98774008.000000\n",
      "Train Epoch: 487 [160/869 (18%)]\tLoss: 132347232.000000\n",
      "Train Epoch: 487 [320/869 (36%)]\tLoss: 163987152.000000\n",
      "Train Epoch: 487 [480/869 (55%)]\tLoss: 83129832.000000\n",
      "Train Epoch: 487 [640/869 (73%)]\tLoss: 61636520.000000\n",
      "Train Epoch: 487 [800/869 (91%)]\tLoss: 84233056.000000\n",
      "\n",
      "Test set: Avg. loss: 7504885.4925, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 488 [0/869 (0%)]\tLoss: 125489408.000000\n",
      "Train Epoch: 488 [160/869 (18%)]\tLoss: 161614000.000000\n",
      "Train Epoch: 488 [320/869 (36%)]\tLoss: 99865376.000000\n",
      "Train Epoch: 488 [480/869 (55%)]\tLoss: 46963400.000000\n",
      "Train Epoch: 488 [640/869 (73%)]\tLoss: 254431872.000000\n",
      "Train Epoch: 488 [800/869 (91%)]\tLoss: 270936608.000000\n",
      "\n",
      "Test set: Avg. loss: 7432133.3049, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 489 [0/869 (0%)]\tLoss: 130807264.000000\n",
      "Train Epoch: 489 [160/869 (18%)]\tLoss: 234209216.000000\n",
      "Train Epoch: 489 [320/869 (36%)]\tLoss: 278737600.000000\n",
      "Train Epoch: 489 [480/869 (55%)]\tLoss: 94158496.000000\n",
      "Train Epoch: 489 [640/869 (73%)]\tLoss: 117933760.000000\n",
      "Train Epoch: 489 [800/869 (91%)]\tLoss: 69201912.000000\n",
      "\n",
      "Test set: Avg. loss: 7548008.3412, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 490 [0/869 (0%)]\tLoss: 84102440.000000\n",
      "Train Epoch: 490 [160/869 (18%)]\tLoss: 307094304.000000\n",
      "Train Epoch: 490 [320/869 (36%)]\tLoss: 174262928.000000\n",
      "Train Epoch: 490 [480/869 (55%)]\tLoss: 106470032.000000\n",
      "Train Epoch: 490 [640/869 (73%)]\tLoss: 217026416.000000\n",
      "Train Epoch: 490 [800/869 (91%)]\tLoss: 164766144.000000\n",
      "\n",
      "Test set: Avg. loss: 7502352.1706, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 491 [0/869 (0%)]\tLoss: 174652768.000000\n",
      "Train Epoch: 491 [160/869 (18%)]\tLoss: 53352652.000000\n",
      "Train Epoch: 491 [320/869 (36%)]\tLoss: 136455296.000000\n",
      "Train Epoch: 491 [480/869 (55%)]\tLoss: 181855920.000000\n",
      "Train Epoch: 491 [640/869 (73%)]\tLoss: 124712000.000000\n",
      "Train Epoch: 491 [800/869 (91%)]\tLoss: 147548944.000000\n",
      "\n",
      "Test set: Avg. loss: 7457454.9424, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 492 [0/869 (0%)]\tLoss: 95589400.000000\n",
      "Train Epoch: 492 [160/869 (18%)]\tLoss: 158960736.000000\n",
      "Train Epoch: 492 [320/869 (36%)]\tLoss: 46962560.000000\n",
      "Train Epoch: 492 [480/869 (55%)]\tLoss: 118905376.000000\n",
      "Train Epoch: 492 [640/869 (73%)]\tLoss: 85468224.000000\n",
      "Train Epoch: 492 [800/869 (91%)]\tLoss: 134067912.000000\n",
      "\n",
      "Test set: Avg. loss: 7426199.7612, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 493 [0/869 (0%)]\tLoss: 80323264.000000\n",
      "Train Epoch: 493 [160/869 (18%)]\tLoss: 106591128.000000\n",
      "Train Epoch: 493 [320/869 (36%)]\tLoss: 34224460.000000\n",
      "Train Epoch: 493 [480/869 (55%)]\tLoss: 127355248.000000\n",
      "Train Epoch: 493 [640/869 (73%)]\tLoss: 55943136.000000\n",
      "Train Epoch: 493 [800/869 (91%)]\tLoss: 91815320.000000\n",
      "\n",
      "Test set: Avg. loss: 7507978.6780, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 494 [0/869 (0%)]\tLoss: 97152784.000000\n",
      "Train Epoch: 494 [160/869 (18%)]\tLoss: 218954464.000000\n",
      "Train Epoch: 494 [320/869 (36%)]\tLoss: 99119408.000000\n",
      "Train Epoch: 494 [480/869 (55%)]\tLoss: 123574416.000000\n",
      "Train Epoch: 494 [640/869 (73%)]\tLoss: 201011008.000000\n",
      "Train Epoch: 494 [800/869 (91%)]\tLoss: 104526632.000000\n",
      "\n",
      "Test set: Avg. loss: 7586419.1215, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 495 [0/869 (0%)]\tLoss: 226989568.000000\n",
      "Train Epoch: 495 [160/869 (18%)]\tLoss: 169833408.000000\n",
      "Train Epoch: 495 [320/869 (36%)]\tLoss: 105493904.000000\n",
      "Train Epoch: 495 [480/869 (55%)]\tLoss: 181000128.000000\n",
      "Train Epoch: 495 [640/869 (73%)]\tLoss: 164162992.000000\n",
      "Train Epoch: 495 [800/869 (91%)]\tLoss: 70460200.000000\n",
      "\n",
      "Test set: Avg. loss: 8373808.6397, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 496 [0/869 (0%)]\tLoss: 197630768.000000\n",
      "Train Epoch: 496 [160/869 (18%)]\tLoss: 34304360.000000\n",
      "Train Epoch: 496 [320/869 (36%)]\tLoss: 35943220.000000\n",
      "Train Epoch: 496 [480/869 (55%)]\tLoss: 251402272.000000\n",
      "Train Epoch: 496 [640/869 (73%)]\tLoss: 137361200.000000\n",
      "Train Epoch: 496 [800/869 (91%)]\tLoss: 140856080.000000\n",
      "\n",
      "Test set: Avg. loss: 7331872.2388, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 497 [0/869 (0%)]\tLoss: 103095688.000000\n",
      "Train Epoch: 497 [160/869 (18%)]\tLoss: 135596416.000000\n",
      "Train Epoch: 497 [320/869 (36%)]\tLoss: 82715368.000000\n",
      "Train Epoch: 497 [480/869 (55%)]\tLoss: 50381444.000000\n",
      "Train Epoch: 497 [640/869 (73%)]\tLoss: 192644736.000000\n",
      "Train Epoch: 497 [800/869 (91%)]\tLoss: 357843872.000000\n",
      "\n",
      "Test set: Avg. loss: 7522649.3049, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 498 [0/869 (0%)]\tLoss: 186041472.000000\n",
      "Train Epoch: 498 [160/869 (18%)]\tLoss: 91056280.000000\n",
      "Train Epoch: 498 [320/869 (36%)]\tLoss: 172469552.000000\n",
      "Train Epoch: 498 [480/869 (55%)]\tLoss: 186186048.000000\n",
      "Train Epoch: 498 [640/869 (73%)]\tLoss: 159897200.000000\n",
      "Train Epoch: 498 [800/869 (91%)]\tLoss: 191806144.000000\n",
      "\n",
      "Test set: Avg. loss: 7434338.9851, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 499 [0/869 (0%)]\tLoss: 180537952.000000\n",
      "Train Epoch: 499 [160/869 (18%)]\tLoss: 113847416.000000\n",
      "Train Epoch: 499 [320/869 (36%)]\tLoss: 85139496.000000\n",
      "Train Epoch: 499 [480/869 (55%)]\tLoss: 49533708.000000\n",
      "Train Epoch: 499 [640/869 (73%)]\tLoss: 91511464.000000\n",
      "Train Epoch: 499 [800/869 (91%)]\tLoss: 89569640.000000\n",
      "\n",
      "Test set: Avg. loss: 7392907.6674, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 500 [0/869 (0%)]\tLoss: 94286848.000000\n",
      "Train Epoch: 500 [160/869 (18%)]\tLoss: 81488240.000000\n",
      "Train Epoch: 500 [320/869 (36%)]\tLoss: 45187044.000000\n",
      "Train Epoch: 500 [480/869 (55%)]\tLoss: 249462544.000000\n",
      "Train Epoch: 500 [640/869 (73%)]\tLoss: 82302384.000000\n",
      "Train Epoch: 500 [800/869 (91%)]\tLoss: 102144528.000000\n",
      "\n",
      "Test set: Avg. loss: 7488332.6823, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 501 [0/869 (0%)]\tLoss: 139836480.000000\n",
      "Train Epoch: 501 [160/869 (18%)]\tLoss: 91939976.000000\n",
      "Train Epoch: 501 [320/869 (36%)]\tLoss: 193589088.000000\n",
      "Train Epoch: 501 [480/869 (55%)]\tLoss: 191185216.000000\n",
      "Train Epoch: 501 [640/869 (73%)]\tLoss: 110658720.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 501 [800/869 (91%)]\tLoss: 108715968.000000\n",
      "\n",
      "Test set: Avg. loss: 7492109.6290, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 502 [0/869 (0%)]\tLoss: 243606304.000000\n",
      "Train Epoch: 502 [160/869 (18%)]\tLoss: 192981552.000000\n",
      "Train Epoch: 502 [320/869 (36%)]\tLoss: 35516272.000000\n",
      "Train Epoch: 502 [480/869 (55%)]\tLoss: 81966168.000000\n",
      "Train Epoch: 502 [640/869 (73%)]\tLoss: 232176560.000000\n",
      "Train Epoch: 502 [800/869 (91%)]\tLoss: 151031424.000000\n",
      "\n",
      "Test set: Avg. loss: 7853613.9318, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 503 [0/869 (0%)]\tLoss: 56657996.000000\n",
      "Train Epoch: 503 [160/869 (18%)]\tLoss: 72585856.000000\n",
      "Train Epoch: 503 [320/869 (36%)]\tLoss: 55331068.000000\n",
      "Train Epoch: 503 [480/869 (55%)]\tLoss: 44240228.000000\n",
      "Train Epoch: 503 [640/869 (73%)]\tLoss: 41995080.000000\n",
      "Train Epoch: 503 [800/869 (91%)]\tLoss: 103657576.000000\n",
      "\n",
      "Test set: Avg. loss: 7633918.1151, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 504 [0/869 (0%)]\tLoss: 100268488.000000\n",
      "Train Epoch: 504 [160/869 (18%)]\tLoss: 135920608.000000\n",
      "Train Epoch: 504 [320/869 (36%)]\tLoss: 219494976.000000\n",
      "Train Epoch: 504 [480/869 (55%)]\tLoss: 88882768.000000\n",
      "Train Epoch: 504 [640/869 (73%)]\tLoss: 238086848.000000\n",
      "Train Epoch: 504 [800/869 (91%)]\tLoss: 138697088.000000\n",
      "\n",
      "Test set: Avg. loss: 7523121.1087, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 505 [0/869 (0%)]\tLoss: 178898096.000000\n",
      "Train Epoch: 505 [160/869 (18%)]\tLoss: 216700592.000000\n",
      "Train Epoch: 505 [320/869 (36%)]\tLoss: 83881840.000000\n",
      "Train Epoch: 505 [480/869 (55%)]\tLoss: 88859552.000000\n",
      "Train Epoch: 505 [640/869 (73%)]\tLoss: 97595136.000000\n",
      "Train Epoch: 505 [800/869 (91%)]\tLoss: 118370016.000000\n",
      "\n",
      "Test set: Avg. loss: 7587200.6226, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 506 [0/869 (0%)]\tLoss: 114392776.000000\n",
      "Train Epoch: 506 [160/869 (18%)]\tLoss: 126662344.000000\n",
      "Train Epoch: 506 [320/869 (36%)]\tLoss: 145989216.000000\n",
      "Train Epoch: 506 [480/869 (55%)]\tLoss: 143920656.000000\n",
      "Train Epoch: 506 [640/869 (73%)]\tLoss: 117778168.000000\n",
      "Train Epoch: 506 [800/869 (91%)]\tLoss: 30215908.000000\n",
      "\n",
      "Test set: Avg. loss: 7423956.8657, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 507 [0/869 (0%)]\tLoss: 120337704.000000\n",
      "Train Epoch: 507 [160/869 (18%)]\tLoss: 164249168.000000\n",
      "Train Epoch: 507 [320/869 (36%)]\tLoss: 87984120.000000\n",
      "Train Epoch: 507 [480/869 (55%)]\tLoss: 116728456.000000\n",
      "Train Epoch: 507 [640/869 (73%)]\tLoss: 40898036.000000\n",
      "Train Epoch: 507 [800/869 (91%)]\tLoss: 94045640.000000\n",
      "\n",
      "Test set: Avg. loss: 7666309.8934, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 508 [0/869 (0%)]\tLoss: 128092136.000000\n",
      "Train Epoch: 508 [160/869 (18%)]\tLoss: 92882192.000000\n",
      "Train Epoch: 508 [320/869 (36%)]\tLoss: 144801088.000000\n",
      "Train Epoch: 508 [480/869 (55%)]\tLoss: 131628752.000000\n",
      "Train Epoch: 508 [640/869 (73%)]\tLoss: 84662808.000000\n",
      "Train Epoch: 508 [800/869 (91%)]\tLoss: 95081168.000000\n",
      "\n",
      "Test set: Avg. loss: 7312024.4264, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 509 [0/869 (0%)]\tLoss: 264465664.000000\n",
      "Train Epoch: 509 [160/869 (18%)]\tLoss: 128784208.000000\n",
      "Train Epoch: 509 [320/869 (36%)]\tLoss: 125859376.000000\n",
      "Train Epoch: 509 [480/869 (55%)]\tLoss: 50101864.000000\n",
      "Train Epoch: 509 [640/869 (73%)]\tLoss: 149776624.000000\n",
      "Train Epoch: 509 [800/869 (91%)]\tLoss: 150328480.000000\n",
      "\n",
      "Test set: Avg. loss: 7603356.4350, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 510 [0/869 (0%)]\tLoss: 115583936.000000\n",
      "Train Epoch: 510 [160/869 (18%)]\tLoss: 177263424.000000\n",
      "Train Epoch: 510 [320/869 (36%)]\tLoss: 222221696.000000\n",
      "Train Epoch: 510 [480/869 (55%)]\tLoss: 188242448.000000\n",
      "Train Epoch: 510 [640/869 (73%)]\tLoss: 272503360.000000\n",
      "Train Epoch: 510 [800/869 (91%)]\tLoss: 129232176.000000\n",
      "\n",
      "Test set: Avg. loss: 7430284.3753, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 511 [0/869 (0%)]\tLoss: 250405280.000000\n",
      "Train Epoch: 511 [160/869 (18%)]\tLoss: 124418576.000000\n",
      "Train Epoch: 511 [320/869 (36%)]\tLoss: 160252944.000000\n",
      "Train Epoch: 511 [480/869 (55%)]\tLoss: 153486608.000000\n",
      "Train Epoch: 511 [640/869 (73%)]\tLoss: 53086356.000000\n",
      "Train Epoch: 511 [800/869 (91%)]\tLoss: 139115568.000000\n",
      "\n",
      "Test set: Avg. loss: 7395727.4542, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 512 [0/869 (0%)]\tLoss: 213545936.000000\n",
      "Train Epoch: 512 [160/869 (18%)]\tLoss: 77013408.000000\n",
      "Train Epoch: 512 [320/869 (36%)]\tLoss: 79185808.000000\n",
      "Train Epoch: 512 [480/869 (55%)]\tLoss: 211975328.000000\n",
      "Train Epoch: 512 [640/869 (73%)]\tLoss: 101339688.000000\n",
      "Train Epoch: 512 [800/869 (91%)]\tLoss: 189336704.000000\n",
      "\n",
      "Test set: Avg. loss: 7817775.0661, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 513 [0/869 (0%)]\tLoss: 219225264.000000\n",
      "Train Epoch: 513 [160/869 (18%)]\tLoss: 124072176.000000\n",
      "Train Epoch: 513 [320/869 (36%)]\tLoss: 113338456.000000\n",
      "Train Epoch: 513 [480/869 (55%)]\tLoss: 148109536.000000\n",
      "Train Epoch: 513 [640/869 (73%)]\tLoss: 211109264.000000\n",
      "Train Epoch: 513 [800/869 (91%)]\tLoss: 186291936.000000\n",
      "\n",
      "Test set: Avg. loss: 7526832.6994, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 514 [0/869 (0%)]\tLoss: 139273216.000000\n",
      "Train Epoch: 514 [160/869 (18%)]\tLoss: 115218880.000000\n",
      "Train Epoch: 514 [320/869 (36%)]\tLoss: 73414248.000000\n",
      "Train Epoch: 514 [480/869 (55%)]\tLoss: 87274080.000000\n",
      "Train Epoch: 514 [640/869 (73%)]\tLoss: 80096136.000000\n",
      "Train Epoch: 514 [800/869 (91%)]\tLoss: 246316496.000000\n",
      "\n",
      "Test set: Avg. loss: 7539260.3838, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 515 [0/869 (0%)]\tLoss: 126810184.000000\n",
      "Train Epoch: 515 [160/869 (18%)]\tLoss: 85855512.000000\n",
      "Train Epoch: 515 [320/869 (36%)]\tLoss: 59935296.000000\n",
      "Train Epoch: 515 [480/869 (55%)]\tLoss: 127589120.000000\n",
      "Train Epoch: 515 [640/869 (73%)]\tLoss: 132476416.000000\n",
      "Train Epoch: 515 [800/869 (91%)]\tLoss: 145969264.000000\n",
      "\n",
      "Test set: Avg. loss: 7661579.0192, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 516 [0/869 (0%)]\tLoss: 426344192.000000\n",
      "Train Epoch: 516 [160/869 (18%)]\tLoss: 39616828.000000\n",
      "Train Epoch: 516 [320/869 (36%)]\tLoss: 108852728.000000\n",
      "Train Epoch: 516 [480/869 (55%)]\tLoss: 154621056.000000\n",
      "Train Epoch: 516 [640/869 (73%)]\tLoss: 181488080.000000\n",
      "Train Epoch: 516 [800/869 (91%)]\tLoss: 60204924.000000\n",
      "\n",
      "Test set: Avg. loss: 7453550.6269, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 517 [0/869 (0%)]\tLoss: 242381072.000000\n",
      "Train Epoch: 517 [160/869 (18%)]\tLoss: 116264728.000000\n",
      "Train Epoch: 517 [320/869 (36%)]\tLoss: 102346144.000000\n",
      "Train Epoch: 517 [480/869 (55%)]\tLoss: 135468288.000000\n",
      "Train Epoch: 517 [640/869 (73%)]\tLoss: 88142840.000000\n",
      "Train Epoch: 517 [800/869 (91%)]\tLoss: 216113584.000000\n",
      "\n",
      "Test set: Avg. loss: 7522605.1343, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 518 [0/869 (0%)]\tLoss: 231017504.000000\n",
      "Train Epoch: 518 [160/869 (18%)]\tLoss: 183771472.000000\n",
      "Train Epoch: 518 [320/869 (36%)]\tLoss: 190793840.000000\n",
      "Train Epoch: 518 [480/869 (55%)]\tLoss: 126863400.000000\n",
      "Train Epoch: 518 [640/869 (73%)]\tLoss: 149150064.000000\n",
      "Train Epoch: 518 [800/869 (91%)]\tLoss: 90242568.000000\n",
      "\n",
      "Test set: Avg. loss: 7352872.0426, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 519 [0/869 (0%)]\tLoss: 215336272.000000\n",
      "Train Epoch: 519 [160/869 (18%)]\tLoss: 79685752.000000\n",
      "Train Epoch: 519 [320/869 (36%)]\tLoss: 151334464.000000\n",
      "Train Epoch: 519 [480/869 (55%)]\tLoss: 159031040.000000\n",
      "Train Epoch: 519 [640/869 (73%)]\tLoss: 129334688.000000\n",
      "Train Epoch: 519 [800/869 (91%)]\tLoss: 84243960.000000\n",
      "\n",
      "Test set: Avg. loss: 7547627.7186, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 520 [0/869 (0%)]\tLoss: 261089808.000000\n",
      "Train Epoch: 520 [160/869 (18%)]\tLoss: 199285392.000000\n",
      "Train Epoch: 520 [320/869 (36%)]\tLoss: 198259056.000000\n",
      "Train Epoch: 520 [480/869 (55%)]\tLoss: 93141208.000000\n",
      "Train Epoch: 520 [640/869 (73%)]\tLoss: 36506576.000000\n",
      "Train Epoch: 520 [800/869 (91%)]\tLoss: 124203768.000000\n",
      "\n",
      "Test set: Avg. loss: 7589687.2324, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 521 [0/869 (0%)]\tLoss: 60370576.000000\n",
      "Train Epoch: 521 [160/869 (18%)]\tLoss: 80781648.000000\n",
      "Train Epoch: 521 [320/869 (36%)]\tLoss: 93279120.000000\n",
      "Train Epoch: 521 [480/869 (55%)]\tLoss: 77434224.000000\n",
      "Train Epoch: 521 [640/869 (73%)]\tLoss: 295047744.000000\n",
      "Train Epoch: 521 [800/869 (91%)]\tLoss: 289085760.000000\n",
      "\n",
      "Test set: Avg. loss: 8029402.1748, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 522 [0/869 (0%)]\tLoss: 115131272.000000\n",
      "Train Epoch: 522 [160/869 (18%)]\tLoss: 152400912.000000\n",
      "Train Epoch: 522 [320/869 (36%)]\tLoss: 160600064.000000\n",
      "Train Epoch: 522 [480/869 (55%)]\tLoss: 123769272.000000\n",
      "Train Epoch: 522 [640/869 (73%)]\tLoss: 174111280.000000\n",
      "Train Epoch: 522 [800/869 (91%)]\tLoss: 85384184.000000\n",
      "\n",
      "Test set: Avg. loss: 7428558.0384, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 523 [0/869 (0%)]\tLoss: 175117328.000000\n",
      "Train Epoch: 523 [160/869 (18%)]\tLoss: 202431632.000000\n",
      "Train Epoch: 523 [320/869 (36%)]\tLoss: 51965036.000000\n",
      "Train Epoch: 523 [480/869 (55%)]\tLoss: 116743608.000000\n",
      "Train Epoch: 523 [640/869 (73%)]\tLoss: 44525304.000000\n",
      "Train Epoch: 523 [800/869 (91%)]\tLoss: 87750192.000000\n",
      "\n",
      "Test set: Avg. loss: 7337280.8870, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 524 [0/869 (0%)]\tLoss: 136337552.000000\n",
      "Train Epoch: 524 [160/869 (18%)]\tLoss: 123607680.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 524 [320/869 (36%)]\tLoss: 88945768.000000\n",
      "Train Epoch: 524 [480/869 (55%)]\tLoss: 177663952.000000\n",
      "Train Epoch: 524 [640/869 (73%)]\tLoss: 147925584.000000\n",
      "Train Epoch: 524 [800/869 (91%)]\tLoss: 129412408.000000\n",
      "\n",
      "Test set: Avg. loss: 7542940.8955, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 525 [0/869 (0%)]\tLoss: 30158636.000000\n",
      "Train Epoch: 525 [160/869 (18%)]\tLoss: 174511904.000000\n",
      "Train Epoch: 525 [320/869 (36%)]\tLoss: 256574528.000000\n",
      "Train Epoch: 525 [480/869 (55%)]\tLoss: 247189920.000000\n",
      "Train Epoch: 525 [640/869 (73%)]\tLoss: 147828816.000000\n",
      "Train Epoch: 525 [800/869 (91%)]\tLoss: 242944848.000000\n",
      "\n",
      "Test set: Avg. loss: 7809615.1557, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 526 [0/869 (0%)]\tLoss: 34377144.000000\n",
      "Train Epoch: 526 [160/869 (18%)]\tLoss: 44353288.000000\n",
      "Train Epoch: 526 [320/869 (36%)]\tLoss: 90680800.000000\n",
      "Train Epoch: 526 [480/869 (55%)]\tLoss: 150413936.000000\n",
      "Train Epoch: 526 [640/869 (73%)]\tLoss: 128133168.000000\n",
      "Train Epoch: 526 [800/869 (91%)]\tLoss: 131559360.000000\n",
      "\n",
      "Test set: Avg. loss: 7850882.5288, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 527 [0/869 (0%)]\tLoss: 205848896.000000\n",
      "Train Epoch: 527 [160/869 (18%)]\tLoss: 130781592.000000\n",
      "Train Epoch: 527 [320/869 (36%)]\tLoss: 90525408.000000\n",
      "Train Epoch: 527 [480/869 (55%)]\tLoss: 239669200.000000\n",
      "Train Epoch: 527 [640/869 (73%)]\tLoss: 192087984.000000\n",
      "Train Epoch: 527 [800/869 (91%)]\tLoss: 75498584.000000\n",
      "\n",
      "Test set: Avg. loss: 7554616.1791, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 528 [0/869 (0%)]\tLoss: 88063216.000000\n",
      "Train Epoch: 528 [160/869 (18%)]\tLoss: 90838784.000000\n",
      "Train Epoch: 528 [320/869 (36%)]\tLoss: 201088960.000000\n",
      "Train Epoch: 528 [480/869 (55%)]\tLoss: 221255744.000000\n",
      "Train Epoch: 528 [640/869 (73%)]\tLoss: 129136040.000000\n",
      "Train Epoch: 528 [800/869 (91%)]\tLoss: 181488192.000000\n",
      "\n",
      "Test set: Avg. loss: 7427857.5267, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 529 [0/869 (0%)]\tLoss: 199781056.000000\n",
      "Train Epoch: 529 [160/869 (18%)]\tLoss: 97736056.000000\n",
      "Train Epoch: 529 [320/869 (36%)]\tLoss: 212112432.000000\n",
      "Train Epoch: 529 [480/869 (55%)]\tLoss: 196488416.000000\n",
      "Train Epoch: 529 [640/869 (73%)]\tLoss: 33234860.000000\n",
      "Train Epoch: 529 [800/869 (91%)]\tLoss: 229572336.000000\n",
      "\n",
      "Test set: Avg. loss: 7389629.0917, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 530 [0/869 (0%)]\tLoss: 93980392.000000\n",
      "Train Epoch: 530 [160/869 (18%)]\tLoss: 148144800.000000\n",
      "Train Epoch: 530 [320/869 (36%)]\tLoss: 162252352.000000\n",
      "Train Epoch: 530 [480/869 (55%)]\tLoss: 82425392.000000\n",
      "Train Epoch: 530 [640/869 (73%)]\tLoss: 143584256.000000\n",
      "Train Epoch: 530 [800/869 (91%)]\tLoss: 85964960.000000\n",
      "\n",
      "Test set: Avg. loss: 7529938.9936, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 531 [0/869 (0%)]\tLoss: 126565872.000000\n",
      "Train Epoch: 531 [160/869 (18%)]\tLoss: 98521752.000000\n",
      "Train Epoch: 531 [320/869 (36%)]\tLoss: 177602544.000000\n",
      "Train Epoch: 531 [480/869 (55%)]\tLoss: 294982336.000000\n",
      "Train Epoch: 531 [640/869 (73%)]\tLoss: 94321136.000000\n",
      "Train Epoch: 531 [800/869 (91%)]\tLoss: 134439680.000000\n",
      "\n",
      "Test set: Avg. loss: 8104458.0949, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 532 [0/869 (0%)]\tLoss: 72766144.000000\n",
      "Train Epoch: 532 [160/869 (18%)]\tLoss: 132130344.000000\n",
      "Train Epoch: 532 [320/869 (36%)]\tLoss: 40151860.000000\n",
      "Train Epoch: 532 [480/869 (55%)]\tLoss: 212327792.000000\n",
      "Train Epoch: 532 [640/869 (73%)]\tLoss: 119619744.000000\n",
      "Train Epoch: 532 [800/869 (91%)]\tLoss: 162097536.000000\n",
      "\n",
      "Test set: Avg. loss: 7653104.0000, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 533 [0/869 (0%)]\tLoss: 28476828.000000\n",
      "Train Epoch: 533 [160/869 (18%)]\tLoss: 161651136.000000\n",
      "Train Epoch: 533 [320/869 (36%)]\tLoss: 45916684.000000\n",
      "Train Epoch: 533 [480/869 (55%)]\tLoss: 94291880.000000\n",
      "Train Epoch: 533 [640/869 (73%)]\tLoss: 153077520.000000\n",
      "Train Epoch: 533 [800/869 (91%)]\tLoss: 35500808.000000\n",
      "\n",
      "Test set: Avg. loss: 7406058.6951, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 534 [0/869 (0%)]\tLoss: 44211284.000000\n",
      "Train Epoch: 534 [160/869 (18%)]\tLoss: 106325064.000000\n",
      "Train Epoch: 534 [320/869 (36%)]\tLoss: 72662032.000000\n",
      "Train Epoch: 534 [480/869 (55%)]\tLoss: 139176816.000000\n",
      "Train Epoch: 534 [640/869 (73%)]\tLoss: 108559216.000000\n",
      "Train Epoch: 534 [800/869 (91%)]\tLoss: 134115376.000000\n",
      "\n",
      "Test set: Avg. loss: 7367611.3177, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 535 [0/869 (0%)]\tLoss: 185974720.000000\n",
      "Train Epoch: 535 [160/869 (18%)]\tLoss: 191951248.000000\n",
      "Train Epoch: 535 [320/869 (36%)]\tLoss: 138545152.000000\n",
      "Train Epoch: 535 [480/869 (55%)]\tLoss: 169130176.000000\n",
      "Train Epoch: 535 [640/869 (73%)]\tLoss: 32051468.000000\n",
      "Train Epoch: 535 [800/869 (91%)]\tLoss: 82031240.000000\n",
      "\n",
      "Test set: Avg. loss: 7666660.7633, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 536 [0/869 (0%)]\tLoss: 23404912.000000\n",
      "Train Epoch: 536 [160/869 (18%)]\tLoss: 44906752.000000\n",
      "Train Epoch: 536 [320/869 (36%)]\tLoss: 120403992.000000\n",
      "Train Epoch: 536 [480/869 (55%)]\tLoss: 108070176.000000\n",
      "Train Epoch: 536 [640/869 (73%)]\tLoss: 142296416.000000\n",
      "Train Epoch: 536 [800/869 (91%)]\tLoss: 206626448.000000\n",
      "\n",
      "Test set: Avg. loss: 7883229.7825, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 537 [0/869 (0%)]\tLoss: 225750672.000000\n",
      "Train Epoch: 537 [160/869 (18%)]\tLoss: 295681888.000000\n",
      "Train Epoch: 537 [320/869 (36%)]\tLoss: 204867328.000000\n",
      "Train Epoch: 537 [480/869 (55%)]\tLoss: 187183424.000000\n",
      "Train Epoch: 537 [640/869 (73%)]\tLoss: 139974384.000000\n",
      "Train Epoch: 537 [800/869 (91%)]\tLoss: 246405520.000000\n",
      "\n",
      "Test set: Avg. loss: 7764223.3518, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 538 [0/869 (0%)]\tLoss: 60008500.000000\n",
      "Train Epoch: 538 [160/869 (18%)]\tLoss: 165944736.000000\n",
      "Train Epoch: 538 [320/869 (36%)]\tLoss: 54108620.000000\n",
      "Train Epoch: 538 [480/869 (55%)]\tLoss: 195047664.000000\n",
      "Train Epoch: 538 [640/869 (73%)]\tLoss: 146569680.000000\n",
      "Train Epoch: 538 [800/869 (91%)]\tLoss: 151952592.000000\n",
      "\n",
      "Test set: Avg. loss: 7654859.1898, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 539 [0/869 (0%)]\tLoss: 120571880.000000\n",
      "Train Epoch: 539 [160/869 (18%)]\tLoss: 104880896.000000\n",
      "Train Epoch: 539 [320/869 (36%)]\tLoss: 98126064.000000\n",
      "Train Epoch: 539 [480/869 (55%)]\tLoss: 18429834.000000\n",
      "Train Epoch: 539 [640/869 (73%)]\tLoss: 74605800.000000\n",
      "Train Epoch: 539 [800/869 (91%)]\tLoss: 165939888.000000\n",
      "\n",
      "Test set: Avg. loss: 7324730.7548, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 540 [0/869 (0%)]\tLoss: 192056112.000000\n",
      "Train Epoch: 540 [160/869 (18%)]\tLoss: 61555056.000000\n",
      "Train Epoch: 540 [320/869 (36%)]\tLoss: 93992704.000000\n",
      "Train Epoch: 540 [480/869 (55%)]\tLoss: 180721408.000000\n",
      "Train Epoch: 540 [640/869 (73%)]\tLoss: 106515856.000000\n",
      "Train Epoch: 540 [800/869 (91%)]\tLoss: 42189984.000000\n",
      "\n",
      "Test set: Avg. loss: 7885172.4264, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 541 [0/869 (0%)]\tLoss: 170326368.000000\n",
      "Train Epoch: 541 [160/869 (18%)]\tLoss: 55406672.000000\n",
      "Train Epoch: 541 [320/869 (36%)]\tLoss: 130319048.000000\n",
      "Train Epoch: 541 [480/869 (55%)]\tLoss: 204207520.000000\n",
      "Train Epoch: 541 [640/869 (73%)]\tLoss: 88402432.000000\n",
      "Train Epoch: 541 [800/869 (91%)]\tLoss: 262491104.000000\n",
      "\n",
      "Test set: Avg. loss: 7458897.7825, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 542 [0/869 (0%)]\tLoss: 183942960.000000\n",
      "Train Epoch: 542 [160/869 (18%)]\tLoss: 125041488.000000\n",
      "Train Epoch: 542 [320/869 (36%)]\tLoss: 211941792.000000\n",
      "Train Epoch: 542 [480/869 (55%)]\tLoss: 31185374.000000\n",
      "Train Epoch: 542 [640/869 (73%)]\tLoss: 111521568.000000\n",
      "Train Epoch: 542 [800/869 (91%)]\tLoss: 292173344.000000\n",
      "\n",
      "Test set: Avg. loss: 7402640.0000, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 543 [0/869 (0%)]\tLoss: 50419344.000000\n",
      "Train Epoch: 543 [160/869 (18%)]\tLoss: 190305072.000000\n",
      "Train Epoch: 543 [320/869 (36%)]\tLoss: 67161344.000000\n",
      "Train Epoch: 543 [480/869 (55%)]\tLoss: 121081288.000000\n",
      "Train Epoch: 543 [640/869 (73%)]\tLoss: 123079768.000000\n",
      "Train Epoch: 543 [800/869 (91%)]\tLoss: 92724416.000000\n",
      "\n",
      "Test set: Avg. loss: 7307320.9552, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 544 [0/869 (0%)]\tLoss: 167359264.000000\n",
      "Train Epoch: 544 [160/869 (18%)]\tLoss: 104172696.000000\n",
      "Train Epoch: 544 [320/869 (36%)]\tLoss: 130211632.000000\n",
      "Train Epoch: 544 [480/869 (55%)]\tLoss: 158040656.000000\n",
      "Train Epoch: 544 [640/869 (73%)]\tLoss: 98619136.000000\n",
      "Train Epoch: 544 [800/869 (91%)]\tLoss: 250824544.000000\n",
      "\n",
      "Test set: Avg. loss: 7598985.5864, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 545 [0/869 (0%)]\tLoss: 231535280.000000\n",
      "Train Epoch: 545 [160/869 (18%)]\tLoss: 127184232.000000\n",
      "Train Epoch: 545 [320/869 (36%)]\tLoss: 174514112.000000\n",
      "Train Epoch: 545 [480/869 (55%)]\tLoss: 184809472.000000\n",
      "Train Epoch: 545 [640/869 (73%)]\tLoss: 36053816.000000\n",
      "Train Epoch: 545 [800/869 (91%)]\tLoss: 177319280.000000\n",
      "\n",
      "Test set: Avg. loss: 7726633.5778, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 546 [0/869 (0%)]\tLoss: 137437936.000000\n",
      "Train Epoch: 546 [160/869 (18%)]\tLoss: 182159472.000000\n",
      "Train Epoch: 546 [320/869 (36%)]\tLoss: 294816128.000000\n",
      "Train Epoch: 546 [480/869 (55%)]\tLoss: 464697792.000000\n",
      "Train Epoch: 546 [640/869 (73%)]\tLoss: 42651020.000000\n",
      "Train Epoch: 546 [800/869 (91%)]\tLoss: 187127520.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 7440989.9190, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 547 [0/869 (0%)]\tLoss: 107898600.000000\n",
      "Train Epoch: 547 [160/869 (18%)]\tLoss: 255497344.000000\n",
      "Train Epoch: 547 [320/869 (36%)]\tLoss: 90911360.000000\n",
      "Train Epoch: 547 [480/869 (55%)]\tLoss: 123612064.000000\n",
      "Train Epoch: 547 [640/869 (73%)]\tLoss: 78674960.000000\n",
      "Train Epoch: 547 [800/869 (91%)]\tLoss: 81796208.000000\n",
      "\n",
      "Test set: Avg. loss: 7388924.3923, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 548 [0/869 (0%)]\tLoss: 47882528.000000\n",
      "Train Epoch: 548 [160/869 (18%)]\tLoss: 104652232.000000\n",
      "Train Epoch: 548 [320/869 (36%)]\tLoss: 56810104.000000\n",
      "Train Epoch: 548 [480/869 (55%)]\tLoss: 195182224.000000\n",
      "Train Epoch: 548 [640/869 (73%)]\tLoss: 156201920.000000\n",
      "Train Epoch: 548 [800/869 (91%)]\tLoss: 181995616.000000\n",
      "\n",
      "Test set: Avg. loss: 7402660.8614, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 549 [0/869 (0%)]\tLoss: 116897056.000000\n",
      "Train Epoch: 549 [160/869 (18%)]\tLoss: 77913960.000000\n",
      "Train Epoch: 549 [320/869 (36%)]\tLoss: 210168624.000000\n",
      "Train Epoch: 549 [480/869 (55%)]\tLoss: 132436088.000000\n",
      "Train Epoch: 549 [640/869 (73%)]\tLoss: 55425412.000000\n",
      "Train Epoch: 549 [800/869 (91%)]\tLoss: 170988192.000000\n",
      "\n",
      "Test set: Avg. loss: 7752907.6503, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 550 [0/869 (0%)]\tLoss: 251504160.000000\n",
      "Train Epoch: 550 [160/869 (18%)]\tLoss: 124195400.000000\n",
      "Train Epoch: 550 [320/869 (36%)]\tLoss: 207466464.000000\n",
      "Train Epoch: 550 [480/869 (55%)]\tLoss: 60180064.000000\n",
      "Train Epoch: 550 [640/869 (73%)]\tLoss: 137296896.000000\n",
      "Train Epoch: 550 [800/869 (91%)]\tLoss: 214888032.000000\n",
      "\n",
      "Test set: Avg. loss: 7460119.0959, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 551 [0/869 (0%)]\tLoss: 116376568.000000\n",
      "Train Epoch: 551 [160/869 (18%)]\tLoss: 126200832.000000\n",
      "Train Epoch: 551 [320/869 (36%)]\tLoss: 309145600.000000\n",
      "Train Epoch: 551 [480/869 (55%)]\tLoss: 83329600.000000\n",
      "Train Epoch: 551 [640/869 (73%)]\tLoss: 128827976.000000\n",
      "Train Epoch: 551 [800/869 (91%)]\tLoss: 208588096.000000\n",
      "\n",
      "Test set: Avg. loss: 7552955.3305, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 552 [0/869 (0%)]\tLoss: 99925504.000000\n",
      "Train Epoch: 552 [160/869 (18%)]\tLoss: 40251348.000000\n",
      "Train Epoch: 552 [320/869 (36%)]\tLoss: 84242176.000000\n",
      "Train Epoch: 552 [480/869 (55%)]\tLoss: 91214944.000000\n",
      "Train Epoch: 552 [640/869 (73%)]\tLoss: 38219216.000000\n",
      "Train Epoch: 552 [800/869 (91%)]\tLoss: 199722224.000000\n",
      "\n",
      "Test set: Avg. loss: 7743373.8252, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 553 [0/869 (0%)]\tLoss: 73833016.000000\n",
      "Train Epoch: 553 [160/869 (18%)]\tLoss: 62995408.000000\n",
      "Train Epoch: 553 [320/869 (36%)]\tLoss: 103805216.000000\n",
      "Train Epoch: 553 [480/869 (55%)]\tLoss: 234106608.000000\n",
      "Train Epoch: 553 [640/869 (73%)]\tLoss: 316208256.000000\n",
      "Train Epoch: 553 [800/869 (91%)]\tLoss: 178946048.000000\n",
      "\n",
      "Test set: Avg. loss: 7741020.2004, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 554 [0/869 (0%)]\tLoss: 193025664.000000\n",
      "Train Epoch: 554 [160/869 (18%)]\tLoss: 83199376.000000\n",
      "Train Epoch: 554 [320/869 (36%)]\tLoss: 155650544.000000\n",
      "Train Epoch: 554 [480/869 (55%)]\tLoss: 123525952.000000\n",
      "Train Epoch: 554 [640/869 (73%)]\tLoss: 209531664.000000\n",
      "Train Epoch: 554 [800/869 (91%)]\tLoss: 80832976.000000\n",
      "\n",
      "Test set: Avg. loss: 7470159.3987, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 555 [0/869 (0%)]\tLoss: 79810856.000000\n",
      "Train Epoch: 555 [160/869 (18%)]\tLoss: 180845856.000000\n",
      "Train Epoch: 555 [320/869 (36%)]\tLoss: 220734144.000000\n",
      "Train Epoch: 555 [480/869 (55%)]\tLoss: 41875440.000000\n",
      "Train Epoch: 555 [640/869 (73%)]\tLoss: 120027040.000000\n",
      "Train Epoch: 555 [800/869 (91%)]\tLoss: 179019568.000000\n",
      "\n",
      "Test set: Avg. loss: 7676050.6226, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 556 [0/869 (0%)]\tLoss: 92728448.000000\n",
      "Train Epoch: 556 [160/869 (18%)]\tLoss: 185751696.000000\n",
      "Train Epoch: 556 [320/869 (36%)]\tLoss: 44621236.000000\n",
      "Train Epoch: 556 [480/869 (55%)]\tLoss: 133493104.000000\n",
      "Train Epoch: 556 [640/869 (73%)]\tLoss: 88224216.000000\n",
      "Train Epoch: 556 [800/869 (91%)]\tLoss: 82285520.000000\n",
      "\n",
      "Test set: Avg. loss: 7453731.8465, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 557 [0/869 (0%)]\tLoss: 179427040.000000\n",
      "Train Epoch: 557 [160/869 (18%)]\tLoss: 81177392.000000\n",
      "Train Epoch: 557 [320/869 (36%)]\tLoss: 134154216.000000\n",
      "Train Epoch: 557 [480/869 (55%)]\tLoss: 167468272.000000\n",
      "Train Epoch: 557 [640/869 (73%)]\tLoss: 220531888.000000\n",
      "Train Epoch: 557 [800/869 (91%)]\tLoss: 107162816.000000\n",
      "\n",
      "Test set: Avg. loss: 7590092.3582, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 558 [0/869 (0%)]\tLoss: 93089568.000000\n",
      "Train Epoch: 558 [160/869 (18%)]\tLoss: 170876176.000000\n",
      "Train Epoch: 558 [320/869 (36%)]\tLoss: 131517952.000000\n",
      "Train Epoch: 558 [480/869 (55%)]\tLoss: 112119920.000000\n",
      "Train Epoch: 558 [640/869 (73%)]\tLoss: 45465068.000000\n",
      "Train Epoch: 558 [800/869 (91%)]\tLoss: 226502528.000000\n",
      "\n",
      "Test set: Avg. loss: 7532161.1258, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 559 [0/869 (0%)]\tLoss: 194086560.000000\n",
      "Train Epoch: 559 [160/869 (18%)]\tLoss: 87833720.000000\n",
      "Train Epoch: 559 [320/869 (36%)]\tLoss: 198066640.000000\n",
      "Train Epoch: 559 [480/869 (55%)]\tLoss: 90269808.000000\n",
      "Train Epoch: 559 [640/869 (73%)]\tLoss: 167351840.000000\n",
      "Train Epoch: 559 [800/869 (91%)]\tLoss: 86598216.000000\n",
      "\n",
      "Test set: Avg. loss: 7751964.9296, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 560 [0/869 (0%)]\tLoss: 80785312.000000\n",
      "Train Epoch: 560 [160/869 (18%)]\tLoss: 82720432.000000\n",
      "Train Epoch: 560 [320/869 (36%)]\tLoss: 112005864.000000\n",
      "Train Epoch: 560 [480/869 (55%)]\tLoss: 42066440.000000\n",
      "Train Epoch: 560 [640/869 (73%)]\tLoss: 98116088.000000\n",
      "Train Epoch: 560 [800/869 (91%)]\tLoss: 177605808.000000\n",
      "\n",
      "Test set: Avg. loss: 7382352.2857, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 561 [0/869 (0%)]\tLoss: 86988320.000000\n",
      "Train Epoch: 561 [160/869 (18%)]\tLoss: 236518832.000000\n",
      "Train Epoch: 561 [320/869 (36%)]\tLoss: 62226524.000000\n",
      "Train Epoch: 561 [480/869 (55%)]\tLoss: 102384152.000000\n",
      "Train Epoch: 561 [640/869 (73%)]\tLoss: 138855152.000000\n",
      "Train Epoch: 561 [800/869 (91%)]\tLoss: 78258680.000000\n",
      "\n",
      "Test set: Avg. loss: 7388304.2559, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 562 [0/869 (0%)]\tLoss: 243588384.000000\n",
      "Train Epoch: 562 [160/869 (18%)]\tLoss: 73817472.000000\n",
      "Train Epoch: 562 [320/869 (36%)]\tLoss: 59618552.000000\n",
      "Train Epoch: 562 [480/869 (55%)]\tLoss: 82068040.000000\n",
      "Train Epoch: 562 [640/869 (73%)]\tLoss: 96270400.000000\n",
      "Train Epoch: 562 [800/869 (91%)]\tLoss: 245418272.000000\n",
      "\n",
      "Test set: Avg. loss: 7410345.5693, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 563 [0/869 (0%)]\tLoss: 150917888.000000\n",
      "Train Epoch: 563 [160/869 (18%)]\tLoss: 82665736.000000\n",
      "Train Epoch: 563 [320/869 (36%)]\tLoss: 103652928.000000\n",
      "Train Epoch: 563 [480/869 (55%)]\tLoss: 134833568.000000\n",
      "Train Epoch: 563 [640/869 (73%)]\tLoss: 44842344.000000\n",
      "Train Epoch: 563 [800/869 (91%)]\tLoss: 70678008.000000\n",
      "\n",
      "Test set: Avg. loss: 7705475.7868, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 564 [0/869 (0%)]\tLoss: 69129448.000000\n",
      "Train Epoch: 564 [160/869 (18%)]\tLoss: 91127112.000000\n",
      "Train Epoch: 564 [320/869 (36%)]\tLoss: 96878200.000000\n",
      "Train Epoch: 564 [480/869 (55%)]\tLoss: 142786416.000000\n",
      "Train Epoch: 564 [640/869 (73%)]\tLoss: 127704200.000000\n",
      "Train Epoch: 564 [800/869 (91%)]\tLoss: 137117472.000000\n",
      "\n",
      "Test set: Avg. loss: 7572371.4627, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 565 [0/869 (0%)]\tLoss: 89804560.000000\n",
      "Train Epoch: 565 [160/869 (18%)]\tLoss: 53267248.000000\n",
      "Train Epoch: 565 [320/869 (36%)]\tLoss: 102061504.000000\n",
      "Train Epoch: 565 [480/869 (55%)]\tLoss: 84947424.000000\n",
      "Train Epoch: 565 [640/869 (73%)]\tLoss: 298748448.000000\n",
      "Train Epoch: 565 [800/869 (91%)]\tLoss: 215427024.000000\n",
      "\n",
      "Test set: Avg. loss: 7520154.4563, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 566 [0/869 (0%)]\tLoss: 138309936.000000\n",
      "Train Epoch: 566 [160/869 (18%)]\tLoss: 93202968.000000\n",
      "Train Epoch: 566 [320/869 (36%)]\tLoss: 123512808.000000\n",
      "Train Epoch: 566 [480/869 (55%)]\tLoss: 147284736.000000\n",
      "Train Epoch: 566 [640/869 (73%)]\tLoss: 252011264.000000\n",
      "Train Epoch: 566 [800/869 (91%)]\tLoss: 180692592.000000\n",
      "\n",
      "Test set: Avg. loss: 7432945.0746, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 567 [0/869 (0%)]\tLoss: 41928880.000000\n",
      "Train Epoch: 567 [160/869 (18%)]\tLoss: 202524096.000000\n",
      "Train Epoch: 567 [320/869 (36%)]\tLoss: 234694944.000000\n",
      "Train Epoch: 567 [480/869 (55%)]\tLoss: 131850144.000000\n",
      "Train Epoch: 567 [640/869 (73%)]\tLoss: 154968032.000000\n",
      "Train Epoch: 567 [800/869 (91%)]\tLoss: 140649840.000000\n",
      "\n",
      "Test set: Avg. loss: 7673855.0618, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 568 [0/869 (0%)]\tLoss: 86399552.000000\n",
      "Train Epoch: 568 [160/869 (18%)]\tLoss: 83707184.000000\n",
      "Train Epoch: 568 [320/869 (36%)]\tLoss: 234231648.000000\n",
      "Train Epoch: 568 [480/869 (55%)]\tLoss: 126155600.000000\n",
      "Train Epoch: 568 [640/869 (73%)]\tLoss: 135467472.000000\n",
      "Train Epoch: 568 [800/869 (91%)]\tLoss: 96476584.000000\n",
      "\n",
      "Test set: Avg. loss: 7808792.8614, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 569 [0/869 (0%)]\tLoss: 164046448.000000\n",
      "Train Epoch: 569 [160/869 (18%)]\tLoss: 208468832.000000\n",
      "Train Epoch: 569 [320/869 (36%)]\tLoss: 96577984.000000\n",
      "Train Epoch: 569 [480/869 (55%)]\tLoss: 228721440.000000\n",
      "Train Epoch: 569 [640/869 (73%)]\tLoss: 37501180.000000\n",
      "Train Epoch: 569 [800/869 (91%)]\tLoss: 94382072.000000\n",
      "\n",
      "Test set: Avg. loss: 7498318.3198, Accuracy: 0/469 (0%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 570 [0/869 (0%)]\tLoss: 173697312.000000\n",
      "Train Epoch: 570 [160/869 (18%)]\tLoss: 102593864.000000\n",
      "Train Epoch: 570 [320/869 (36%)]\tLoss: 284248768.000000\n",
      "Train Epoch: 570 [480/869 (55%)]\tLoss: 172041408.000000\n",
      "Train Epoch: 570 [640/869 (73%)]\tLoss: 182116512.000000\n",
      "Train Epoch: 570 [800/869 (91%)]\tLoss: 105206488.000000\n",
      "\n",
      "Test set: Avg. loss: 7678606.3625, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 571 [0/869 (0%)]\tLoss: 237550176.000000\n",
      "Train Epoch: 571 [160/869 (18%)]\tLoss: 75710832.000000\n",
      "Train Epoch: 571 [320/869 (36%)]\tLoss: 136472176.000000\n",
      "Train Epoch: 571 [480/869 (55%)]\tLoss: 89284672.000000\n",
      "Train Epoch: 571 [640/869 (73%)]\tLoss: 84313640.000000\n",
      "Train Epoch: 571 [800/869 (91%)]\tLoss: 45219740.000000\n",
      "\n",
      "Test set: Avg. loss: 7394151.4286, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 572 [0/869 (0%)]\tLoss: 100228696.000000\n",
      "Train Epoch: 572 [160/869 (18%)]\tLoss: 93640064.000000\n",
      "Train Epoch: 572 [320/869 (36%)]\tLoss: 134396096.000000\n",
      "Train Epoch: 572 [480/869 (55%)]\tLoss: 146485136.000000\n",
      "Train Epoch: 572 [640/869 (73%)]\tLoss: 47293584.000000\n",
      "Train Epoch: 572 [800/869 (91%)]\tLoss: 161344672.000000\n",
      "\n",
      "Test set: Avg. loss: 7534207.4286, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 573 [0/869 (0%)]\tLoss: 92818992.000000\n",
      "Train Epoch: 573 [160/869 (18%)]\tLoss: 90029800.000000\n",
      "Train Epoch: 573 [320/869 (36%)]\tLoss: 103155488.000000\n",
      "Train Epoch: 573 [480/869 (55%)]\tLoss: 61656512.000000\n",
      "Train Epoch: 573 [640/869 (73%)]\tLoss: 120092072.000000\n",
      "Train Epoch: 573 [800/869 (91%)]\tLoss: 78022008.000000\n",
      "\n",
      "Test set: Avg. loss: 7442297.4499, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 574 [0/869 (0%)]\tLoss: 169425728.000000\n",
      "Train Epoch: 574 [160/869 (18%)]\tLoss: 157109328.000000\n",
      "Train Epoch: 574 [320/869 (36%)]\tLoss: 177291504.000000\n",
      "Train Epoch: 574 [480/869 (55%)]\tLoss: 71096464.000000\n",
      "Train Epoch: 574 [640/869 (73%)]\tLoss: 128953408.000000\n",
      "Train Epoch: 574 [800/869 (91%)]\tLoss: 105285344.000000\n",
      "\n",
      "Test set: Avg. loss: 7524488.3923, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 575 [0/869 (0%)]\tLoss: 86857104.000000\n",
      "Train Epoch: 575 [160/869 (18%)]\tLoss: 83251928.000000\n",
      "Train Epoch: 575 [320/869 (36%)]\tLoss: 131460576.000000\n",
      "Train Epoch: 575 [480/869 (55%)]\tLoss: 140515792.000000\n",
      "Train Epoch: 575 [640/869 (73%)]\tLoss: 100972880.000000\n",
      "Train Epoch: 575 [800/869 (91%)]\tLoss: 61412144.000000\n",
      "\n",
      "Test set: Avg. loss: 7679595.4968, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 576 [0/869 (0%)]\tLoss: 202617344.000000\n",
      "Train Epoch: 576 [160/869 (18%)]\tLoss: 157512432.000000\n",
      "Train Epoch: 576 [320/869 (36%)]\tLoss: 64340944.000000\n",
      "Train Epoch: 576 [480/869 (55%)]\tLoss: 103081240.000000\n",
      "Train Epoch: 576 [640/869 (73%)]\tLoss: 58162300.000000\n",
      "Train Epoch: 576 [800/869 (91%)]\tLoss: 88273712.000000\n",
      "\n",
      "Test set: Avg. loss: 7346177.3305, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 577 [0/869 (0%)]\tLoss: 39167808.000000\n",
      "Train Epoch: 577 [160/869 (18%)]\tLoss: 195787312.000000\n",
      "Train Epoch: 577 [320/869 (36%)]\tLoss: 161456800.000000\n",
      "Train Epoch: 577 [480/869 (55%)]\tLoss: 191451680.000000\n",
      "Train Epoch: 577 [640/869 (73%)]\tLoss: 129257264.000000\n",
      "Train Epoch: 577 [800/869 (91%)]\tLoss: 123968520.000000\n",
      "\n",
      "Test set: Avg. loss: 7383072.4094, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 578 [0/869 (0%)]\tLoss: 133729456.000000\n",
      "Train Epoch: 578 [160/869 (18%)]\tLoss: 34417448.000000\n",
      "Train Epoch: 578 [320/869 (36%)]\tLoss: 44628020.000000\n",
      "Train Epoch: 578 [480/869 (55%)]\tLoss: 100477296.000000\n",
      "Train Epoch: 578 [640/869 (73%)]\tLoss: 152166976.000000\n",
      "Train Epoch: 578 [800/869 (91%)]\tLoss: 233206848.000000\n",
      "\n",
      "Test set: Avg. loss: 7865921.8849, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 579 [0/869 (0%)]\tLoss: 331959872.000000\n",
      "Train Epoch: 579 [160/869 (18%)]\tLoss: 60157280.000000\n",
      "Train Epoch: 579 [320/869 (36%)]\tLoss: 140907040.000000\n",
      "Train Epoch: 579 [480/869 (55%)]\tLoss: 196413056.000000\n",
      "Train Epoch: 579 [640/869 (73%)]\tLoss: 96829424.000000\n",
      "Train Epoch: 579 [800/869 (91%)]\tLoss: 105261544.000000\n",
      "\n",
      "Test set: Avg. loss: 7992738.2004, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 580 [0/869 (0%)]\tLoss: 45768004.000000\n",
      "Train Epoch: 580 [160/869 (18%)]\tLoss: 101834736.000000\n",
      "Train Epoch: 580 [320/869 (36%)]\tLoss: 91732848.000000\n",
      "Train Epoch: 580 [480/869 (55%)]\tLoss: 88163600.000000\n",
      "Train Epoch: 580 [640/869 (73%)]\tLoss: 189302336.000000\n",
      "Train Epoch: 580 [800/869 (91%)]\tLoss: 98947320.000000\n",
      "\n",
      "Test set: Avg. loss: 7797660.0000, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 581 [0/869 (0%)]\tLoss: 32654710.000000\n",
      "Train Epoch: 581 [160/869 (18%)]\tLoss: 94241368.000000\n",
      "Train Epoch: 581 [320/869 (36%)]\tLoss: 154215856.000000\n",
      "Train Epoch: 581 [480/869 (55%)]\tLoss: 97068384.000000\n",
      "Train Epoch: 581 [640/869 (73%)]\tLoss: 78689992.000000\n",
      "Train Epoch: 581 [800/869 (91%)]\tLoss: 114485464.000000\n",
      "\n",
      "Test set: Avg. loss: 7450931.3177, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 582 [0/869 (0%)]\tLoss: 165393584.000000\n",
      "Train Epoch: 582 [160/869 (18%)]\tLoss: 123654632.000000\n",
      "Train Epoch: 582 [320/869 (36%)]\tLoss: 153583424.000000\n",
      "Train Epoch: 582 [480/869 (55%)]\tLoss: 56766520.000000\n",
      "Train Epoch: 582 [640/869 (73%)]\tLoss: 203991056.000000\n",
      "Train Epoch: 582 [800/869 (91%)]\tLoss: 97554408.000000\n",
      "\n",
      "Test set: Avg. loss: 7629083.3177, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 583 [0/869 (0%)]\tLoss: 137905792.000000\n",
      "Train Epoch: 583 [160/869 (18%)]\tLoss: 84305688.000000\n",
      "Train Epoch: 583 [320/869 (36%)]\tLoss: 88514488.000000\n",
      "Train Epoch: 583 [480/869 (55%)]\tLoss: 173131104.000000\n",
      "Train Epoch: 583 [640/869 (73%)]\tLoss: 142839328.000000\n",
      "Train Epoch: 583 [800/869 (91%)]\tLoss: 185499328.000000\n",
      "\n",
      "Test set: Avg. loss: 7594069.3817, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 584 [0/869 (0%)]\tLoss: 94285632.000000\n",
      "Train Epoch: 584 [160/869 (18%)]\tLoss: 166671072.000000\n",
      "Train Epoch: 584 [320/869 (36%)]\tLoss: 140072512.000000\n",
      "Train Epoch: 584 [480/869 (55%)]\tLoss: 27846418.000000\n",
      "Train Epoch: 584 [640/869 (73%)]\tLoss: 45764740.000000\n",
      "Train Epoch: 584 [800/869 (91%)]\tLoss: 145392192.000000\n",
      "\n",
      "Test set: Avg. loss: 7396914.7548, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 585 [0/869 (0%)]\tLoss: 236191408.000000\n",
      "Train Epoch: 585 [160/869 (18%)]\tLoss: 84432568.000000\n",
      "Train Epoch: 585 [320/869 (36%)]\tLoss: 40984508.000000\n",
      "Train Epoch: 585 [480/869 (55%)]\tLoss: 148277936.000000\n",
      "Train Epoch: 585 [640/869 (73%)]\tLoss: 179724384.000000\n",
      "Train Epoch: 585 [800/869 (91%)]\tLoss: 90669488.000000\n",
      "\n",
      "Test set: Avg. loss: 7570224.4435, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 586 [0/869 (0%)]\tLoss: 93638128.000000\n",
      "Train Epoch: 586 [160/869 (18%)]\tLoss: 121337440.000000\n",
      "Train Epoch: 586 [320/869 (36%)]\tLoss: 54329984.000000\n",
      "Train Epoch: 586 [480/869 (55%)]\tLoss: 109834960.000000\n",
      "Train Epoch: 586 [640/869 (73%)]\tLoss: 194947312.000000\n",
      "Train Epoch: 586 [800/869 (91%)]\tLoss: 120504536.000000\n",
      "\n",
      "Test set: Avg. loss: 7476424.1791, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 587 [0/869 (0%)]\tLoss: 233131376.000000\n",
      "Train Epoch: 587 [160/869 (18%)]\tLoss: 86622112.000000\n",
      "Train Epoch: 587 [320/869 (36%)]\tLoss: 159174944.000000\n",
      "Train Epoch: 587 [480/869 (55%)]\tLoss: 45017924.000000\n",
      "Train Epoch: 587 [640/869 (73%)]\tLoss: 116230152.000000\n",
      "Train Epoch: 587 [800/869 (91%)]\tLoss: 64298840.000000\n",
      "\n",
      "Test set: Avg. loss: 7435556.5544, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 588 [0/869 (0%)]\tLoss: 55912344.000000\n",
      "Train Epoch: 588 [160/869 (18%)]\tLoss: 83988528.000000\n",
      "Train Epoch: 588 [320/869 (36%)]\tLoss: 128822384.000000\n",
      "Train Epoch: 588 [480/869 (55%)]\tLoss: 204498320.000000\n",
      "Train Epoch: 588 [640/869 (73%)]\tLoss: 101385584.000000\n",
      "Train Epoch: 588 [800/869 (91%)]\tLoss: 22189922.000000\n",
      "\n",
      "Test set: Avg. loss: 7343109.5267, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 589 [0/869 (0%)]\tLoss: 107795512.000000\n",
      "Train Epoch: 589 [160/869 (18%)]\tLoss: 179560624.000000\n",
      "Train Epoch: 589 [320/869 (36%)]\tLoss: 35782708.000000\n",
      "Train Epoch: 589 [480/869 (55%)]\tLoss: 149079008.000000\n",
      "Train Epoch: 589 [640/869 (73%)]\tLoss: 197180800.000000\n",
      "Train Epoch: 589 [800/869 (91%)]\tLoss: 84606408.000000\n",
      "\n",
      "Test set: Avg. loss: 7390395.1386, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 590 [0/869 (0%)]\tLoss: 93698336.000000\n",
      "Train Epoch: 590 [160/869 (18%)]\tLoss: 130200248.000000\n",
      "Train Epoch: 590 [320/869 (36%)]\tLoss: 213568848.000000\n",
      "Train Epoch: 590 [480/869 (55%)]\tLoss: 184437648.000000\n",
      "Train Epoch: 590 [640/869 (73%)]\tLoss: 196466464.000000\n",
      "Train Epoch: 590 [800/869 (91%)]\tLoss: 124987320.000000\n",
      "\n",
      "Test set: Avg. loss: 7499095.9147, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 591 [0/869 (0%)]\tLoss: 405239712.000000\n",
      "Train Epoch: 591 [160/869 (18%)]\tLoss: 182004960.000000\n",
      "Train Epoch: 591 [320/869 (36%)]\tLoss: 139876272.000000\n",
      "Train Epoch: 591 [480/869 (55%)]\tLoss: 50861020.000000\n",
      "Train Epoch: 591 [640/869 (73%)]\tLoss: 146720800.000000\n",
      "Train Epoch: 591 [800/869 (91%)]\tLoss: 83615048.000000\n",
      "\n",
      "Test set: Avg. loss: 7418653.6461, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 592 [0/869 (0%)]\tLoss: 39851156.000000\n",
      "Train Epoch: 592 [160/869 (18%)]\tLoss: 189054560.000000\n",
      "Train Epoch: 592 [320/869 (36%)]\tLoss: 251626432.000000\n",
      "Train Epoch: 592 [480/869 (55%)]\tLoss: 76159272.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 592 [640/869 (73%)]\tLoss: 25758286.000000\n",
      "Train Epoch: 592 [800/869 (91%)]\tLoss: 66769556.000000\n",
      "\n",
      "Test set: Avg. loss: 7330362.3582, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 593 [0/869 (0%)]\tLoss: 96828704.000000\n",
      "Train Epoch: 593 [160/869 (18%)]\tLoss: 41697888.000000\n",
      "Train Epoch: 593 [320/869 (36%)]\tLoss: 104245656.000000\n",
      "Train Epoch: 593 [480/869 (55%)]\tLoss: 141963808.000000\n",
      "Train Epoch: 593 [640/869 (73%)]\tLoss: 65998016.000000\n",
      "Train Epoch: 593 [800/869 (91%)]\tLoss: 138261744.000000\n",
      "\n",
      "Test set: Avg. loss: 7588357.5352, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 594 [0/869 (0%)]\tLoss: 35561780.000000\n",
      "Train Epoch: 594 [160/869 (18%)]\tLoss: 170025312.000000\n",
      "Train Epoch: 594 [320/869 (36%)]\tLoss: 144734272.000000\n",
      "Train Epoch: 594 [480/869 (55%)]\tLoss: 104105800.000000\n",
      "Train Epoch: 594 [640/869 (73%)]\tLoss: 49539964.000000\n",
      "Train Epoch: 594 [800/869 (91%)]\tLoss: 106334640.000000\n",
      "\n",
      "Test set: Avg. loss: 7647556.0597, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 595 [0/869 (0%)]\tLoss: 51395940.000000\n",
      "Train Epoch: 595 [160/869 (18%)]\tLoss: 132158792.000000\n",
      "Train Epoch: 595 [320/869 (36%)]\tLoss: 141703728.000000\n",
      "Train Epoch: 595 [480/869 (55%)]\tLoss: 74457392.000000\n",
      "Train Epoch: 595 [640/869 (73%)]\tLoss: 92210096.000000\n",
      "Train Epoch: 595 [800/869 (91%)]\tLoss: 42852016.000000\n",
      "\n",
      "Test set: Avg. loss: 7659695.4115, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 596 [0/869 (0%)]\tLoss: 203300480.000000\n",
      "Train Epoch: 596 [160/869 (18%)]\tLoss: 180035856.000000\n",
      "Train Epoch: 596 [320/869 (36%)]\tLoss: 87203392.000000\n",
      "Train Epoch: 596 [480/869 (55%)]\tLoss: 101123760.000000\n",
      "Train Epoch: 596 [640/869 (73%)]\tLoss: 82906904.000000\n",
      "Train Epoch: 596 [800/869 (91%)]\tLoss: 97188752.000000\n",
      "\n",
      "Test set: Avg. loss: 7738521.6034, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 597 [0/869 (0%)]\tLoss: 286541408.000000\n",
      "Train Epoch: 597 [160/869 (18%)]\tLoss: 89222216.000000\n",
      "Train Epoch: 597 [320/869 (36%)]\tLoss: 179884112.000000\n",
      "Train Epoch: 597 [480/869 (55%)]\tLoss: 90096616.000000\n",
      "Train Epoch: 597 [640/869 (73%)]\tLoss: 167347808.000000\n",
      "Train Epoch: 597 [800/869 (91%)]\tLoss: 152205744.000000\n",
      "\n",
      "Test set: Avg. loss: 7354720.7335, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 598 [0/869 (0%)]\tLoss: 105105376.000000\n",
      "Train Epoch: 598 [160/869 (18%)]\tLoss: 104269960.000000\n",
      "Train Epoch: 598 [320/869 (36%)]\tLoss: 253750704.000000\n",
      "Train Epoch: 598 [480/869 (55%)]\tLoss: 67390472.000000\n",
      "Train Epoch: 598 [640/869 (73%)]\tLoss: 118903192.000000\n",
      "Train Epoch: 598 [800/869 (91%)]\tLoss: 88934808.000000\n",
      "\n",
      "Test set: Avg. loss: 7438890.0128, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 599 [0/869 (0%)]\tLoss: 83688728.000000\n",
      "Train Epoch: 599 [160/869 (18%)]\tLoss: 50450896.000000\n",
      "Train Epoch: 599 [320/869 (36%)]\tLoss: 248550736.000000\n",
      "Train Epoch: 599 [480/869 (55%)]\tLoss: 117369048.000000\n",
      "Train Epoch: 599 [640/869 (73%)]\tLoss: 133187824.000000\n",
      "Train Epoch: 599 [800/869 (91%)]\tLoss: 175373680.000000\n",
      "\n",
      "Test set: Avg. loss: 7329100.2047, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 600 [0/869 (0%)]\tLoss: 143273296.000000\n",
      "Train Epoch: 600 [160/869 (18%)]\tLoss: 65734616.000000\n",
      "Train Epoch: 600 [320/869 (36%)]\tLoss: 120766464.000000\n",
      "Train Epoch: 600 [480/869 (55%)]\tLoss: 183408064.000000\n",
      "Train Epoch: 600 [640/869 (73%)]\tLoss: 219578032.000000\n",
      "Train Epoch: 600 [800/869 (91%)]\tLoss: 78664888.000000\n",
      "\n",
      "Test set: Avg. loss: 7501922.0469, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 601 [0/869 (0%)]\tLoss: 49435904.000000\n",
      "Train Epoch: 601 [160/869 (18%)]\tLoss: 87761664.000000\n",
      "Train Epoch: 601 [320/869 (36%)]\tLoss: 185196096.000000\n",
      "Train Epoch: 601 [480/869 (55%)]\tLoss: 53579552.000000\n",
      "Train Epoch: 601 [640/869 (73%)]\tLoss: 106975056.000000\n",
      "Train Epoch: 601 [800/869 (91%)]\tLoss: 151234656.000000\n",
      "\n",
      "Test set: Avg. loss: 7332244.8273, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 602 [0/869 (0%)]\tLoss: 137234528.000000\n",
      "Train Epoch: 602 [160/869 (18%)]\tLoss: 59306656.000000\n",
      "Train Epoch: 602 [320/869 (36%)]\tLoss: 208090336.000000\n",
      "Train Epoch: 602 [480/869 (55%)]\tLoss: 106322152.000000\n",
      "Train Epoch: 602 [640/869 (73%)]\tLoss: 150516384.000000\n",
      "Train Epoch: 602 [800/869 (91%)]\tLoss: 145438560.000000\n",
      "\n",
      "Test set: Avg. loss: 7404196.9893, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 603 [0/869 (0%)]\tLoss: 173181152.000000\n",
      "Train Epoch: 603 [160/869 (18%)]\tLoss: 56028608.000000\n",
      "Train Epoch: 603 [320/869 (36%)]\tLoss: 79131312.000000\n",
      "Train Epoch: 603 [480/869 (55%)]\tLoss: 92792744.000000\n",
      "Train Epoch: 603 [640/869 (73%)]\tLoss: 206925264.000000\n",
      "Train Epoch: 603 [800/869 (91%)]\tLoss: 105448008.000000\n",
      "\n",
      "Test set: Avg. loss: 7306610.0384, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 604 [0/869 (0%)]\tLoss: 198051264.000000\n",
      "Train Epoch: 604 [160/869 (18%)]\tLoss: 20710472.000000\n",
      "Train Epoch: 604 [320/869 (36%)]\tLoss: 81268720.000000\n",
      "Train Epoch: 604 [480/869 (55%)]\tLoss: 59243200.000000\n",
      "Train Epoch: 604 [640/869 (73%)]\tLoss: 117566672.000000\n",
      "Train Epoch: 604 [800/869 (91%)]\tLoss: 99316832.000000\n",
      "\n",
      "Test set: Avg. loss: 7391787.0192, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 605 [0/869 (0%)]\tLoss: 61149272.000000\n",
      "Train Epoch: 605 [160/869 (18%)]\tLoss: 66775276.000000\n",
      "Train Epoch: 605 [320/869 (36%)]\tLoss: 135955616.000000\n",
      "Train Epoch: 605 [480/869 (55%)]\tLoss: 101586056.000000\n",
      "Train Epoch: 605 [640/869 (73%)]\tLoss: 89550776.000000\n",
      "Train Epoch: 605 [800/869 (91%)]\tLoss: 170134720.000000\n",
      "\n",
      "Test set: Avg. loss: 7363234.8998, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 606 [0/869 (0%)]\tLoss: 208317952.000000\n",
      "Train Epoch: 606 [160/869 (18%)]\tLoss: 46688240.000000\n",
      "Train Epoch: 606 [320/869 (36%)]\tLoss: 157575040.000000\n",
      "Train Epoch: 606 [480/869 (55%)]\tLoss: 64033016.000000\n",
      "Train Epoch: 606 [640/869 (73%)]\tLoss: 170002000.000000\n",
      "Train Epoch: 606 [800/869 (91%)]\tLoss: 129623248.000000\n",
      "\n",
      "Test set: Avg. loss: 7482838.5501, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 607 [0/869 (0%)]\tLoss: 89935480.000000\n",
      "Train Epoch: 607 [160/869 (18%)]\tLoss: 163894752.000000\n",
      "Train Epoch: 607 [320/869 (36%)]\tLoss: 135904816.000000\n",
      "Train Epoch: 607 [480/869 (55%)]\tLoss: 332670496.000000\n",
      "Train Epoch: 607 [640/869 (73%)]\tLoss: 131274536.000000\n",
      "Train Epoch: 607 [800/869 (91%)]\tLoss: 122703648.000000\n",
      "\n",
      "Test set: Avg. loss: 8095177.8209, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 608 [0/869 (0%)]\tLoss: 31106254.000000\n",
      "Train Epoch: 608 [160/869 (18%)]\tLoss: 84748600.000000\n",
      "Train Epoch: 608 [320/869 (36%)]\tLoss: 84186672.000000\n",
      "Train Epoch: 608 [480/869 (55%)]\tLoss: 155332832.000000\n",
      "Train Epoch: 608 [640/869 (73%)]\tLoss: 38144952.000000\n",
      "Train Epoch: 608 [800/869 (91%)]\tLoss: 46605508.000000\n",
      "\n",
      "Test set: Avg. loss: 8753370.1493, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 609 [0/869 (0%)]\tLoss: 135851248.000000\n",
      "Train Epoch: 609 [160/869 (18%)]\tLoss: 130014488.000000\n",
      "Train Epoch: 609 [320/869 (36%)]\tLoss: 83940904.000000\n",
      "Train Epoch: 609 [480/869 (55%)]\tLoss: 81823624.000000\n",
      "Train Epoch: 609 [640/869 (73%)]\tLoss: 131379896.000000\n",
      "Train Epoch: 609 [800/869 (91%)]\tLoss: 153345856.000000\n",
      "\n",
      "Test set: Avg. loss: 7988519.6162, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 610 [0/869 (0%)]\tLoss: 109015744.000000\n",
      "Train Epoch: 610 [160/869 (18%)]\tLoss: 179543664.000000\n",
      "Train Epoch: 610 [320/869 (36%)]\tLoss: 80717416.000000\n",
      "Train Epoch: 610 [480/869 (55%)]\tLoss: 46004304.000000\n",
      "Train Epoch: 610 [640/869 (73%)]\tLoss: 97063736.000000\n",
      "Train Epoch: 610 [800/869 (91%)]\tLoss: 185417408.000000\n",
      "\n",
      "Test set: Avg. loss: 7452893.8166, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 611 [0/869 (0%)]\tLoss: 249808624.000000\n",
      "Train Epoch: 611 [160/869 (18%)]\tLoss: 159451408.000000\n",
      "Train Epoch: 611 [320/869 (36%)]\tLoss: 38674440.000000\n",
      "Train Epoch: 611 [480/869 (55%)]\tLoss: 55432472.000000\n",
      "Train Epoch: 611 [640/869 (73%)]\tLoss: 224874912.000000\n",
      "Train Epoch: 611 [800/869 (91%)]\tLoss: 47247680.000000\n",
      "\n",
      "Test set: Avg. loss: 7835670.6866, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 612 [0/869 (0%)]\tLoss: 69148600.000000\n",
      "Train Epoch: 612 [160/869 (18%)]\tLoss: 19060550.000000\n",
      "Train Epoch: 612 [320/869 (36%)]\tLoss: 163660400.000000\n",
      "Train Epoch: 612 [480/869 (55%)]\tLoss: 114870264.000000\n",
      "Train Epoch: 612 [640/869 (73%)]\tLoss: 151727616.000000\n",
      "Train Epoch: 612 [800/869 (91%)]\tLoss: 136942688.000000\n",
      "\n",
      "Test set: Avg. loss: 7499132.3838, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 613 [0/869 (0%)]\tLoss: 185284944.000000\n",
      "Train Epoch: 613 [160/869 (18%)]\tLoss: 185891824.000000\n",
      "Train Epoch: 613 [320/869 (36%)]\tLoss: 162348272.000000\n",
      "Train Epoch: 613 [480/869 (55%)]\tLoss: 77649200.000000\n",
      "Train Epoch: 613 [640/869 (73%)]\tLoss: 79067720.000000\n",
      "Train Epoch: 613 [800/869 (91%)]\tLoss: 186515168.000000\n",
      "\n",
      "Test set: Avg. loss: 7456348.7249, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 614 [0/869 (0%)]\tLoss: 142140752.000000\n",
      "Train Epoch: 614 [160/869 (18%)]\tLoss: 140097808.000000\n",
      "Train Epoch: 614 [320/869 (36%)]\tLoss: 125015208.000000\n",
      "Train Epoch: 614 [480/869 (55%)]\tLoss: 129148032.000000\n",
      "Train Epoch: 614 [640/869 (73%)]\tLoss: 129950216.000000\n",
      "Train Epoch: 614 [800/869 (91%)]\tLoss: 160697328.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 7646146.0810, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 615 [0/869 (0%)]\tLoss: 138311168.000000\n",
      "Train Epoch: 615 [160/869 (18%)]\tLoss: 61314512.000000\n",
      "Train Epoch: 615 [320/869 (36%)]\tLoss: 137300592.000000\n",
      "Train Epoch: 615 [480/869 (55%)]\tLoss: 25180298.000000\n",
      "Train Epoch: 615 [640/869 (73%)]\tLoss: 127958688.000000\n",
      "Train Epoch: 615 [800/869 (91%)]\tLoss: 80053704.000000\n",
      "\n",
      "Test set: Avg. loss: 7468133.5181, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 616 [0/869 (0%)]\tLoss: 128887032.000000\n",
      "Train Epoch: 616 [160/869 (18%)]\tLoss: 147616096.000000\n",
      "Train Epoch: 616 [320/869 (36%)]\tLoss: 55964292.000000\n",
      "Train Epoch: 616 [480/869 (55%)]\tLoss: 28094962.000000\n",
      "Train Epoch: 616 [640/869 (73%)]\tLoss: 117836472.000000\n",
      "Train Epoch: 616 [800/869 (91%)]\tLoss: 87546408.000000\n",
      "\n",
      "Test set: Avg. loss: 7843721.2281, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 617 [0/869 (0%)]\tLoss: 252380832.000000\n",
      "Train Epoch: 617 [160/869 (18%)]\tLoss: 45827472.000000\n",
      "Train Epoch: 617 [320/869 (36%)]\tLoss: 149783776.000000\n",
      "Train Epoch: 617 [480/869 (55%)]\tLoss: 153829072.000000\n",
      "Train Epoch: 617 [640/869 (73%)]\tLoss: 61440716.000000\n",
      "Train Epoch: 617 [800/869 (91%)]\tLoss: 147512512.000000\n",
      "\n",
      "Test set: Avg. loss: 7468985.1173, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 618 [0/869 (0%)]\tLoss: 117243344.000000\n",
      "Train Epoch: 618 [160/869 (18%)]\tLoss: 125859080.000000\n",
      "Train Epoch: 618 [320/869 (36%)]\tLoss: 143038176.000000\n",
      "Train Epoch: 618 [480/869 (55%)]\tLoss: 107050320.000000\n",
      "Train Epoch: 618 [640/869 (73%)]\tLoss: 117404736.000000\n",
      "Train Epoch: 618 [800/869 (91%)]\tLoss: 111702816.000000\n",
      "\n",
      "Test set: Avg. loss: 7748794.0128, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 619 [0/869 (0%)]\tLoss: 168566336.000000\n",
      "Train Epoch: 619 [160/869 (18%)]\tLoss: 115721432.000000\n",
      "Train Epoch: 619 [320/869 (36%)]\tLoss: 169391072.000000\n",
      "Train Epoch: 619 [480/869 (55%)]\tLoss: 98317656.000000\n",
      "Train Epoch: 619 [640/869 (73%)]\tLoss: 130176840.000000\n",
      "Train Epoch: 619 [800/869 (91%)]\tLoss: 31220094.000000\n",
      "\n",
      "Test set: Avg. loss: 7550589.3561, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 620 [0/869 (0%)]\tLoss: 118040848.000000\n",
      "Train Epoch: 620 [160/869 (18%)]\tLoss: 135337216.000000\n",
      "Train Epoch: 620 [320/869 (36%)]\tLoss: 168535776.000000\n",
      "Train Epoch: 620 [480/869 (55%)]\tLoss: 107032032.000000\n",
      "Train Epoch: 620 [640/869 (73%)]\tLoss: 92205720.000000\n",
      "Train Epoch: 620 [800/869 (91%)]\tLoss: 226563536.000000\n",
      "\n",
      "Test set: Avg. loss: 7859451.8465, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 621 [0/869 (0%)]\tLoss: 27825840.000000\n",
      "Train Epoch: 621 [160/869 (18%)]\tLoss: 164963664.000000\n",
      "Train Epoch: 621 [320/869 (36%)]\tLoss: 85107832.000000\n",
      "Train Epoch: 621 [480/869 (55%)]\tLoss: 76754096.000000\n",
      "Train Epoch: 621 [640/869 (73%)]\tLoss: 269499744.000000\n",
      "Train Epoch: 621 [800/869 (91%)]\tLoss: 46494564.000000\n",
      "\n",
      "Test set: Avg. loss: 7423887.4371, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 622 [0/869 (0%)]\tLoss: 156110336.000000\n",
      "Train Epoch: 622 [160/869 (18%)]\tLoss: 151975712.000000\n",
      "Train Epoch: 622 [320/869 (36%)]\tLoss: 109737160.000000\n",
      "Train Epoch: 622 [480/869 (55%)]\tLoss: 104712248.000000\n",
      "Train Epoch: 622 [640/869 (73%)]\tLoss: 100134072.000000\n",
      "Train Epoch: 622 [800/869 (91%)]\tLoss: 142778528.000000\n",
      "\n",
      "Test set: Avg. loss: 7591261.9360, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 623 [0/869 (0%)]\tLoss: 185829296.000000\n",
      "Train Epoch: 623 [160/869 (18%)]\tLoss: 223967776.000000\n",
      "Train Epoch: 623 [320/869 (36%)]\tLoss: 182841120.000000\n",
      "Train Epoch: 623 [480/869 (55%)]\tLoss: 43173784.000000\n",
      "Train Epoch: 623 [640/869 (73%)]\tLoss: 49198472.000000\n",
      "Train Epoch: 623 [800/869 (91%)]\tLoss: 36043412.000000\n",
      "\n",
      "Test set: Avg. loss: 7744181.7313, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 624 [0/869 (0%)]\tLoss: 75293744.000000\n",
      "Train Epoch: 624 [160/869 (18%)]\tLoss: 71333176.000000\n",
      "Train Epoch: 624 [320/869 (36%)]\tLoss: 198783136.000000\n",
      "Train Epoch: 624 [480/869 (55%)]\tLoss: 41068060.000000\n",
      "Train Epoch: 624 [640/869 (73%)]\tLoss: 138139040.000000\n",
      "Train Epoch: 624 [800/869 (91%)]\tLoss: 181674112.000000\n",
      "\n",
      "Test set: Avg. loss: 7333919.9915, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 625 [0/869 (0%)]\tLoss: 116670016.000000\n",
      "Train Epoch: 625 [160/869 (18%)]\tLoss: 59605028.000000\n",
      "Train Epoch: 625 [320/869 (36%)]\tLoss: 124048912.000000\n",
      "Train Epoch: 625 [480/869 (55%)]\tLoss: 89638968.000000\n",
      "Train Epoch: 625 [640/869 (73%)]\tLoss: 121443400.000000\n",
      "Train Epoch: 625 [800/869 (91%)]\tLoss: 52708376.000000\n",
      "\n",
      "Test set: Avg. loss: 7923699.0235, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 626 [0/869 (0%)]\tLoss: 170410800.000000\n",
      "Train Epoch: 626 [160/869 (18%)]\tLoss: 82087536.000000\n",
      "Train Epoch: 626 [320/869 (36%)]\tLoss: 43826716.000000\n",
      "Train Epoch: 626 [480/869 (55%)]\tLoss: 171823088.000000\n",
      "Train Epoch: 626 [640/869 (73%)]\tLoss: 144317120.000000\n",
      "Train Epoch: 626 [800/869 (91%)]\tLoss: 170613248.000000\n",
      "\n",
      "Test set: Avg. loss: 7720280.5714, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 627 [0/869 (0%)]\tLoss: 50783632.000000\n",
      "Train Epoch: 627 [160/869 (18%)]\tLoss: 32168712.000000\n",
      "Train Epoch: 627 [320/869 (36%)]\tLoss: 157529344.000000\n",
      "Train Epoch: 627 [480/869 (55%)]\tLoss: 94206904.000000\n",
      "Train Epoch: 627 [640/869 (73%)]\tLoss: 50574256.000000\n",
      "Train Epoch: 627 [800/869 (91%)]\tLoss: 49677148.000000\n",
      "\n",
      "Test set: Avg. loss: 7382220.0384, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 628 [0/869 (0%)]\tLoss: 109140264.000000\n",
      "Train Epoch: 628 [160/869 (18%)]\tLoss: 55910092.000000\n",
      "Train Epoch: 628 [320/869 (36%)]\tLoss: 200814352.000000\n",
      "Train Epoch: 628 [480/869 (55%)]\tLoss: 148138592.000000\n",
      "Train Epoch: 628 [640/869 (73%)]\tLoss: 102668400.000000\n",
      "Train Epoch: 628 [800/869 (91%)]\tLoss: 37744104.000000\n",
      "\n",
      "Test set: Avg. loss: 7546437.7910, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 629 [0/869 (0%)]\tLoss: 243575680.000000\n",
      "Train Epoch: 629 [160/869 (18%)]\tLoss: 214317456.000000\n",
      "Train Epoch: 629 [320/869 (36%)]\tLoss: 167814944.000000\n",
      "Train Epoch: 629 [480/869 (55%)]\tLoss: 89001064.000000\n",
      "Train Epoch: 629 [640/869 (73%)]\tLoss: 78692520.000000\n",
      "Train Epoch: 629 [800/869 (91%)]\tLoss: 101712472.000000\n",
      "\n",
      "Test set: Avg. loss: 7398222.8913, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 630 [0/869 (0%)]\tLoss: 114138720.000000\n",
      "Train Epoch: 630 [160/869 (18%)]\tLoss: 87812560.000000\n",
      "Train Epoch: 630 [320/869 (36%)]\tLoss: 106776160.000000\n",
      "Train Epoch: 630 [480/869 (55%)]\tLoss: 141895104.000000\n",
      "Train Epoch: 630 [640/869 (73%)]\tLoss: 95142400.000000\n",
      "Train Epoch: 630 [800/869 (91%)]\tLoss: 91604976.000000\n",
      "\n",
      "Test set: Avg. loss: 7724997.6119, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 631 [0/869 (0%)]\tLoss: 96634040.000000\n",
      "Train Epoch: 631 [160/869 (18%)]\tLoss: 135476896.000000\n",
      "Train Epoch: 631 [320/869 (36%)]\tLoss: 103392784.000000\n",
      "Train Epoch: 631 [480/869 (55%)]\tLoss: 215638704.000000\n",
      "Train Epoch: 631 [640/869 (73%)]\tLoss: 45736776.000000\n",
      "Train Epoch: 631 [800/869 (91%)]\tLoss: 239648688.000000\n",
      "\n",
      "Test set: Avg. loss: 7469721.5437, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 632 [0/869 (0%)]\tLoss: 88995600.000000\n",
      "Train Epoch: 632 [160/869 (18%)]\tLoss: 169908016.000000\n",
      "Train Epoch: 632 [320/869 (36%)]\tLoss: 108460088.000000\n",
      "Train Epoch: 632 [480/869 (55%)]\tLoss: 43364916.000000\n",
      "Train Epoch: 632 [640/869 (73%)]\tLoss: 98942200.000000\n",
      "Train Epoch: 632 [800/869 (91%)]\tLoss: 86482144.000000\n",
      "\n",
      "Test set: Avg. loss: 7312468.3241, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 633 [0/869 (0%)]\tLoss: 47250460.000000\n",
      "Train Epoch: 633 [160/869 (18%)]\tLoss: 92472144.000000\n",
      "Train Epoch: 633 [320/869 (36%)]\tLoss: 86610888.000000\n",
      "Train Epoch: 633 [480/869 (55%)]\tLoss: 35329720.000000\n",
      "Train Epoch: 633 [640/869 (73%)]\tLoss: 89510832.000000\n",
      "Train Epoch: 633 [800/869 (91%)]\tLoss: 76885992.000000\n",
      "\n",
      "Test set: Avg. loss: 7519085.1514, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 634 [0/869 (0%)]\tLoss: 118681520.000000\n",
      "Train Epoch: 634 [160/869 (18%)]\tLoss: 136316176.000000\n",
      "Train Epoch: 634 [320/869 (36%)]\tLoss: 94164032.000000\n",
      "Train Epoch: 634 [480/869 (55%)]\tLoss: 198115616.000000\n",
      "Train Epoch: 634 [640/869 (73%)]\tLoss: 75460528.000000\n",
      "Train Epoch: 634 [800/869 (91%)]\tLoss: 194156832.000000\n",
      "\n",
      "Test set: Avg. loss: 7398456.1876, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 635 [0/869 (0%)]\tLoss: 322304832.000000\n",
      "Train Epoch: 635 [160/869 (18%)]\tLoss: 178460144.000000\n",
      "Train Epoch: 635 [320/869 (36%)]\tLoss: 132797640.000000\n",
      "Train Epoch: 635 [480/869 (55%)]\tLoss: 201325472.000000\n",
      "Train Epoch: 635 [640/869 (73%)]\tLoss: 229451216.000000\n",
      "Train Epoch: 635 [800/869 (91%)]\tLoss: 152242784.000000\n",
      "\n",
      "Test set: Avg. loss: 7994248.0682, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 636 [0/869 (0%)]\tLoss: 112036312.000000\n",
      "Train Epoch: 636 [160/869 (18%)]\tLoss: 316789024.000000\n",
      "Train Epoch: 636 [320/869 (36%)]\tLoss: 156992896.000000\n",
      "Train Epoch: 636 [480/869 (55%)]\tLoss: 135148080.000000\n",
      "Train Epoch: 636 [640/869 (73%)]\tLoss: 84301360.000000\n",
      "Train Epoch: 636 [800/869 (91%)]\tLoss: 140939728.000000\n",
      "\n",
      "Test set: Avg. loss: 7399844.2985, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 637 [0/869 (0%)]\tLoss: 89215384.000000\n",
      "Train Epoch: 637 [160/869 (18%)]\tLoss: 30348294.000000\n",
      "Train Epoch: 637 [320/869 (36%)]\tLoss: 242681808.000000\n",
      "Train Epoch: 637 [480/869 (55%)]\tLoss: 45329144.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 637 [640/869 (73%)]\tLoss: 111505664.000000\n",
      "Train Epoch: 637 [800/869 (91%)]\tLoss: 61004712.000000\n",
      "\n",
      "Test set: Avg. loss: 7392028.2985, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 638 [0/869 (0%)]\tLoss: 44854772.000000\n",
      "Train Epoch: 638 [160/869 (18%)]\tLoss: 178304896.000000\n",
      "Train Epoch: 638 [320/869 (36%)]\tLoss: 55852464.000000\n",
      "Train Epoch: 638 [480/869 (55%)]\tLoss: 136165232.000000\n",
      "Train Epoch: 638 [640/869 (73%)]\tLoss: 77555672.000000\n",
      "Train Epoch: 638 [800/869 (91%)]\tLoss: 91860096.000000\n",
      "\n",
      "Test set: Avg. loss: 7575832.4179, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 639 [0/869 (0%)]\tLoss: 83034208.000000\n",
      "Train Epoch: 639 [160/869 (18%)]\tLoss: 86089688.000000\n",
      "Train Epoch: 639 [320/869 (36%)]\tLoss: 95568504.000000\n",
      "Train Epoch: 639 [480/869 (55%)]\tLoss: 158510784.000000\n",
      "Train Epoch: 639 [640/869 (73%)]\tLoss: 44111516.000000\n",
      "Train Epoch: 639 [800/869 (91%)]\tLoss: 122703320.000000\n",
      "\n",
      "Test set: Avg. loss: 7442749.5011, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 640 [0/869 (0%)]\tLoss: 81667168.000000\n",
      "Train Epoch: 640 [160/869 (18%)]\tLoss: 279435776.000000\n",
      "Train Epoch: 640 [320/869 (36%)]\tLoss: 97519368.000000\n",
      "Train Epoch: 640 [480/869 (55%)]\tLoss: 97910904.000000\n",
      "Train Epoch: 640 [640/869 (73%)]\tLoss: 86822576.000000\n",
      "Train Epoch: 640 [800/869 (91%)]\tLoss: 124853104.000000\n",
      "\n",
      "Test set: Avg. loss: 8277010.0469, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 641 [0/869 (0%)]\tLoss: 151994768.000000\n",
      "Train Epoch: 641 [160/869 (18%)]\tLoss: 85083488.000000\n",
      "Train Epoch: 641 [320/869 (36%)]\tLoss: 118031496.000000\n",
      "Train Epoch: 641 [480/869 (55%)]\tLoss: 112594936.000000\n",
      "Train Epoch: 641 [640/869 (73%)]\tLoss: 289703168.000000\n",
      "Train Epoch: 641 [800/869 (91%)]\tLoss: 73374768.000000\n",
      "\n",
      "Test set: Avg. loss: 7309055.9744, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 642 [0/869 (0%)]\tLoss: 92699120.000000\n",
      "Train Epoch: 642 [160/869 (18%)]\tLoss: 116397080.000000\n",
      "Train Epoch: 642 [320/869 (36%)]\tLoss: 182081488.000000\n",
      "Train Epoch: 642 [480/869 (55%)]\tLoss: 340095936.000000\n",
      "Train Epoch: 642 [640/869 (73%)]\tLoss: 83688192.000000\n",
      "Train Epoch: 642 [800/869 (91%)]\tLoss: 44513224.000000\n",
      "\n",
      "Test set: Avg. loss: 7579223.1471, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 643 [0/869 (0%)]\tLoss: 123594624.000000\n",
      "Train Epoch: 643 [160/869 (18%)]\tLoss: 35163244.000000\n",
      "Train Epoch: 643 [320/869 (36%)]\tLoss: 140530976.000000\n",
      "Train Epoch: 643 [480/869 (55%)]\tLoss: 88449432.000000\n",
      "Train Epoch: 643 [640/869 (73%)]\tLoss: 76851096.000000\n",
      "Train Epoch: 643 [800/869 (91%)]\tLoss: 168464288.000000\n",
      "\n",
      "Test set: Avg. loss: 7758424.6652, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 644 [0/869 (0%)]\tLoss: 71276680.000000\n",
      "Train Epoch: 644 [160/869 (18%)]\tLoss: 37548816.000000\n",
      "Train Epoch: 644 [320/869 (36%)]\tLoss: 123086896.000000\n",
      "Train Epoch: 644 [480/869 (55%)]\tLoss: 96051568.000000\n",
      "Train Epoch: 644 [640/869 (73%)]\tLoss: 67407776.000000\n",
      "Train Epoch: 644 [800/869 (91%)]\tLoss: 250734608.000000\n",
      "\n",
      "Test set: Avg. loss: 7333778.6439, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 645 [0/869 (0%)]\tLoss: 142082848.000000\n",
      "Train Epoch: 645 [160/869 (18%)]\tLoss: 89160440.000000\n",
      "Train Epoch: 645 [320/869 (36%)]\tLoss: 27190136.000000\n",
      "Train Epoch: 645 [480/869 (55%)]\tLoss: 130386784.000000\n",
      "Train Epoch: 645 [640/869 (73%)]\tLoss: 99656080.000000\n",
      "Train Epoch: 645 [800/869 (91%)]\tLoss: 110126336.000000\n",
      "\n",
      "Test set: Avg. loss: 7769797.8422, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 646 [0/869 (0%)]\tLoss: 192775888.000000\n",
      "Train Epoch: 646 [160/869 (18%)]\tLoss: 175297872.000000\n",
      "Train Epoch: 646 [320/869 (36%)]\tLoss: 141386784.000000\n",
      "Train Epoch: 646 [480/869 (55%)]\tLoss: 92498680.000000\n",
      "Train Epoch: 646 [640/869 (73%)]\tLoss: 97531528.000000\n",
      "Train Epoch: 646 [800/869 (91%)]\tLoss: 217795744.000000\n",
      "\n",
      "Test set: Avg. loss: 7605812.0256, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 647 [0/869 (0%)]\tLoss: 144508000.000000\n",
      "Train Epoch: 647 [160/869 (18%)]\tLoss: 130736464.000000\n",
      "Train Epoch: 647 [320/869 (36%)]\tLoss: 196974976.000000\n",
      "Train Epoch: 647 [480/869 (55%)]\tLoss: 129481872.000000\n",
      "Train Epoch: 647 [640/869 (73%)]\tLoss: 36475692.000000\n",
      "Train Epoch: 647 [800/869 (91%)]\tLoss: 93307272.000000\n",
      "\n",
      "Test set: Avg. loss: 7753591.4542, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 648 [0/869 (0%)]\tLoss: 254682096.000000\n",
      "Train Epoch: 648 [160/869 (18%)]\tLoss: 102353576.000000\n",
      "Train Epoch: 648 [320/869 (36%)]\tLoss: 99325520.000000\n",
      "Train Epoch: 648 [480/869 (55%)]\tLoss: 43989404.000000\n",
      "Train Epoch: 648 [640/869 (73%)]\tLoss: 55207800.000000\n",
      "Train Epoch: 648 [800/869 (91%)]\tLoss: 186820944.000000\n",
      "\n",
      "Test set: Avg. loss: 7822377.4840, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 649 [0/869 (0%)]\tLoss: 310284736.000000\n",
      "Train Epoch: 649 [160/869 (18%)]\tLoss: 133943072.000000\n",
      "Train Epoch: 649 [320/869 (36%)]\tLoss: 277645056.000000\n",
      "Train Epoch: 649 [480/869 (55%)]\tLoss: 25057112.000000\n",
      "Train Epoch: 649 [640/869 (73%)]\tLoss: 122249712.000000\n",
      "Train Epoch: 649 [800/869 (91%)]\tLoss: 98153480.000000\n",
      "\n",
      "Test set: Avg. loss: 7399703.8550, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 650 [0/869 (0%)]\tLoss: 86926176.000000\n",
      "Train Epoch: 650 [160/869 (18%)]\tLoss: 245463616.000000\n",
      "Train Epoch: 650 [320/869 (36%)]\tLoss: 40786448.000000\n",
      "Train Epoch: 650 [480/869 (55%)]\tLoss: 167454336.000000\n",
      "Train Epoch: 650 [640/869 (73%)]\tLoss: 137709152.000000\n",
      "Train Epoch: 650 [800/869 (91%)]\tLoss: 387286016.000000\n",
      "\n",
      "Test set: Avg. loss: 7420908.5970, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 651 [0/869 (0%)]\tLoss: 180913168.000000\n",
      "Train Epoch: 651 [160/869 (18%)]\tLoss: 88422664.000000\n",
      "Train Epoch: 651 [320/869 (36%)]\tLoss: 260315120.000000\n",
      "Train Epoch: 651 [480/869 (55%)]\tLoss: 55277228.000000\n",
      "Train Epoch: 651 [640/869 (73%)]\tLoss: 132916160.000000\n",
      "Train Epoch: 651 [800/869 (91%)]\tLoss: 69481184.000000\n",
      "\n",
      "Test set: Avg. loss: 8221327.8337, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 652 [0/869 (0%)]\tLoss: 109628272.000000\n",
      "Train Epoch: 652 [160/869 (18%)]\tLoss: 121681696.000000\n",
      "Train Epoch: 652 [320/869 (36%)]\tLoss: 128689904.000000\n",
      "Train Epoch: 652 [480/869 (55%)]\tLoss: 102549416.000000\n",
      "Train Epoch: 652 [640/869 (73%)]\tLoss: 244052672.000000\n",
      "Train Epoch: 652 [800/869 (91%)]\tLoss: 166507168.000000\n",
      "\n",
      "Test set: Avg. loss: 7693168.3070, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 653 [0/869 (0%)]\tLoss: 261527600.000000\n",
      "Train Epoch: 653 [160/869 (18%)]\tLoss: 104057080.000000\n",
      "Train Epoch: 653 [320/869 (36%)]\tLoss: 152770976.000000\n",
      "Train Epoch: 653 [480/869 (55%)]\tLoss: 36755360.000000\n",
      "Train Epoch: 653 [640/869 (73%)]\tLoss: 156785024.000000\n",
      "Train Epoch: 653 [800/869 (91%)]\tLoss: 98439408.000000\n",
      "\n",
      "Test set: Avg. loss: 7679902.3881, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 654 [0/869 (0%)]\tLoss: 76989024.000000\n",
      "Train Epoch: 654 [160/869 (18%)]\tLoss: 215088368.000000\n",
      "Train Epoch: 654 [320/869 (36%)]\tLoss: 328553088.000000\n",
      "Train Epoch: 654 [480/869 (55%)]\tLoss: 81529592.000000\n",
      "Train Epoch: 654 [640/869 (73%)]\tLoss: 164327200.000000\n",
      "Train Epoch: 654 [800/869 (91%)]\tLoss: 48881496.000000\n",
      "\n",
      "Test set: Avg. loss: 7399190.6610, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 655 [0/869 (0%)]\tLoss: 133541960.000000\n",
      "Train Epoch: 655 [160/869 (18%)]\tLoss: 162264352.000000\n",
      "Train Epoch: 655 [320/869 (36%)]\tLoss: 76517096.000000\n",
      "Train Epoch: 655 [480/869 (55%)]\tLoss: 199550960.000000\n",
      "Train Epoch: 655 [640/869 (73%)]\tLoss: 50300108.000000\n",
      "Train Epoch: 655 [800/869 (91%)]\tLoss: 81967640.000000\n",
      "\n",
      "Test set: Avg. loss: 8155075.2239, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 656 [0/869 (0%)]\tLoss: 186279216.000000\n",
      "Train Epoch: 656 [160/869 (18%)]\tLoss: 115948992.000000\n",
      "Train Epoch: 656 [320/869 (36%)]\tLoss: 44552324.000000\n",
      "Train Epoch: 656 [480/869 (55%)]\tLoss: 134832608.000000\n",
      "Train Epoch: 656 [640/869 (73%)]\tLoss: 127886632.000000\n",
      "Train Epoch: 656 [800/869 (91%)]\tLoss: 173856592.000000\n",
      "\n",
      "Test set: Avg. loss: 7406838.8742, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 657 [0/869 (0%)]\tLoss: 149669520.000000\n",
      "Train Epoch: 657 [160/869 (18%)]\tLoss: 53994988.000000\n",
      "Train Epoch: 657 [320/869 (36%)]\tLoss: 24107196.000000\n",
      "Train Epoch: 657 [480/869 (55%)]\tLoss: 155939584.000000\n",
      "Train Epoch: 657 [640/869 (73%)]\tLoss: 104351824.000000\n",
      "Train Epoch: 657 [800/869 (91%)]\tLoss: 63984748.000000\n",
      "\n",
      "Test set: Avg. loss: 7523331.2836, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 658 [0/869 (0%)]\tLoss: 127751280.000000\n",
      "Train Epoch: 658 [160/869 (18%)]\tLoss: 151026384.000000\n",
      "Train Epoch: 658 [320/869 (36%)]\tLoss: 123851008.000000\n",
      "Train Epoch: 658 [480/869 (55%)]\tLoss: 164905440.000000\n",
      "Train Epoch: 658 [640/869 (73%)]\tLoss: 58320420.000000\n",
      "Train Epoch: 658 [800/869 (91%)]\tLoss: 144587344.000000\n",
      "\n",
      "Test set: Avg. loss: 7482859.5991, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 659 [0/869 (0%)]\tLoss: 165016320.000000\n",
      "Train Epoch: 659 [160/869 (18%)]\tLoss: 215900448.000000\n",
      "Train Epoch: 659 [320/869 (36%)]\tLoss: 220942672.000000\n",
      "Train Epoch: 659 [480/869 (55%)]\tLoss: 56923512.000000\n",
      "Train Epoch: 659 [640/869 (73%)]\tLoss: 138211200.000000\n",
      "Train Epoch: 659 [800/869 (91%)]\tLoss: 201669472.000000\n",
      "\n",
      "Test set: Avg. loss: 7761044.6141, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 660 [0/869 (0%)]\tLoss: 132677024.000000\n",
      "Train Epoch: 660 [160/869 (18%)]\tLoss: 183310704.000000\n",
      "Train Epoch: 660 [320/869 (36%)]\tLoss: 49108432.000000\n",
      "Train Epoch: 660 [480/869 (55%)]\tLoss: 279111296.000000\n",
      "Train Epoch: 660 [640/869 (73%)]\tLoss: 69583632.000000\n",
      "Train Epoch: 660 [800/869 (91%)]\tLoss: 82950328.000000\n",
      "\n",
      "Test set: Avg. loss: 7335515.0362, Accuracy: 0/469 (0%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 661 [0/869 (0%)]\tLoss: 147230144.000000\n",
      "Train Epoch: 661 [160/869 (18%)]\tLoss: 274338880.000000\n",
      "Train Epoch: 661 [320/869 (36%)]\tLoss: 128536968.000000\n",
      "Train Epoch: 661 [480/869 (55%)]\tLoss: 141449792.000000\n",
      "Train Epoch: 661 [640/869 (73%)]\tLoss: 183716512.000000\n",
      "Train Epoch: 661 [800/869 (91%)]\tLoss: 79506920.000000\n",
      "\n",
      "Test set: Avg. loss: 7755415.8891, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 662 [0/869 (0%)]\tLoss: 95195184.000000\n",
      "Train Epoch: 662 [160/869 (18%)]\tLoss: 170499920.000000\n",
      "Train Epoch: 662 [320/869 (36%)]\tLoss: 143990656.000000\n",
      "Train Epoch: 662 [480/869 (55%)]\tLoss: 49925820.000000\n",
      "Train Epoch: 662 [640/869 (73%)]\tLoss: 55288888.000000\n",
      "Train Epoch: 662 [800/869 (91%)]\tLoss: 101532256.000000\n",
      "\n",
      "Test set: Avg. loss: 7481735.1727, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 663 [0/869 (0%)]\tLoss: 99977704.000000\n",
      "Train Epoch: 663 [160/869 (18%)]\tLoss: 112616160.000000\n",
      "Train Epoch: 663 [320/869 (36%)]\tLoss: 135321296.000000\n",
      "Train Epoch: 663 [480/869 (55%)]\tLoss: 157434864.000000\n",
      "Train Epoch: 663 [640/869 (73%)]\tLoss: 96548848.000000\n",
      "Train Epoch: 663 [800/869 (91%)]\tLoss: 185401408.000000\n",
      "\n",
      "Test set: Avg. loss: 7506610.0299, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 664 [0/869 (0%)]\tLoss: 110161184.000000\n",
      "Train Epoch: 664 [160/869 (18%)]\tLoss: 223782160.000000\n",
      "Train Epoch: 664 [320/869 (36%)]\tLoss: 291356768.000000\n",
      "Train Epoch: 664 [480/869 (55%)]\tLoss: 139481888.000000\n",
      "Train Epoch: 664 [640/869 (73%)]\tLoss: 42625744.000000\n",
      "Train Epoch: 664 [800/869 (91%)]\tLoss: 78163512.000000\n",
      "\n",
      "Test set: Avg. loss: 7533246.8657, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 665 [0/869 (0%)]\tLoss: 39726488.000000\n",
      "Train Epoch: 665 [160/869 (18%)]\tLoss: 62487824.000000\n",
      "Train Epoch: 665 [320/869 (36%)]\tLoss: 157914624.000000\n",
      "Train Epoch: 665 [480/869 (55%)]\tLoss: 173055744.000000\n",
      "Train Epoch: 665 [640/869 (73%)]\tLoss: 145986832.000000\n",
      "Train Epoch: 665 [800/869 (91%)]\tLoss: 36551612.000000\n",
      "\n",
      "Test set: Avg. loss: 7568040.7079, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 666 [0/869 (0%)]\tLoss: 133340584.000000\n",
      "Train Epoch: 666 [160/869 (18%)]\tLoss: 144097872.000000\n",
      "Train Epoch: 666 [320/869 (36%)]\tLoss: 226724864.000000\n",
      "Train Epoch: 666 [480/869 (55%)]\tLoss: 84442080.000000\n",
      "Train Epoch: 666 [640/869 (73%)]\tLoss: 63292380.000000\n",
      "Train Epoch: 666 [800/869 (91%)]\tLoss: 104243416.000000\n",
      "\n",
      "Test set: Avg. loss: 7723245.9446, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 667 [0/869 (0%)]\tLoss: 267130944.000000\n",
      "Train Epoch: 667 [160/869 (18%)]\tLoss: 160460528.000000\n",
      "Train Epoch: 667 [320/869 (36%)]\tLoss: 44062816.000000\n",
      "Train Epoch: 667 [480/869 (55%)]\tLoss: 136692768.000000\n",
      "Train Epoch: 667 [640/869 (73%)]\tLoss: 76079104.000000\n",
      "Train Epoch: 667 [800/869 (91%)]\tLoss: 228582448.000000\n",
      "\n",
      "Test set: Avg. loss: 7459025.9019, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 668 [0/869 (0%)]\tLoss: 204281360.000000\n",
      "Train Epoch: 668 [160/869 (18%)]\tLoss: 133045520.000000\n",
      "Train Epoch: 668 [320/869 (36%)]\tLoss: 82577264.000000\n",
      "Train Epoch: 668 [480/869 (55%)]\tLoss: 102584112.000000\n",
      "Train Epoch: 668 [640/869 (73%)]\tLoss: 167190416.000000\n",
      "Train Epoch: 668 [800/869 (91%)]\tLoss: 218188240.000000\n",
      "\n",
      "Test set: Avg. loss: 7389002.9083, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 669 [0/869 (0%)]\tLoss: 127144968.000000\n",
      "Train Epoch: 669 [160/869 (18%)]\tLoss: 128233480.000000\n",
      "Train Epoch: 669 [320/869 (36%)]\tLoss: 48015328.000000\n",
      "Train Epoch: 669 [480/869 (55%)]\tLoss: 240306304.000000\n",
      "Train Epoch: 669 [640/869 (73%)]\tLoss: 35381672.000000\n",
      "Train Epoch: 669 [800/869 (91%)]\tLoss: 108781952.000000\n",
      "\n",
      "Test set: Avg. loss: 7407655.3262, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 670 [0/869 (0%)]\tLoss: 63220416.000000\n",
      "Train Epoch: 670 [160/869 (18%)]\tLoss: 147939840.000000\n",
      "Train Epoch: 670 [320/869 (36%)]\tLoss: 195980240.000000\n",
      "Train Epoch: 670 [480/869 (55%)]\tLoss: 250778944.000000\n",
      "Train Epoch: 670 [640/869 (73%)]\tLoss: 26332030.000000\n",
      "Train Epoch: 670 [800/869 (91%)]\tLoss: 49724220.000000\n",
      "\n",
      "Test set: Avg. loss: 7510053.5949, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 671 [0/869 (0%)]\tLoss: 69576256.000000\n",
      "Train Epoch: 671 [160/869 (18%)]\tLoss: 132594936.000000\n",
      "Train Epoch: 671 [320/869 (36%)]\tLoss: 94370096.000000\n",
      "Train Epoch: 671 [480/869 (55%)]\tLoss: 133303808.000000\n",
      "Train Epoch: 671 [640/869 (73%)]\tLoss: 101928496.000000\n",
      "Train Epoch: 671 [800/869 (91%)]\tLoss: 85496072.000000\n",
      "\n",
      "Test set: Avg. loss: 7350882.1322, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 672 [0/869 (0%)]\tLoss: 176366400.000000\n",
      "Train Epoch: 672 [160/869 (18%)]\tLoss: 52256300.000000\n",
      "Train Epoch: 672 [320/869 (36%)]\tLoss: 112806928.000000\n",
      "Train Epoch: 672 [480/869 (55%)]\tLoss: 92223280.000000\n",
      "Train Epoch: 672 [640/869 (73%)]\tLoss: 258726368.000000\n",
      "Train Epoch: 672 [800/869 (91%)]\tLoss: 137633728.000000\n",
      "\n",
      "Test set: Avg. loss: 7526032.7420, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 673 [0/869 (0%)]\tLoss: 82607792.000000\n",
      "Train Epoch: 673 [160/869 (18%)]\tLoss: 100715512.000000\n",
      "Train Epoch: 673 [320/869 (36%)]\tLoss: 83749168.000000\n",
      "Train Epoch: 673 [480/869 (55%)]\tLoss: 106682160.000000\n",
      "Train Epoch: 673 [640/869 (73%)]\tLoss: 140485328.000000\n",
      "Train Epoch: 673 [800/869 (91%)]\tLoss: 109083776.000000\n",
      "\n",
      "Test set: Avg. loss: 7834431.9062, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 674 [0/869 (0%)]\tLoss: 132421168.000000\n",
      "Train Epoch: 674 [160/869 (18%)]\tLoss: 159649776.000000\n",
      "Train Epoch: 674 [320/869 (36%)]\tLoss: 108826976.000000\n",
      "Train Epoch: 674 [480/869 (55%)]\tLoss: 130694240.000000\n",
      "Train Epoch: 674 [640/869 (73%)]\tLoss: 97476704.000000\n",
      "Train Epoch: 674 [800/869 (91%)]\tLoss: 81438376.000000\n",
      "\n",
      "Test set: Avg. loss: 7726913.8593, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 675 [0/869 (0%)]\tLoss: 82876336.000000\n",
      "Train Epoch: 675 [160/869 (18%)]\tLoss: 279577952.000000\n",
      "Train Epoch: 675 [320/869 (36%)]\tLoss: 168486912.000000\n",
      "Train Epoch: 675 [480/869 (55%)]\tLoss: 84441776.000000\n",
      "Train Epoch: 675 [640/869 (73%)]\tLoss: 123705240.000000\n",
      "Train Epoch: 675 [800/869 (91%)]\tLoss: 90386448.000000\n",
      "\n",
      "Test set: Avg. loss: 7511205.8337, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 676 [0/869 (0%)]\tLoss: 223518496.000000\n",
      "Train Epoch: 676 [160/869 (18%)]\tLoss: 90542272.000000\n",
      "Train Epoch: 676 [320/869 (36%)]\tLoss: 88000960.000000\n",
      "Train Epoch: 676 [480/869 (55%)]\tLoss: 131291720.000000\n",
      "Train Epoch: 676 [640/869 (73%)]\tLoss: 98835696.000000\n",
      "Train Epoch: 676 [800/869 (91%)]\tLoss: 51639152.000000\n",
      "\n",
      "Test set: Avg. loss: 7391901.8166, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 677 [0/869 (0%)]\tLoss: 147573328.000000\n",
      "Train Epoch: 677 [160/869 (18%)]\tLoss: 106996624.000000\n",
      "Train Epoch: 677 [320/869 (36%)]\tLoss: 107443656.000000\n",
      "Train Epoch: 677 [480/869 (55%)]\tLoss: 54235756.000000\n",
      "Train Epoch: 677 [640/869 (73%)]\tLoss: 129658168.000000\n",
      "Train Epoch: 677 [800/869 (91%)]\tLoss: 267261728.000000\n",
      "\n",
      "Test set: Avg. loss: 7399436.5800, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 678 [0/869 (0%)]\tLoss: 161536656.000000\n",
      "Train Epoch: 678 [160/869 (18%)]\tLoss: 271503488.000000\n",
      "Train Epoch: 678 [320/869 (36%)]\tLoss: 41261200.000000\n",
      "Train Epoch: 678 [480/869 (55%)]\tLoss: 143123824.000000\n",
      "Train Epoch: 678 [640/869 (73%)]\tLoss: 124225440.000000\n",
      "Train Epoch: 678 [800/869 (91%)]\tLoss: 142930096.000000\n",
      "\n",
      "Test set: Avg. loss: 7428992.7761, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 679 [0/869 (0%)]\tLoss: 91085488.000000\n",
      "Train Epoch: 679 [160/869 (18%)]\tLoss: 149977584.000000\n",
      "Train Epoch: 679 [320/869 (36%)]\tLoss: 30226910.000000\n",
      "Train Epoch: 679 [480/869 (55%)]\tLoss: 206959152.000000\n",
      "Train Epoch: 679 [640/869 (73%)]\tLoss: 101371368.000000\n",
      "Train Epoch: 679 [800/869 (91%)]\tLoss: 117185304.000000\n",
      "\n",
      "Test set: Avg. loss: 7416271.5224, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 680 [0/869 (0%)]\tLoss: 56633496.000000\n",
      "Train Epoch: 680 [160/869 (18%)]\tLoss: 162248144.000000\n",
      "Train Epoch: 680 [320/869 (36%)]\tLoss: 70537064.000000\n",
      "Train Epoch: 680 [480/869 (55%)]\tLoss: 126696744.000000\n",
      "Train Epoch: 680 [640/869 (73%)]\tLoss: 83366560.000000\n",
      "Train Epoch: 680 [800/869 (91%)]\tLoss: 95242112.000000\n",
      "\n",
      "Test set: Avg. loss: 7330788.3326, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 681 [0/869 (0%)]\tLoss: 174380624.000000\n",
      "Train Epoch: 681 [160/869 (18%)]\tLoss: 82960152.000000\n",
      "Train Epoch: 681 [320/869 (36%)]\tLoss: 176325152.000000\n",
      "Train Epoch: 681 [480/869 (55%)]\tLoss: 64447904.000000\n",
      "Train Epoch: 681 [640/869 (73%)]\tLoss: 82441424.000000\n",
      "Train Epoch: 681 [800/869 (91%)]\tLoss: 79319304.000000\n",
      "\n",
      "Test set: Avg. loss: 7388808.6652, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 682 [0/869 (0%)]\tLoss: 131761624.000000\n",
      "Train Epoch: 682 [160/869 (18%)]\tLoss: 229200480.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 682 [320/869 (36%)]\tLoss: 87940384.000000\n",
      "Train Epoch: 682 [480/869 (55%)]\tLoss: 225890080.000000\n",
      "Train Epoch: 682 [640/869 (73%)]\tLoss: 119796832.000000\n",
      "Train Epoch: 682 [800/869 (91%)]\tLoss: 42939740.000000\n",
      "\n",
      "Test set: Avg. loss: 7634469.6205, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 683 [0/869 (0%)]\tLoss: 127672224.000000\n",
      "Train Epoch: 683 [160/869 (18%)]\tLoss: 94853704.000000\n",
      "Train Epoch: 683 [320/869 (36%)]\tLoss: 128128848.000000\n",
      "Train Epoch: 683 [480/869 (55%)]\tLoss: 127797968.000000\n",
      "Train Epoch: 683 [640/869 (73%)]\tLoss: 120066720.000000\n",
      "Train Epoch: 683 [800/869 (91%)]\tLoss: 157222640.000000\n",
      "\n",
      "Test set: Avg. loss: 7392320.5458, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 684 [0/869 (0%)]\tLoss: 60770076.000000\n",
      "Train Epoch: 684 [160/869 (18%)]\tLoss: 75860712.000000\n",
      "Train Epoch: 684 [320/869 (36%)]\tLoss: 292239616.000000\n",
      "Train Epoch: 684 [480/869 (55%)]\tLoss: 135912832.000000\n",
      "Train Epoch: 684 [640/869 (73%)]\tLoss: 49533352.000000\n",
      "Train Epoch: 684 [800/869 (91%)]\tLoss: 297544224.000000\n",
      "\n",
      "Test set: Avg. loss: 7574290.9510, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 685 [0/869 (0%)]\tLoss: 73788704.000000\n",
      "Train Epoch: 685 [160/869 (18%)]\tLoss: 298043072.000000\n",
      "Train Epoch: 685 [320/869 (36%)]\tLoss: 169991104.000000\n",
      "Train Epoch: 685 [480/869 (55%)]\tLoss: 50482740.000000\n",
      "Train Epoch: 685 [640/869 (73%)]\tLoss: 86659608.000000\n",
      "Train Epoch: 685 [800/869 (91%)]\tLoss: 150256176.000000\n",
      "\n",
      "Test set: Avg. loss: 7492865.5267, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 686 [0/869 (0%)]\tLoss: 114333144.000000\n",
      "Train Epoch: 686 [160/869 (18%)]\tLoss: 182125472.000000\n",
      "Train Epoch: 686 [320/869 (36%)]\tLoss: 49487416.000000\n",
      "Train Epoch: 686 [480/869 (55%)]\tLoss: 186387312.000000\n",
      "Train Epoch: 686 [640/869 (73%)]\tLoss: 47005236.000000\n",
      "Train Epoch: 686 [800/869 (91%)]\tLoss: 142559904.000000\n",
      "\n",
      "Test set: Avg. loss: 7514947.5139, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 687 [0/869 (0%)]\tLoss: 203070640.000000\n",
      "Train Epoch: 687 [160/869 (18%)]\tLoss: 88089792.000000\n",
      "Train Epoch: 687 [320/869 (36%)]\tLoss: 80118712.000000\n",
      "Train Epoch: 687 [480/869 (55%)]\tLoss: 272469920.000000\n",
      "Train Epoch: 687 [640/869 (73%)]\tLoss: 256443440.000000\n",
      "Train Epoch: 687 [800/869 (91%)]\tLoss: 101858928.000000\n",
      "\n",
      "Test set: Avg. loss: 7399325.2793, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 688 [0/869 (0%)]\tLoss: 45453492.000000\n",
      "Train Epoch: 688 [160/869 (18%)]\tLoss: 90074904.000000\n",
      "Train Epoch: 688 [320/869 (36%)]\tLoss: 83700840.000000\n",
      "Train Epoch: 688 [480/869 (55%)]\tLoss: 82811384.000000\n",
      "Train Epoch: 688 [640/869 (73%)]\tLoss: 38569352.000000\n",
      "Train Epoch: 688 [800/869 (91%)]\tLoss: 217291248.000000\n",
      "\n",
      "Test set: Avg. loss: 7422603.1215, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 689 [0/869 (0%)]\tLoss: 108868824.000000\n",
      "Train Epoch: 689 [160/869 (18%)]\tLoss: 97500320.000000\n",
      "Train Epoch: 689 [320/869 (36%)]\tLoss: 311466496.000000\n",
      "Train Epoch: 689 [480/869 (55%)]\tLoss: 214838496.000000\n",
      "Train Epoch: 689 [640/869 (73%)]\tLoss: 291889216.000000\n",
      "Train Epoch: 689 [800/869 (91%)]\tLoss: 100124160.000000\n",
      "\n",
      "Test set: Avg. loss: 7490107.6077, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 690 [0/869 (0%)]\tLoss: 93299128.000000\n",
      "Train Epoch: 690 [160/869 (18%)]\tLoss: 144573408.000000\n",
      "Train Epoch: 690 [320/869 (36%)]\tLoss: 47846088.000000\n",
      "Train Epoch: 690 [480/869 (55%)]\tLoss: 107815632.000000\n",
      "Train Epoch: 690 [640/869 (73%)]\tLoss: 95186256.000000\n",
      "Train Epoch: 690 [800/869 (91%)]\tLoss: 121391424.000000\n",
      "\n",
      "Test set: Avg. loss: 7518577.9446, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 691 [0/869 (0%)]\tLoss: 86881008.000000\n",
      "Train Epoch: 691 [160/869 (18%)]\tLoss: 44130012.000000\n",
      "Train Epoch: 691 [320/869 (36%)]\tLoss: 238647200.000000\n",
      "Train Epoch: 691 [480/869 (55%)]\tLoss: 86024952.000000\n",
      "Train Epoch: 691 [640/869 (73%)]\tLoss: 88987656.000000\n",
      "Train Epoch: 691 [800/869 (91%)]\tLoss: 172522224.000000\n",
      "\n",
      "Test set: Avg. loss: 8117721.0384, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 692 [0/869 (0%)]\tLoss: 36118488.000000\n",
      "Train Epoch: 692 [160/869 (18%)]\tLoss: 65833016.000000\n",
      "Train Epoch: 692 [320/869 (36%)]\tLoss: 165225632.000000\n",
      "Train Epoch: 692 [480/869 (55%)]\tLoss: 84348432.000000\n",
      "Train Epoch: 692 [640/869 (73%)]\tLoss: 141491712.000000\n",
      "Train Epoch: 692 [800/869 (91%)]\tLoss: 141436592.000000\n",
      "\n",
      "Test set: Avg. loss: 7511930.1066, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 693 [0/869 (0%)]\tLoss: 153120576.000000\n",
      "Train Epoch: 693 [160/869 (18%)]\tLoss: 37791568.000000\n",
      "Train Epoch: 693 [320/869 (36%)]\tLoss: 121284216.000000\n",
      "Train Epoch: 693 [480/869 (55%)]\tLoss: 151570560.000000\n",
      "Train Epoch: 693 [640/869 (73%)]\tLoss: 125944872.000000\n",
      "Train Epoch: 693 [800/869 (91%)]\tLoss: 153109488.000000\n",
      "\n",
      "Test set: Avg. loss: 7615156.1791, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 694 [0/869 (0%)]\tLoss: 33233612.000000\n",
      "Train Epoch: 694 [160/869 (18%)]\tLoss: 106636784.000000\n",
      "Train Epoch: 694 [320/869 (36%)]\tLoss: 140479504.000000\n",
      "Train Epoch: 694 [480/869 (55%)]\tLoss: 31303464.000000\n",
      "Train Epoch: 694 [640/869 (73%)]\tLoss: 227845312.000000\n",
      "Train Epoch: 694 [800/869 (91%)]\tLoss: 101324672.000000\n",
      "\n",
      "Test set: Avg. loss: 7513213.4584, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 695 [0/869 (0%)]\tLoss: 38827300.000000\n",
      "Train Epoch: 695 [160/869 (18%)]\tLoss: 57265980.000000\n",
      "Train Epoch: 695 [320/869 (36%)]\tLoss: 62971552.000000\n",
      "Train Epoch: 695 [480/869 (55%)]\tLoss: 54218176.000000\n",
      "Train Epoch: 695 [640/869 (73%)]\tLoss: 175643248.000000\n",
      "Train Epoch: 695 [800/869 (91%)]\tLoss: 207022480.000000\n",
      "\n",
      "Test set: Avg. loss: 7430869.8166, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 696 [0/869 (0%)]\tLoss: 270500992.000000\n",
      "Train Epoch: 696 [160/869 (18%)]\tLoss: 142377504.000000\n",
      "Train Epoch: 696 [320/869 (36%)]\tLoss: 57857532.000000\n",
      "Train Epoch: 696 [480/869 (55%)]\tLoss: 128324528.000000\n",
      "Train Epoch: 696 [640/869 (73%)]\tLoss: 117386016.000000\n",
      "Train Epoch: 696 [800/869 (91%)]\tLoss: 128226928.000000\n",
      "\n",
      "Test set: Avg. loss: 7442361.1684, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 697 [0/869 (0%)]\tLoss: 250207856.000000\n",
      "Train Epoch: 697 [160/869 (18%)]\tLoss: 146596704.000000\n",
      "Train Epoch: 697 [320/869 (36%)]\tLoss: 83375904.000000\n",
      "Train Epoch: 697 [480/869 (55%)]\tLoss: 98540256.000000\n",
      "Train Epoch: 697 [640/869 (73%)]\tLoss: 128964176.000000\n",
      "Train Epoch: 697 [800/869 (91%)]\tLoss: 132975960.000000\n",
      "\n",
      "Test set: Avg. loss: 7865481.6205, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 698 [0/869 (0%)]\tLoss: 118333000.000000\n",
      "Train Epoch: 698 [160/869 (18%)]\tLoss: 100744824.000000\n",
      "Train Epoch: 698 [320/869 (36%)]\tLoss: 98760360.000000\n",
      "Train Epoch: 698 [480/869 (55%)]\tLoss: 121946240.000000\n",
      "Train Epoch: 698 [640/869 (73%)]\tLoss: 98008560.000000\n",
      "Train Epoch: 698 [800/869 (91%)]\tLoss: 85279296.000000\n",
      "\n",
      "Test set: Avg. loss: 8226071.2452, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 699 [0/869 (0%)]\tLoss: 224764224.000000\n",
      "Train Epoch: 699 [160/869 (18%)]\tLoss: 136521024.000000\n",
      "Train Epoch: 699 [320/869 (36%)]\tLoss: 39839816.000000\n",
      "Train Epoch: 699 [480/869 (55%)]\tLoss: 83677312.000000\n",
      "Train Epoch: 699 [640/869 (73%)]\tLoss: 93848080.000000\n",
      "Train Epoch: 699 [800/869 (91%)]\tLoss: 102371056.000000\n",
      "\n",
      "Test set: Avg. loss: 7634849.6546, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 700 [0/869 (0%)]\tLoss: 65702144.000000\n",
      "Train Epoch: 700 [160/869 (18%)]\tLoss: 55486432.000000\n",
      "Train Epoch: 700 [320/869 (36%)]\tLoss: 35355404.000000\n",
      "Train Epoch: 700 [480/869 (55%)]\tLoss: 88884432.000000\n",
      "Train Epoch: 700 [640/869 (73%)]\tLoss: 195741472.000000\n",
      "Train Epoch: 700 [800/869 (91%)]\tLoss: 84085136.000000\n",
      "\n",
      "Test set: Avg. loss: 7492096.1407, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 701 [0/869 (0%)]\tLoss: 119237632.000000\n",
      "Train Epoch: 701 [160/869 (18%)]\tLoss: 112789152.000000\n",
      "Train Epoch: 701 [320/869 (36%)]\tLoss: 112428696.000000\n",
      "Train Epoch: 701 [480/869 (55%)]\tLoss: 229227216.000000\n",
      "Train Epoch: 701 [640/869 (73%)]\tLoss: 72691832.000000\n",
      "Train Epoch: 701 [800/869 (91%)]\tLoss: 171959088.000000\n",
      "\n",
      "Test set: Avg. loss: 7618828.1620, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 702 [0/869 (0%)]\tLoss: 157310480.000000\n",
      "Train Epoch: 702 [160/869 (18%)]\tLoss: 77535648.000000\n",
      "Train Epoch: 702 [320/869 (36%)]\tLoss: 244686624.000000\n",
      "Train Epoch: 702 [480/869 (55%)]\tLoss: 95767040.000000\n",
      "Train Epoch: 702 [640/869 (73%)]\tLoss: 232878976.000000\n",
      "Train Epoch: 702 [800/869 (91%)]\tLoss: 84236312.000000\n",
      "\n",
      "Test set: Avg. loss: 7421991.7953, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 703 [0/869 (0%)]\tLoss: 127649144.000000\n",
      "Train Epoch: 703 [160/869 (18%)]\tLoss: 168987904.000000\n",
      "Train Epoch: 703 [320/869 (36%)]\tLoss: 106247280.000000\n",
      "Train Epoch: 703 [480/869 (55%)]\tLoss: 84880736.000000\n",
      "Train Epoch: 703 [640/869 (73%)]\tLoss: 92286736.000000\n",
      "Train Epoch: 703 [800/869 (91%)]\tLoss: 170520976.000000\n",
      "\n",
      "Test set: Avg. loss: 7432791.3262, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 704 [0/869 (0%)]\tLoss: 71898240.000000\n",
      "Train Epoch: 704 [160/869 (18%)]\tLoss: 55546568.000000\n",
      "Train Epoch: 704 [320/869 (36%)]\tLoss: 93766472.000000\n",
      "Train Epoch: 704 [480/869 (55%)]\tLoss: 79953816.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 704 [640/869 (73%)]\tLoss: 77367552.000000\n",
      "Train Epoch: 704 [800/869 (91%)]\tLoss: 212405856.000000\n",
      "\n",
      "Test set: Avg. loss: 7727146.3369, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 705 [0/869 (0%)]\tLoss: 195844752.000000\n",
      "Train Epoch: 705 [160/869 (18%)]\tLoss: 127677912.000000\n",
      "Train Epoch: 705 [320/869 (36%)]\tLoss: 71115584.000000\n",
      "Train Epoch: 705 [480/869 (55%)]\tLoss: 181229920.000000\n",
      "Train Epoch: 705 [640/869 (73%)]\tLoss: 278557696.000000\n",
      "Train Epoch: 705 [800/869 (91%)]\tLoss: 144711360.000000\n",
      "\n",
      "Test set: Avg. loss: 7787201.9701, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 706 [0/869 (0%)]\tLoss: 134397424.000000\n",
      "Train Epoch: 706 [160/869 (18%)]\tLoss: 213161728.000000\n",
      "Train Epoch: 706 [320/869 (36%)]\tLoss: 79586304.000000\n",
      "Train Epoch: 706 [480/869 (55%)]\tLoss: 169214592.000000\n",
      "Train Epoch: 706 [640/869 (73%)]\tLoss: 93245328.000000\n",
      "Train Epoch: 706 [800/869 (91%)]\tLoss: 76569688.000000\n",
      "\n",
      "Test set: Avg. loss: 7453789.2793, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 707 [0/869 (0%)]\tLoss: 59511476.000000\n",
      "Train Epoch: 707 [160/869 (18%)]\tLoss: 163865616.000000\n",
      "Train Epoch: 707 [320/869 (36%)]\tLoss: 87528056.000000\n",
      "Train Epoch: 707 [480/869 (55%)]\tLoss: 104856688.000000\n",
      "Train Epoch: 707 [640/869 (73%)]\tLoss: 39193940.000000\n",
      "Train Epoch: 707 [800/869 (91%)]\tLoss: 92555680.000000\n",
      "\n",
      "Test set: Avg. loss: 7665790.2090, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 708 [0/869 (0%)]\tLoss: 85197680.000000\n",
      "Train Epoch: 708 [160/869 (18%)]\tLoss: 203487664.000000\n",
      "Train Epoch: 708 [320/869 (36%)]\tLoss: 99357872.000000\n",
      "Train Epoch: 708 [480/869 (55%)]\tLoss: 32221436.000000\n",
      "Train Epoch: 708 [640/869 (73%)]\tLoss: 136813904.000000\n",
      "Train Epoch: 708 [800/869 (91%)]\tLoss: 130098240.000000\n",
      "\n",
      "Test set: Avg. loss: 7391291.3518, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 709 [0/869 (0%)]\tLoss: 135869584.000000\n",
      "Train Epoch: 709 [160/869 (18%)]\tLoss: 126222640.000000\n",
      "Train Epoch: 709 [320/869 (36%)]\tLoss: 217460192.000000\n",
      "Train Epoch: 709 [480/869 (55%)]\tLoss: 134346000.000000\n",
      "Train Epoch: 709 [640/869 (73%)]\tLoss: 252969152.000000\n",
      "Train Epoch: 709 [800/869 (91%)]\tLoss: 81793136.000000\n",
      "\n",
      "Test set: Avg. loss: 7482768.4264, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 710 [0/869 (0%)]\tLoss: 121698768.000000\n",
      "Train Epoch: 710 [160/869 (18%)]\tLoss: 88482552.000000\n",
      "Train Epoch: 710 [320/869 (36%)]\tLoss: 89340536.000000\n",
      "Train Epoch: 710 [480/869 (55%)]\tLoss: 49199844.000000\n",
      "Train Epoch: 710 [640/869 (73%)]\tLoss: 181915872.000000\n",
      "Train Epoch: 710 [800/869 (91%)]\tLoss: 142339440.000000\n",
      "\n",
      "Test set: Avg. loss: 7778797.1002, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 711 [0/869 (0%)]\tLoss: 82889320.000000\n",
      "Train Epoch: 711 [160/869 (18%)]\tLoss: 39146668.000000\n",
      "Train Epoch: 711 [320/869 (36%)]\tLoss: 134048752.000000\n",
      "Train Epoch: 711 [480/869 (55%)]\tLoss: 84836608.000000\n",
      "Train Epoch: 711 [640/869 (73%)]\tLoss: 121160288.000000\n",
      "Train Epoch: 711 [800/869 (91%)]\tLoss: 44040352.000000\n",
      "\n",
      "Test set: Avg. loss: 7484931.4456, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 712 [0/869 (0%)]\tLoss: 104796032.000000\n",
      "Train Epoch: 712 [160/869 (18%)]\tLoss: 170368800.000000\n",
      "Train Epoch: 712 [320/869 (36%)]\tLoss: 59808880.000000\n",
      "Train Epoch: 712 [480/869 (55%)]\tLoss: 50008828.000000\n",
      "Train Epoch: 712 [640/869 (73%)]\tLoss: 95416872.000000\n",
      "Train Epoch: 712 [800/869 (91%)]\tLoss: 81589264.000000\n",
      "\n",
      "Test set: Avg. loss: 7332324.8188, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 713 [0/869 (0%)]\tLoss: 146483184.000000\n",
      "Train Epoch: 713 [160/869 (18%)]\tLoss: 124464976.000000\n",
      "Train Epoch: 713 [320/869 (36%)]\tLoss: 192888528.000000\n",
      "Train Epoch: 713 [480/869 (55%)]\tLoss: 96231360.000000\n",
      "Train Epoch: 713 [640/869 (73%)]\tLoss: 121125808.000000\n",
      "Train Epoch: 713 [800/869 (91%)]\tLoss: 58219312.000000\n",
      "\n",
      "Test set: Avg. loss: 7458344.6823, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 714 [0/869 (0%)]\tLoss: 90055656.000000\n",
      "Train Epoch: 714 [160/869 (18%)]\tLoss: 285090240.000000\n",
      "Train Epoch: 714 [320/869 (36%)]\tLoss: 145332432.000000\n",
      "Train Epoch: 714 [480/869 (55%)]\tLoss: 85058368.000000\n",
      "Train Epoch: 714 [640/869 (73%)]\tLoss: 49307368.000000\n",
      "Train Epoch: 714 [800/869 (91%)]\tLoss: 93851976.000000\n",
      "\n",
      "Test set: Avg. loss: 7320060.0171, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 715 [0/869 (0%)]\tLoss: 86931536.000000\n",
      "Train Epoch: 715 [160/869 (18%)]\tLoss: 228451552.000000\n",
      "Train Epoch: 715 [320/869 (36%)]\tLoss: 133739872.000000\n",
      "Train Epoch: 715 [480/869 (55%)]\tLoss: 54590520.000000\n",
      "Train Epoch: 715 [640/869 (73%)]\tLoss: 94270768.000000\n",
      "Train Epoch: 715 [800/869 (91%)]\tLoss: 156322656.000000\n",
      "\n",
      "Test set: Avg. loss: 7585812.6226, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 716 [0/869 (0%)]\tLoss: 121248576.000000\n",
      "Train Epoch: 716 [160/869 (18%)]\tLoss: 98954960.000000\n",
      "Train Epoch: 716 [320/869 (36%)]\tLoss: 83920440.000000\n",
      "Train Epoch: 716 [480/869 (55%)]\tLoss: 172414448.000000\n",
      "Train Epoch: 716 [640/869 (73%)]\tLoss: 149654640.000000\n",
      "Train Epoch: 716 [800/869 (91%)]\tLoss: 143740416.000000\n",
      "\n",
      "Test set: Avg. loss: 7345945.0576, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 717 [0/869 (0%)]\tLoss: 141269200.000000\n",
      "Train Epoch: 717 [160/869 (18%)]\tLoss: 189988608.000000\n",
      "Train Epoch: 717 [320/869 (36%)]\tLoss: 36591128.000000\n",
      "Train Epoch: 717 [480/869 (55%)]\tLoss: 171762432.000000\n",
      "Train Epoch: 717 [640/869 (73%)]\tLoss: 182652240.000000\n",
      "Train Epoch: 717 [800/869 (91%)]\tLoss: 110612808.000000\n",
      "\n",
      "Test set: Avg. loss: 7413581.6205, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 718 [0/869 (0%)]\tLoss: 77983304.000000\n",
      "Train Epoch: 718 [160/869 (18%)]\tLoss: 59774984.000000\n",
      "Train Epoch: 718 [320/869 (36%)]\tLoss: 104832856.000000\n",
      "Train Epoch: 718 [480/869 (55%)]\tLoss: 85816744.000000\n",
      "Train Epoch: 718 [640/869 (73%)]\tLoss: 132962752.000000\n",
      "Train Epoch: 718 [800/869 (91%)]\tLoss: 99654736.000000\n",
      "\n",
      "Test set: Avg. loss: 7517878.6866, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 719 [0/869 (0%)]\tLoss: 135145440.000000\n",
      "Train Epoch: 719 [160/869 (18%)]\tLoss: 99281296.000000\n",
      "Train Epoch: 719 [320/869 (36%)]\tLoss: 114921552.000000\n",
      "Train Epoch: 719 [480/869 (55%)]\tLoss: 124653896.000000\n",
      "Train Epoch: 719 [640/869 (73%)]\tLoss: 248337104.000000\n",
      "Train Epoch: 719 [800/869 (91%)]\tLoss: 146302384.000000\n",
      "\n",
      "Test set: Avg. loss: 7388194.5757, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 720 [0/869 (0%)]\tLoss: 44432044.000000\n",
      "Train Epoch: 720 [160/869 (18%)]\tLoss: 69898048.000000\n",
      "Train Epoch: 720 [320/869 (36%)]\tLoss: 76243952.000000\n",
      "Train Epoch: 720 [480/869 (55%)]\tLoss: 121473616.000000\n",
      "Train Epoch: 720 [640/869 (73%)]\tLoss: 211286400.000000\n",
      "Train Epoch: 720 [800/869 (91%)]\tLoss: 97385376.000000\n",
      "\n",
      "Test set: Avg. loss: 7347507.0917, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 721 [0/869 (0%)]\tLoss: 213233376.000000\n",
      "Train Epoch: 721 [160/869 (18%)]\tLoss: 113571008.000000\n",
      "Train Epoch: 721 [320/869 (36%)]\tLoss: 77274208.000000\n",
      "Train Epoch: 721 [480/869 (55%)]\tLoss: 90394152.000000\n",
      "Train Epoch: 721 [640/869 (73%)]\tLoss: 319398656.000000\n",
      "Train Epoch: 721 [800/869 (91%)]\tLoss: 145817120.000000\n",
      "\n",
      "Test set: Avg. loss: 7545542.9083, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 722 [0/869 (0%)]\tLoss: 81887800.000000\n",
      "Train Epoch: 722 [160/869 (18%)]\tLoss: 55439000.000000\n",
      "Train Epoch: 722 [320/869 (36%)]\tLoss: 139136480.000000\n",
      "Train Epoch: 722 [480/869 (55%)]\tLoss: 45561424.000000\n",
      "Train Epoch: 722 [640/869 (73%)]\tLoss: 129423952.000000\n",
      "Train Epoch: 722 [800/869 (91%)]\tLoss: 150387232.000000\n",
      "\n",
      "Test set: Avg. loss: 7606944.1450, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 723 [0/869 (0%)]\tLoss: 61120460.000000\n",
      "Train Epoch: 723 [160/869 (18%)]\tLoss: 126519816.000000\n",
      "Train Epoch: 723 [320/869 (36%)]\tLoss: 99774944.000000\n",
      "Train Epoch: 723 [480/869 (55%)]\tLoss: 100119696.000000\n",
      "Train Epoch: 723 [640/869 (73%)]\tLoss: 270109024.000000\n",
      "Train Epoch: 723 [800/869 (91%)]\tLoss: 150638976.000000\n",
      "\n",
      "Test set: Avg. loss: 7356602.5672, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 724 [0/869 (0%)]\tLoss: 160060496.000000\n",
      "Train Epoch: 724 [160/869 (18%)]\tLoss: 79277040.000000\n",
      "Train Epoch: 724 [320/869 (36%)]\tLoss: 131640304.000000\n",
      "Train Epoch: 724 [480/869 (55%)]\tLoss: 139270000.000000\n",
      "Train Epoch: 724 [640/869 (73%)]\tLoss: 110958040.000000\n",
      "Train Epoch: 724 [800/869 (91%)]\tLoss: 197165488.000000\n",
      "\n",
      "Test set: Avg. loss: 7542183.2665, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 725 [0/869 (0%)]\tLoss: 219511200.000000\n",
      "Train Epoch: 725 [160/869 (18%)]\tLoss: 193780624.000000\n",
      "Train Epoch: 725 [320/869 (36%)]\tLoss: 189389856.000000\n",
      "Train Epoch: 725 [480/869 (55%)]\tLoss: 179688416.000000\n",
      "Train Epoch: 725 [640/869 (73%)]\tLoss: 78893808.000000\n",
      "Train Epoch: 725 [800/869 (91%)]\tLoss: 199746624.000000\n",
      "\n",
      "Test set: Avg. loss: 7336003.0107, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 726 [0/869 (0%)]\tLoss: 98218032.000000\n",
      "Train Epoch: 726 [160/869 (18%)]\tLoss: 38321644.000000\n",
      "Train Epoch: 726 [320/869 (36%)]\tLoss: 79494440.000000\n",
      "Train Epoch: 726 [480/869 (55%)]\tLoss: 159951856.000000\n",
      "Train Epoch: 726 [640/869 (73%)]\tLoss: 131269040.000000\n",
      "Train Epoch: 726 [800/869 (91%)]\tLoss: 101358384.000000\n",
      "\n",
      "Test set: Avg. loss: 7315854.9083, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 727 [0/869 (0%)]\tLoss: 80774832.000000\n",
      "Train Epoch: 727 [160/869 (18%)]\tLoss: 90268912.000000\n",
      "Train Epoch: 727 [320/869 (36%)]\tLoss: 153970992.000000\n",
      "Train Epoch: 727 [480/869 (55%)]\tLoss: 151926608.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 727 [640/869 (73%)]\tLoss: 105193368.000000\n",
      "Train Epoch: 727 [800/869 (91%)]\tLoss: 387549152.000000\n",
      "\n",
      "Test set: Avg. loss: 7501430.5842, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 728 [0/869 (0%)]\tLoss: 235003872.000000\n",
      "Train Epoch: 728 [160/869 (18%)]\tLoss: 128822016.000000\n",
      "Train Epoch: 728 [320/869 (36%)]\tLoss: 118245216.000000\n",
      "Train Epoch: 728 [480/869 (55%)]\tLoss: 121589376.000000\n",
      "Train Epoch: 728 [640/869 (73%)]\tLoss: 196840032.000000\n",
      "Train Epoch: 728 [800/869 (91%)]\tLoss: 98402352.000000\n",
      "\n",
      "Test set: Avg. loss: 7717995.1301, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 729 [0/869 (0%)]\tLoss: 233273392.000000\n",
      "Train Epoch: 729 [160/869 (18%)]\tLoss: 179998416.000000\n",
      "Train Epoch: 729 [320/869 (36%)]\tLoss: 110060960.000000\n",
      "Train Epoch: 729 [480/869 (55%)]\tLoss: 143865504.000000\n",
      "Train Epoch: 729 [640/869 (73%)]\tLoss: 80786136.000000\n",
      "Train Epoch: 729 [800/869 (91%)]\tLoss: 139197680.000000\n",
      "\n",
      "Test set: Avg. loss: 7509721.6716, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 730 [0/869 (0%)]\tLoss: 129112128.000000\n",
      "Train Epoch: 730 [160/869 (18%)]\tLoss: 180436544.000000\n",
      "Train Epoch: 730 [320/869 (36%)]\tLoss: 45261480.000000\n",
      "Train Epoch: 730 [480/869 (55%)]\tLoss: 169837568.000000\n",
      "Train Epoch: 730 [640/869 (73%)]\tLoss: 99439472.000000\n",
      "Train Epoch: 730 [800/869 (91%)]\tLoss: 106693272.000000\n",
      "\n",
      "Test set: Avg. loss: 7871599.8721, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 731 [0/869 (0%)]\tLoss: 226541088.000000\n",
      "Train Epoch: 731 [160/869 (18%)]\tLoss: 143972096.000000\n",
      "Train Epoch: 731 [320/869 (36%)]\tLoss: 201843600.000000\n",
      "Train Epoch: 731 [480/869 (55%)]\tLoss: 174586992.000000\n",
      "Train Epoch: 731 [640/869 (73%)]\tLoss: 100852560.000000\n",
      "Train Epoch: 731 [800/869 (91%)]\tLoss: 38587364.000000\n",
      "\n",
      "Test set: Avg. loss: 7407455.9829, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 732 [0/869 (0%)]\tLoss: 149716160.000000\n",
      "Train Epoch: 732 [160/869 (18%)]\tLoss: 122991592.000000\n",
      "Train Epoch: 732 [320/869 (36%)]\tLoss: 27690892.000000\n",
      "Train Epoch: 732 [480/869 (55%)]\tLoss: 172438304.000000\n",
      "Train Epoch: 732 [640/869 (73%)]\tLoss: 143893472.000000\n",
      "Train Epoch: 732 [800/869 (91%)]\tLoss: 144585888.000000\n",
      "\n",
      "Test set: Avg. loss: 7535497.2196, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 733 [0/869 (0%)]\tLoss: 102743640.000000\n",
      "Train Epoch: 733 [160/869 (18%)]\tLoss: 182421888.000000\n",
      "Train Epoch: 733 [320/869 (36%)]\tLoss: 94554320.000000\n",
      "Train Epoch: 733 [480/869 (55%)]\tLoss: 123621488.000000\n",
      "Train Epoch: 733 [640/869 (73%)]\tLoss: 44121148.000000\n",
      "Train Epoch: 733 [800/869 (91%)]\tLoss: 89137088.000000\n",
      "\n",
      "Test set: Avg. loss: 7515861.7484, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 734 [0/869 (0%)]\tLoss: 186666720.000000\n",
      "Train Epoch: 734 [160/869 (18%)]\tLoss: 98023784.000000\n",
      "Train Epoch: 734 [320/869 (36%)]\tLoss: 194740960.000000\n",
      "Train Epoch: 734 [480/869 (55%)]\tLoss: 99147744.000000\n",
      "Train Epoch: 734 [640/869 (73%)]\tLoss: 226220656.000000\n",
      "Train Epoch: 734 [800/869 (91%)]\tLoss: 296316288.000000\n",
      "\n",
      "Test set: Avg. loss: 7546152.8870, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 735 [0/869 (0%)]\tLoss: 96278808.000000\n",
      "Train Epoch: 735 [160/869 (18%)]\tLoss: 169196384.000000\n",
      "Train Epoch: 735 [320/869 (36%)]\tLoss: 101436440.000000\n",
      "Train Epoch: 735 [480/869 (55%)]\tLoss: 42286468.000000\n",
      "Train Epoch: 735 [640/869 (73%)]\tLoss: 216791120.000000\n",
      "Train Epoch: 735 [800/869 (91%)]\tLoss: 155051216.000000\n",
      "\n",
      "Test set: Avg. loss: 7401574.2175, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 736 [0/869 (0%)]\tLoss: 59489040.000000\n",
      "Train Epoch: 736 [160/869 (18%)]\tLoss: 100287720.000000\n",
      "Train Epoch: 736 [320/869 (36%)]\tLoss: 182624800.000000\n",
      "Train Epoch: 736 [480/869 (55%)]\tLoss: 39032588.000000\n",
      "Train Epoch: 736 [640/869 (73%)]\tLoss: 144934320.000000\n",
      "Train Epoch: 736 [800/869 (91%)]\tLoss: 209927424.000000\n",
      "\n",
      "Test set: Avg. loss: 7302015.9062, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 737 [0/869 (0%)]\tLoss: 121648040.000000\n",
      "Train Epoch: 737 [160/869 (18%)]\tLoss: 144524288.000000\n",
      "Train Epoch: 737 [320/869 (36%)]\tLoss: 89900400.000000\n",
      "Train Epoch: 737 [480/869 (55%)]\tLoss: 263454736.000000\n",
      "Train Epoch: 737 [640/869 (73%)]\tLoss: 90368328.000000\n",
      "Train Epoch: 737 [800/869 (91%)]\tLoss: 28059724.000000\n",
      "\n",
      "Test set: Avg. loss: 7776723.3433, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 738 [0/869 (0%)]\tLoss: 108732240.000000\n",
      "Train Epoch: 738 [160/869 (18%)]\tLoss: 133634360.000000\n",
      "Train Epoch: 738 [320/869 (36%)]\tLoss: 165962464.000000\n",
      "Train Epoch: 738 [480/869 (55%)]\tLoss: 116123696.000000\n",
      "Train Epoch: 738 [640/869 (73%)]\tLoss: 76074288.000000\n",
      "Train Epoch: 738 [800/869 (91%)]\tLoss: 44604248.000000\n",
      "\n",
      "Test set: Avg. loss: 7418731.5480, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 739 [0/869 (0%)]\tLoss: 58376312.000000\n",
      "Train Epoch: 739 [160/869 (18%)]\tLoss: 132413120.000000\n",
      "Train Epoch: 739 [320/869 (36%)]\tLoss: 172221552.000000\n",
      "Train Epoch: 739 [480/869 (55%)]\tLoss: 29442920.000000\n",
      "Train Epoch: 739 [640/869 (73%)]\tLoss: 86920104.000000\n",
      "Train Epoch: 739 [800/869 (91%)]\tLoss: 142755120.000000\n",
      "\n",
      "Test set: Avg. loss: 7771672.9638, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 740 [0/869 (0%)]\tLoss: 211438384.000000\n",
      "Train Epoch: 740 [160/869 (18%)]\tLoss: 162541248.000000\n",
      "Train Epoch: 740 [320/869 (36%)]\tLoss: 130323328.000000\n",
      "Train Epoch: 740 [480/869 (55%)]\tLoss: 110861032.000000\n",
      "Train Epoch: 740 [640/869 (73%)]\tLoss: 141906080.000000\n",
      "Train Epoch: 740 [800/869 (91%)]\tLoss: 109624552.000000\n",
      "\n",
      "Test set: Avg. loss: 7991330.0896, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 741 [0/869 (0%)]\tLoss: 106400096.000000\n",
      "Train Epoch: 741 [160/869 (18%)]\tLoss: 148349776.000000\n",
      "Train Epoch: 741 [320/869 (36%)]\tLoss: 43351084.000000\n",
      "Train Epoch: 741 [480/869 (55%)]\tLoss: 129090352.000000\n",
      "Train Epoch: 741 [640/869 (73%)]\tLoss: 139988832.000000\n",
      "Train Epoch: 741 [800/869 (91%)]\tLoss: 101454864.000000\n",
      "\n",
      "Test set: Avg. loss: 7543805.0149, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 742 [0/869 (0%)]\tLoss: 92091864.000000\n",
      "Train Epoch: 742 [160/869 (18%)]\tLoss: 290693152.000000\n",
      "Train Epoch: 742 [320/869 (36%)]\tLoss: 81874208.000000\n",
      "Train Epoch: 742 [480/869 (55%)]\tLoss: 28663124.000000\n",
      "Train Epoch: 742 [640/869 (73%)]\tLoss: 121979328.000000\n",
      "Train Epoch: 742 [800/869 (91%)]\tLoss: 83538544.000000\n",
      "\n",
      "Test set: Avg. loss: 7335344.3753, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 743 [0/869 (0%)]\tLoss: 105444288.000000\n",
      "Train Epoch: 743 [160/869 (18%)]\tLoss: 137313536.000000\n",
      "Train Epoch: 743 [320/869 (36%)]\tLoss: 190627184.000000\n",
      "Train Epoch: 743 [480/869 (55%)]\tLoss: 212047616.000000\n",
      "Train Epoch: 743 [640/869 (73%)]\tLoss: 58798060.000000\n",
      "Train Epoch: 743 [800/869 (91%)]\tLoss: 74639688.000000\n",
      "\n",
      "Test set: Avg. loss: 7419428.5544, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 744 [0/869 (0%)]\tLoss: 40716616.000000\n",
      "Train Epoch: 744 [160/869 (18%)]\tLoss: 64672548.000000\n",
      "Train Epoch: 744 [320/869 (36%)]\tLoss: 123830936.000000\n",
      "Train Epoch: 744 [480/869 (55%)]\tLoss: 122674992.000000\n",
      "Train Epoch: 744 [640/869 (73%)]\tLoss: 142823680.000000\n",
      "Train Epoch: 744 [800/869 (91%)]\tLoss: 315880128.000000\n",
      "\n",
      "Test set: Avg. loss: 7802497.0576, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 745 [0/869 (0%)]\tLoss: 142996688.000000\n",
      "Train Epoch: 745 [160/869 (18%)]\tLoss: 272021568.000000\n",
      "Train Epoch: 745 [320/869 (36%)]\tLoss: 202368896.000000\n",
      "Train Epoch: 745 [480/869 (55%)]\tLoss: 212878176.000000\n",
      "Train Epoch: 745 [640/869 (73%)]\tLoss: 166999312.000000\n",
      "Train Epoch: 745 [800/869 (91%)]\tLoss: 117932880.000000\n",
      "\n",
      "Test set: Avg. loss: 7407325.6034, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 746 [0/869 (0%)]\tLoss: 204504144.000000\n",
      "Train Epoch: 746 [160/869 (18%)]\tLoss: 80734512.000000\n",
      "Train Epoch: 746 [320/869 (36%)]\tLoss: 48957304.000000\n",
      "Train Epoch: 746 [480/869 (55%)]\tLoss: 126141080.000000\n",
      "Train Epoch: 746 [640/869 (73%)]\tLoss: 86024888.000000\n",
      "Train Epoch: 746 [800/869 (91%)]\tLoss: 103027944.000000\n",
      "\n",
      "Test set: Avg. loss: 7377401.7655, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 747 [0/869 (0%)]\tLoss: 84725080.000000\n",
      "Train Epoch: 747 [160/869 (18%)]\tLoss: 173002080.000000\n",
      "Train Epoch: 747 [320/869 (36%)]\tLoss: 144046224.000000\n",
      "Train Epoch: 747 [480/869 (55%)]\tLoss: 17595608.000000\n",
      "Train Epoch: 747 [640/869 (73%)]\tLoss: 114077168.000000\n",
      "Train Epoch: 747 [800/869 (91%)]\tLoss: 90240384.000000\n",
      "\n",
      "Test set: Avg. loss: 7349064.7079, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 748 [0/869 (0%)]\tLoss: 107862144.000000\n",
      "Train Epoch: 748 [160/869 (18%)]\tLoss: 121356632.000000\n",
      "Train Epoch: 748 [320/869 (36%)]\tLoss: 180906640.000000\n",
      "Train Epoch: 748 [480/869 (55%)]\tLoss: 101043176.000000\n",
      "Train Epoch: 748 [640/869 (73%)]\tLoss: 189824448.000000\n",
      "Train Epoch: 748 [800/869 (91%)]\tLoss: 172039680.000000\n",
      "\n",
      "Test set: Avg. loss: 7398714.4009, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 749 [0/869 (0%)]\tLoss: 128700416.000000\n",
      "Train Epoch: 749 [160/869 (18%)]\tLoss: 157759920.000000\n",
      "Train Epoch: 749 [320/869 (36%)]\tLoss: 153499664.000000\n",
      "Train Epoch: 749 [480/869 (55%)]\tLoss: 69622912.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 749 [640/869 (73%)]\tLoss: 249640528.000000\n",
      "Train Epoch: 749 [800/869 (91%)]\tLoss: 101141944.000000\n",
      "\n",
      "Test set: Avg. loss: 7648200.0938, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 750 [0/869 (0%)]\tLoss: 88387952.000000\n",
      "Train Epoch: 750 [160/869 (18%)]\tLoss: 84241712.000000\n",
      "Train Epoch: 750 [320/869 (36%)]\tLoss: 102963968.000000\n",
      "Train Epoch: 750 [480/869 (55%)]\tLoss: 149714368.000000\n",
      "Train Epoch: 750 [640/869 (73%)]\tLoss: 130468768.000000\n",
      "Train Epoch: 750 [800/869 (91%)]\tLoss: 389293536.000000\n",
      "\n",
      "Test set: Avg. loss: 7630892.1365, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 751 [0/869 (0%)]\tLoss: 97190704.000000\n",
      "Train Epoch: 751 [160/869 (18%)]\tLoss: 69993424.000000\n",
      "Train Epoch: 751 [320/869 (36%)]\tLoss: 81605864.000000\n",
      "Train Epoch: 751 [480/869 (55%)]\tLoss: 89498936.000000\n",
      "Train Epoch: 751 [640/869 (73%)]\tLoss: 97159512.000000\n",
      "Train Epoch: 751 [800/869 (91%)]\tLoss: 84421584.000000\n",
      "\n",
      "Test set: Avg. loss: 7350212.6823, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 752 [0/869 (0%)]\tLoss: 137474592.000000\n",
      "Train Epoch: 752 [160/869 (18%)]\tLoss: 47897264.000000\n",
      "Train Epoch: 752 [320/869 (36%)]\tLoss: 292555072.000000\n",
      "Train Epoch: 752 [480/869 (55%)]\tLoss: 275169728.000000\n",
      "Train Epoch: 752 [640/869 (73%)]\tLoss: 338714048.000000\n",
      "Train Epoch: 752 [800/869 (91%)]\tLoss: 166597888.000000\n",
      "\n",
      "Test set: Avg. loss: 8080460.6482, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 753 [0/869 (0%)]\tLoss: 30031096.000000\n",
      "Train Epoch: 753 [160/869 (18%)]\tLoss: 95081464.000000\n",
      "Train Epoch: 753 [320/869 (36%)]\tLoss: 189863408.000000\n",
      "Train Epoch: 753 [480/869 (55%)]\tLoss: 147521424.000000\n",
      "Train Epoch: 753 [640/869 (73%)]\tLoss: 44550620.000000\n",
      "Train Epoch: 753 [800/869 (91%)]\tLoss: 111398400.000000\n",
      "\n",
      "Test set: Avg. loss: 7308013.3902, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 754 [0/869 (0%)]\tLoss: 77661704.000000\n",
      "Train Epoch: 754 [160/869 (18%)]\tLoss: 39109388.000000\n",
      "Train Epoch: 754 [320/869 (36%)]\tLoss: 135065920.000000\n",
      "Train Epoch: 754 [480/869 (55%)]\tLoss: 144142832.000000\n",
      "Train Epoch: 754 [640/869 (73%)]\tLoss: 118202208.000000\n",
      "Train Epoch: 754 [800/869 (91%)]\tLoss: 122790160.000000\n",
      "\n",
      "Test set: Avg. loss: 7416363.6418, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 755 [0/869 (0%)]\tLoss: 165785856.000000\n",
      "Train Epoch: 755 [160/869 (18%)]\tLoss: 34339204.000000\n",
      "Train Epoch: 755 [320/869 (36%)]\tLoss: 214309632.000000\n",
      "Train Epoch: 755 [480/869 (55%)]\tLoss: 230304880.000000\n",
      "Train Epoch: 755 [640/869 (73%)]\tLoss: 189050544.000000\n",
      "Train Epoch: 755 [800/869 (91%)]\tLoss: 131735960.000000\n",
      "\n",
      "Test set: Avg. loss: 7308209.0021, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 756 [0/869 (0%)]\tLoss: 78434864.000000\n",
      "Train Epoch: 756 [160/869 (18%)]\tLoss: 69939792.000000\n",
      "Train Epoch: 756 [320/869 (36%)]\tLoss: 51669064.000000\n",
      "Train Epoch: 756 [480/869 (55%)]\tLoss: 98354416.000000\n",
      "Train Epoch: 756 [640/869 (73%)]\tLoss: 128575216.000000\n",
      "Train Epoch: 756 [800/869 (91%)]\tLoss: 126950544.000000\n",
      "\n",
      "Test set: Avg. loss: 7478632.2473, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 757 [0/869 (0%)]\tLoss: 167334880.000000\n",
      "Train Epoch: 757 [160/869 (18%)]\tLoss: 197389056.000000\n",
      "Train Epoch: 757 [320/869 (36%)]\tLoss: 86095760.000000\n",
      "Train Epoch: 757 [480/869 (55%)]\tLoss: 106326272.000000\n",
      "Train Epoch: 757 [640/869 (73%)]\tLoss: 48038996.000000\n",
      "Train Epoch: 757 [800/869 (91%)]\tLoss: 271273952.000000\n",
      "\n",
      "Test set: Avg. loss: 7497946.3198, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 758 [0/869 (0%)]\tLoss: 107267312.000000\n",
      "Train Epoch: 758 [160/869 (18%)]\tLoss: 228963360.000000\n",
      "Train Epoch: 758 [320/869 (36%)]\tLoss: 81190224.000000\n",
      "Train Epoch: 758 [480/869 (55%)]\tLoss: 156268320.000000\n",
      "Train Epoch: 758 [640/869 (73%)]\tLoss: 137286016.000000\n",
      "Train Epoch: 758 [800/869 (91%)]\tLoss: 85470976.000000\n",
      "\n",
      "Test set: Avg. loss: 8309088.7377, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 759 [0/869 (0%)]\tLoss: 78495048.000000\n",
      "Train Epoch: 759 [160/869 (18%)]\tLoss: 138222608.000000\n",
      "Train Epoch: 759 [320/869 (36%)]\tLoss: 83212752.000000\n",
      "Train Epoch: 759 [480/869 (55%)]\tLoss: 33263736.000000\n",
      "Train Epoch: 759 [640/869 (73%)]\tLoss: 167114304.000000\n",
      "Train Epoch: 759 [800/869 (91%)]\tLoss: 215763376.000000\n",
      "\n",
      "Test set: Avg. loss: 7663289.6546, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 760 [0/869 (0%)]\tLoss: 230931584.000000\n",
      "Train Epoch: 760 [160/869 (18%)]\tLoss: 25697894.000000\n",
      "Train Epoch: 760 [320/869 (36%)]\tLoss: 43428512.000000\n",
      "Train Epoch: 760 [480/869 (55%)]\tLoss: 87301312.000000\n",
      "Train Epoch: 760 [640/869 (73%)]\tLoss: 208693792.000000\n",
      "Train Epoch: 760 [800/869 (91%)]\tLoss: 139480832.000000\n",
      "\n",
      "Test set: Avg. loss: 7360162.4989, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 761 [0/869 (0%)]\tLoss: 126576336.000000\n",
      "Train Epoch: 761 [160/869 (18%)]\tLoss: 236530528.000000\n",
      "Train Epoch: 761 [320/869 (36%)]\tLoss: 140676288.000000\n",
      "Train Epoch: 761 [480/869 (55%)]\tLoss: 133348224.000000\n",
      "Train Epoch: 761 [640/869 (73%)]\tLoss: 162779744.000000\n",
      "Train Epoch: 761 [800/869 (91%)]\tLoss: 91999288.000000\n",
      "\n",
      "Test set: Avg. loss: 7562993.1173, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 762 [0/869 (0%)]\tLoss: 101106176.000000\n",
      "Train Epoch: 762 [160/869 (18%)]\tLoss: 115900200.000000\n",
      "Train Epoch: 762 [320/869 (36%)]\tLoss: 87716352.000000\n",
      "Train Epoch: 762 [480/869 (55%)]\tLoss: 197543072.000000\n",
      "Train Epoch: 762 [640/869 (73%)]\tLoss: 104633648.000000\n",
      "Train Epoch: 762 [800/869 (91%)]\tLoss: 239628032.000000\n",
      "\n",
      "Test set: Avg. loss: 7306596.8870, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 763 [0/869 (0%)]\tLoss: 103341864.000000\n",
      "Train Epoch: 763 [160/869 (18%)]\tLoss: 89353288.000000\n",
      "Train Epoch: 763 [320/869 (36%)]\tLoss: 163943760.000000\n",
      "Train Epoch: 763 [480/869 (55%)]\tLoss: 80852720.000000\n",
      "Train Epoch: 763 [640/869 (73%)]\tLoss: 97169720.000000\n",
      "Train Epoch: 763 [800/869 (91%)]\tLoss: 95209912.000000\n",
      "\n",
      "Test set: Avg. loss: 7668135.6759, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 764 [0/869 (0%)]\tLoss: 36891280.000000\n",
      "Train Epoch: 764 [160/869 (18%)]\tLoss: 244036576.000000\n",
      "Train Epoch: 764 [320/869 (36%)]\tLoss: 129352120.000000\n",
      "Train Epoch: 764 [480/869 (55%)]\tLoss: 136226464.000000\n",
      "Train Epoch: 764 [640/869 (73%)]\tLoss: 176264560.000000\n",
      "Train Epoch: 764 [800/869 (91%)]\tLoss: 136340080.000000\n",
      "\n",
      "Test set: Avg. loss: 7621371.4733, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 765 [0/869 (0%)]\tLoss: 251686096.000000\n",
      "Train Epoch: 765 [160/869 (18%)]\tLoss: 35012432.000000\n",
      "Train Epoch: 765 [320/869 (36%)]\tLoss: 78366952.000000\n",
      "Train Epoch: 765 [480/869 (55%)]\tLoss: 184393760.000000\n",
      "Train Epoch: 765 [640/869 (73%)]\tLoss: 52655236.000000\n",
      "Train Epoch: 765 [800/869 (91%)]\tLoss: 55182796.000000\n",
      "\n",
      "Test set: Avg. loss: 8208979.4627, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 766 [0/869 (0%)]\tLoss: 94800528.000000\n",
      "Train Epoch: 766 [160/869 (18%)]\tLoss: 221168480.000000\n",
      "Train Epoch: 766 [320/869 (36%)]\tLoss: 153206272.000000\n",
      "Train Epoch: 766 [480/869 (55%)]\tLoss: 128511160.000000\n",
      "Train Epoch: 766 [640/869 (73%)]\tLoss: 90537896.000000\n",
      "Train Epoch: 766 [800/869 (91%)]\tLoss: 187276864.000000\n",
      "\n",
      "Test set: Avg. loss: 7406624.3326, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 767 [0/869 (0%)]\tLoss: 189519072.000000\n",
      "Train Epoch: 767 [160/869 (18%)]\tLoss: 139516944.000000\n",
      "Train Epoch: 767 [320/869 (36%)]\tLoss: 96937440.000000\n",
      "Train Epoch: 767 [480/869 (55%)]\tLoss: 89937568.000000\n",
      "Train Epoch: 767 [640/869 (73%)]\tLoss: 89516048.000000\n",
      "Train Epoch: 767 [800/869 (91%)]\tLoss: 166884112.000000\n",
      "\n",
      "Test set: Avg. loss: 7401225.6802, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 768 [0/869 (0%)]\tLoss: 110856784.000000\n",
      "Train Epoch: 768 [160/869 (18%)]\tLoss: 73842096.000000\n",
      "Train Epoch: 768 [320/869 (36%)]\tLoss: 90922808.000000\n",
      "Train Epoch: 768 [480/869 (55%)]\tLoss: 333097280.000000\n",
      "Train Epoch: 768 [640/869 (73%)]\tLoss: 101756552.000000\n",
      "Train Epoch: 768 [800/869 (91%)]\tLoss: 57045640.000000\n",
      "\n",
      "Test set: Avg. loss: 7382415.5480, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 769 [0/869 (0%)]\tLoss: 192573744.000000\n",
      "Train Epoch: 769 [160/869 (18%)]\tLoss: 124011992.000000\n",
      "Train Epoch: 769 [320/869 (36%)]\tLoss: 45929200.000000\n",
      "Train Epoch: 769 [480/869 (55%)]\tLoss: 104005600.000000\n",
      "Train Epoch: 769 [640/869 (73%)]\tLoss: 261291968.000000\n",
      "Train Epoch: 769 [800/869 (91%)]\tLoss: 183354544.000000\n",
      "\n",
      "Test set: Avg. loss: 7581919.9232, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 770 [0/869 (0%)]\tLoss: 226456816.000000\n",
      "Train Epoch: 770 [160/869 (18%)]\tLoss: 113573104.000000\n",
      "Train Epoch: 770 [320/869 (36%)]\tLoss: 87437296.000000\n",
      "Train Epoch: 770 [480/869 (55%)]\tLoss: 144828368.000000\n",
      "Train Epoch: 770 [640/869 (73%)]\tLoss: 96906904.000000\n",
      "Train Epoch: 770 [800/869 (91%)]\tLoss: 182142496.000000\n",
      "\n",
      "Test set: Avg. loss: 7661955.8038, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 771 [0/869 (0%)]\tLoss: 226049152.000000\n",
      "Train Epoch: 771 [160/869 (18%)]\tLoss: 153456304.000000\n",
      "Train Epoch: 771 [320/869 (36%)]\tLoss: 132975616.000000\n",
      "Train Epoch: 771 [480/869 (55%)]\tLoss: 215235056.000000\n",
      "Train Epoch: 771 [640/869 (73%)]\tLoss: 76174032.000000\n",
      "Train Epoch: 771 [800/869 (91%)]\tLoss: 216560976.000000\n",
      "\n",
      "Test set: Avg. loss: 7597821.0320, Accuracy: 0/469 (0%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 772 [0/869 (0%)]\tLoss: 146892896.000000\n",
      "Train Epoch: 772 [160/869 (18%)]\tLoss: 122481816.000000\n",
      "Train Epoch: 772 [320/869 (36%)]\tLoss: 173223520.000000\n",
      "Train Epoch: 772 [480/869 (55%)]\tLoss: 140560832.000000\n",
      "Train Epoch: 772 [640/869 (73%)]\tLoss: 182924800.000000\n",
      "Train Epoch: 772 [800/869 (91%)]\tLoss: 154413792.000000\n",
      "\n",
      "Test set: Avg. loss: 7317721.3902, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 773 [0/869 (0%)]\tLoss: 128690488.000000\n",
      "Train Epoch: 773 [160/869 (18%)]\tLoss: 95903936.000000\n",
      "Train Epoch: 773 [320/869 (36%)]\tLoss: 88159752.000000\n",
      "Train Epoch: 773 [480/869 (55%)]\tLoss: 145364528.000000\n",
      "Train Epoch: 773 [640/869 (73%)]\tLoss: 75540248.000000\n",
      "Train Epoch: 773 [800/869 (91%)]\tLoss: 54367096.000000\n",
      "\n",
      "Test set: Avg. loss: 7475111.8635, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 774 [0/869 (0%)]\tLoss: 35474892.000000\n",
      "Train Epoch: 774 [160/869 (18%)]\tLoss: 99576448.000000\n",
      "Train Epoch: 774 [320/869 (36%)]\tLoss: 209986000.000000\n",
      "Train Epoch: 774 [480/869 (55%)]\tLoss: 32300438.000000\n",
      "Train Epoch: 774 [640/869 (73%)]\tLoss: 114865920.000000\n",
      "Train Epoch: 774 [800/869 (91%)]\tLoss: 172965824.000000\n",
      "\n",
      "Test set: Avg. loss: 7403242.8486, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 775 [0/869 (0%)]\tLoss: 132858368.000000\n",
      "Train Epoch: 775 [160/869 (18%)]\tLoss: 113415944.000000\n",
      "Train Epoch: 775 [320/869 (36%)]\tLoss: 206351312.000000\n",
      "Train Epoch: 775 [480/869 (55%)]\tLoss: 194125840.000000\n",
      "Train Epoch: 775 [640/869 (73%)]\tLoss: 50170556.000000\n",
      "Train Epoch: 775 [800/869 (91%)]\tLoss: 81209936.000000\n",
      "\n",
      "Test set: Avg. loss: 7510187.1727, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 776 [0/869 (0%)]\tLoss: 39658996.000000\n",
      "Train Epoch: 776 [160/869 (18%)]\tLoss: 167905344.000000\n",
      "Train Epoch: 776 [320/869 (36%)]\tLoss: 84211872.000000\n",
      "Train Epoch: 776 [480/869 (55%)]\tLoss: 124379344.000000\n",
      "Train Epoch: 776 [640/869 (73%)]\tLoss: 183528960.000000\n",
      "Train Epoch: 776 [800/869 (91%)]\tLoss: 124661416.000000\n",
      "\n",
      "Test set: Avg. loss: 7983121.9787, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 777 [0/869 (0%)]\tLoss: 146740112.000000\n",
      "Train Epoch: 777 [160/869 (18%)]\tLoss: 188856128.000000\n",
      "Train Epoch: 777 [320/869 (36%)]\tLoss: 102276192.000000\n",
      "Train Epoch: 777 [480/869 (55%)]\tLoss: 41431796.000000\n",
      "Train Epoch: 777 [640/869 (73%)]\tLoss: 76318152.000000\n",
      "Train Epoch: 777 [800/869 (91%)]\tLoss: 213811600.000000\n",
      "\n",
      "Test set: Avg. loss: 7302670.8316, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 778 [0/869 (0%)]\tLoss: 192194320.000000\n",
      "Train Epoch: 778 [160/869 (18%)]\tLoss: 270457120.000000\n",
      "Train Epoch: 778 [320/869 (36%)]\tLoss: 57710056.000000\n",
      "Train Epoch: 778 [480/869 (55%)]\tLoss: 127656896.000000\n",
      "Train Epoch: 778 [640/869 (73%)]\tLoss: 97848032.000000\n",
      "Train Epoch: 778 [800/869 (91%)]\tLoss: 177058816.000000\n",
      "\n",
      "Test set: Avg. loss: 7406049.6631, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 779 [0/869 (0%)]\tLoss: 151247616.000000\n",
      "Train Epoch: 779 [160/869 (18%)]\tLoss: 29513646.000000\n",
      "Train Epoch: 779 [320/869 (36%)]\tLoss: 124986272.000000\n",
      "Train Epoch: 779 [480/869 (55%)]\tLoss: 131590864.000000\n",
      "Train Epoch: 779 [640/869 (73%)]\tLoss: 200371776.000000\n",
      "Train Epoch: 779 [800/869 (91%)]\tLoss: 153814000.000000\n",
      "\n",
      "Test set: Avg. loss: 7310835.6844, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 780 [0/869 (0%)]\tLoss: 49174372.000000\n",
      "Train Epoch: 780 [160/869 (18%)]\tLoss: 91462968.000000\n",
      "Train Epoch: 780 [320/869 (36%)]\tLoss: 41488656.000000\n",
      "Train Epoch: 780 [480/869 (55%)]\tLoss: 116903256.000000\n",
      "Train Epoch: 780 [640/869 (73%)]\tLoss: 49501072.000000\n",
      "Train Epoch: 780 [800/869 (91%)]\tLoss: 33915312.000000\n",
      "\n",
      "Test set: Avg. loss: 7327111.0789, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 781 [0/869 (0%)]\tLoss: 82063824.000000\n",
      "Train Epoch: 781 [160/869 (18%)]\tLoss: 111466288.000000\n",
      "Train Epoch: 781 [320/869 (36%)]\tLoss: 86525744.000000\n",
      "Train Epoch: 781 [480/869 (55%)]\tLoss: 157532528.000000\n",
      "Train Epoch: 781 [640/869 (73%)]\tLoss: 195769120.000000\n",
      "Train Epoch: 781 [800/869 (91%)]\tLoss: 241594896.000000\n",
      "\n",
      "Test set: Avg. loss: 7512922.4563, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 782 [0/869 (0%)]\tLoss: 319800960.000000\n",
      "Train Epoch: 782 [160/869 (18%)]\tLoss: 60584320.000000\n",
      "Train Epoch: 782 [320/869 (36%)]\tLoss: 69255448.000000\n",
      "Train Epoch: 782 [480/869 (55%)]\tLoss: 95167144.000000\n",
      "Train Epoch: 782 [640/869 (73%)]\tLoss: 149604192.000000\n",
      "Train Epoch: 782 [800/869 (91%)]\tLoss: 50843808.000000\n",
      "\n",
      "Test set: Avg. loss: 7312668.2132, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 783 [0/869 (0%)]\tLoss: 46663828.000000\n",
      "Train Epoch: 783 [160/869 (18%)]\tLoss: 101417008.000000\n",
      "Train Epoch: 783 [320/869 (36%)]\tLoss: 176804736.000000\n",
      "Train Epoch: 783 [480/869 (55%)]\tLoss: 55193688.000000\n",
      "Train Epoch: 783 [640/869 (73%)]\tLoss: 125569328.000000\n",
      "Train Epoch: 783 [800/869 (91%)]\tLoss: 66577424.000000\n",
      "\n",
      "Test set: Avg. loss: 8172674.7804, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 784 [0/869 (0%)]\tLoss: 89772464.000000\n",
      "Train Epoch: 784 [160/869 (18%)]\tLoss: 158054464.000000\n",
      "Train Epoch: 784 [320/869 (36%)]\tLoss: 178007120.000000\n",
      "Train Epoch: 784 [480/869 (55%)]\tLoss: 474137088.000000\n",
      "Train Epoch: 784 [640/869 (73%)]\tLoss: 61008756.000000\n",
      "Train Epoch: 784 [800/869 (91%)]\tLoss: 85505144.000000\n",
      "\n",
      "Test set: Avg. loss: 7915191.6503, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 785 [0/869 (0%)]\tLoss: 78895800.000000\n",
      "Train Epoch: 785 [160/869 (18%)]\tLoss: 173008560.000000\n",
      "Train Epoch: 785 [320/869 (36%)]\tLoss: 141715136.000000\n",
      "Train Epoch: 785 [480/869 (55%)]\tLoss: 224366656.000000\n",
      "Train Epoch: 785 [640/869 (73%)]\tLoss: 90267328.000000\n",
      "Train Epoch: 785 [800/869 (91%)]\tLoss: 100011616.000000\n",
      "\n",
      "Test set: Avg. loss: 7577828.2473, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 786 [0/869 (0%)]\tLoss: 268925888.000000\n",
      "Train Epoch: 786 [160/869 (18%)]\tLoss: 46231172.000000\n",
      "Train Epoch: 786 [320/869 (36%)]\tLoss: 79811776.000000\n",
      "Train Epoch: 786 [480/869 (55%)]\tLoss: 52155336.000000\n",
      "Train Epoch: 786 [640/869 (73%)]\tLoss: 228571680.000000\n",
      "Train Epoch: 786 [800/869 (91%)]\tLoss: 222930208.000000\n",
      "\n",
      "Test set: Avg. loss: 7495055.7868, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 787 [0/869 (0%)]\tLoss: 91406160.000000\n",
      "Train Epoch: 787 [160/869 (18%)]\tLoss: 104913744.000000\n",
      "Train Epoch: 787 [320/869 (36%)]\tLoss: 163222448.000000\n",
      "Train Epoch: 787 [480/869 (55%)]\tLoss: 77787824.000000\n",
      "Train Epoch: 787 [640/869 (73%)]\tLoss: 93664488.000000\n",
      "Train Epoch: 787 [800/869 (91%)]\tLoss: 76435144.000000\n",
      "\n",
      "Test set: Avg. loss: 7698689.7484, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 788 [0/869 (0%)]\tLoss: 198791680.000000\n",
      "Train Epoch: 788 [160/869 (18%)]\tLoss: 77189376.000000\n",
      "Train Epoch: 788 [320/869 (36%)]\tLoss: 81769440.000000\n",
      "Train Epoch: 788 [480/869 (55%)]\tLoss: 89234320.000000\n",
      "Train Epoch: 788 [640/869 (73%)]\tLoss: 212901568.000000\n",
      "Train Epoch: 788 [800/869 (91%)]\tLoss: 153974848.000000\n",
      "\n",
      "Test set: Avg. loss: 7466186.9424, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 789 [0/869 (0%)]\tLoss: 199354640.000000\n",
      "Train Epoch: 789 [160/869 (18%)]\tLoss: 44888336.000000\n",
      "Train Epoch: 789 [320/869 (36%)]\tLoss: 117704480.000000\n",
      "Train Epoch: 789 [480/869 (55%)]\tLoss: 151616480.000000\n",
      "Train Epoch: 789 [640/869 (73%)]\tLoss: 113804992.000000\n",
      "Train Epoch: 789 [800/869 (91%)]\tLoss: 176764432.000000\n",
      "\n",
      "Test set: Avg. loss: 7887486.5416, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 790 [0/869 (0%)]\tLoss: 32745980.000000\n",
      "Train Epoch: 790 [160/869 (18%)]\tLoss: 242075328.000000\n",
      "Train Epoch: 790 [320/869 (36%)]\tLoss: 101850000.000000\n",
      "Train Epoch: 790 [480/869 (55%)]\tLoss: 76604104.000000\n",
      "Train Epoch: 790 [640/869 (73%)]\tLoss: 42139832.000000\n",
      "Train Epoch: 790 [800/869 (91%)]\tLoss: 91008904.000000\n",
      "\n",
      "Test set: Avg. loss: 7641936.7676, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 791 [0/869 (0%)]\tLoss: 197554592.000000\n",
      "Train Epoch: 791 [160/869 (18%)]\tLoss: 32627056.000000\n",
      "Train Epoch: 791 [320/869 (36%)]\tLoss: 133712656.000000\n",
      "Train Epoch: 791 [480/869 (55%)]\tLoss: 148450288.000000\n",
      "Train Epoch: 791 [640/869 (73%)]\tLoss: 226909264.000000\n",
      "Train Epoch: 791 [800/869 (91%)]\tLoss: 53997080.000000\n",
      "\n",
      "Test set: Avg. loss: 7388235.5480, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 792 [0/869 (0%)]\tLoss: 89031872.000000\n",
      "Train Epoch: 792 [160/869 (18%)]\tLoss: 182272352.000000\n",
      "Train Epoch: 792 [320/869 (36%)]\tLoss: 62449848.000000\n",
      "Train Epoch: 792 [480/869 (55%)]\tLoss: 234065632.000000\n",
      "Train Epoch: 792 [640/869 (73%)]\tLoss: 145633104.000000\n",
      "Train Epoch: 792 [800/869 (91%)]\tLoss: 122840632.000000\n",
      "\n",
      "Test set: Avg. loss: 7447363.9829, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 793 [0/869 (0%)]\tLoss: 83347496.000000\n",
      "Train Epoch: 793 [160/869 (18%)]\tLoss: 93994912.000000\n",
      "Train Epoch: 793 [320/869 (36%)]\tLoss: 99340800.000000\n",
      "Train Epoch: 793 [480/869 (55%)]\tLoss: 82481712.000000\n",
      "Train Epoch: 793 [640/869 (73%)]\tLoss: 96990720.000000\n",
      "Train Epoch: 793 [800/869 (91%)]\tLoss: 236832960.000000\n",
      "\n",
      "Test set: Avg. loss: 8008295.5181, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 794 [0/869 (0%)]\tLoss: 49821976.000000\n",
      "Train Epoch: 794 [160/869 (18%)]\tLoss: 50534472.000000\n",
      "Train Epoch: 794 [320/869 (36%)]\tLoss: 230532576.000000\n",
      "Train Epoch: 794 [480/869 (55%)]\tLoss: 173131616.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 794 [640/869 (73%)]\tLoss: 42964288.000000\n",
      "Train Epoch: 794 [800/869 (91%)]\tLoss: 122909808.000000\n",
      "\n",
      "Test set: Avg. loss: 7569894.3625, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 795 [0/869 (0%)]\tLoss: 165115360.000000\n",
      "Train Epoch: 795 [160/869 (18%)]\tLoss: 101006352.000000\n",
      "Train Epoch: 795 [320/869 (36%)]\tLoss: 127440664.000000\n",
      "Train Epoch: 795 [480/869 (55%)]\tLoss: 155503088.000000\n",
      "Train Epoch: 795 [640/869 (73%)]\tLoss: 130164912.000000\n",
      "Train Epoch: 795 [800/869 (91%)]\tLoss: 100477408.000000\n",
      "\n",
      "Test set: Avg. loss: 7995044.8486, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 796 [0/869 (0%)]\tLoss: 26318990.000000\n",
      "Train Epoch: 796 [160/869 (18%)]\tLoss: 127649368.000000\n",
      "Train Epoch: 796 [320/869 (36%)]\tLoss: 144394272.000000\n",
      "Train Epoch: 796 [480/869 (55%)]\tLoss: 82264080.000000\n",
      "Train Epoch: 796 [640/869 (73%)]\tLoss: 105956840.000000\n",
      "Train Epoch: 796 [800/869 (91%)]\tLoss: 140121792.000000\n",
      "\n",
      "Test set: Avg. loss: 7673221.6034, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 797 [0/869 (0%)]\tLoss: 78212352.000000\n",
      "Train Epoch: 797 [160/869 (18%)]\tLoss: 43630200.000000\n",
      "Train Epoch: 797 [320/869 (36%)]\tLoss: 210277568.000000\n",
      "Train Epoch: 797 [480/869 (55%)]\tLoss: 93840760.000000\n",
      "Train Epoch: 797 [640/869 (73%)]\tLoss: 238794400.000000\n",
      "Train Epoch: 797 [800/869 (91%)]\tLoss: 232478560.000000\n",
      "\n",
      "Test set: Avg. loss: 7533665.6887, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 798 [0/869 (0%)]\tLoss: 64166576.000000\n",
      "Train Epoch: 798 [160/869 (18%)]\tLoss: 92564896.000000\n",
      "Train Epoch: 798 [320/869 (36%)]\tLoss: 100919680.000000\n",
      "Train Epoch: 798 [480/869 (55%)]\tLoss: 59053148.000000\n",
      "Train Epoch: 798 [640/869 (73%)]\tLoss: 140162800.000000\n",
      "Train Epoch: 798 [800/869 (91%)]\tLoss: 122627640.000000\n",
      "\n",
      "Test set: Avg. loss: 7830536.0171, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 799 [0/869 (0%)]\tLoss: 35005700.000000\n",
      "Train Epoch: 799 [160/869 (18%)]\tLoss: 68510624.000000\n",
      "Train Epoch: 799 [320/869 (36%)]\tLoss: 87515248.000000\n",
      "Train Epoch: 799 [480/869 (55%)]\tLoss: 97318400.000000\n",
      "Train Epoch: 799 [640/869 (73%)]\tLoss: 168556000.000000\n",
      "Train Epoch: 799 [800/869 (91%)]\tLoss: 40925520.000000\n",
      "\n",
      "Test set: Avg. loss: 7325097.4414, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 800 [0/869 (0%)]\tLoss: 156887488.000000\n",
      "Train Epoch: 800 [160/869 (18%)]\tLoss: 176345056.000000\n",
      "Train Epoch: 800 [320/869 (36%)]\tLoss: 94839432.000000\n",
      "Train Epoch: 800 [480/869 (55%)]\tLoss: 90127704.000000\n",
      "Train Epoch: 800 [640/869 (73%)]\tLoss: 40411800.000000\n",
      "Train Epoch: 800 [800/869 (91%)]\tLoss: 186280640.000000\n",
      "\n",
      "Test set: Avg. loss: 7518082.5586, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 801 [0/869 (0%)]\tLoss: 98001744.000000\n",
      "Train Epoch: 801 [160/869 (18%)]\tLoss: 139796064.000000\n",
      "Train Epoch: 801 [320/869 (36%)]\tLoss: 136504528.000000\n",
      "Train Epoch: 801 [480/869 (55%)]\tLoss: 152660272.000000\n",
      "Train Epoch: 801 [640/869 (73%)]\tLoss: 34439648.000000\n",
      "Train Epoch: 801 [800/869 (91%)]\tLoss: 80604840.000000\n",
      "\n",
      "Test set: Avg. loss: 7673925.0064, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 802 [0/869 (0%)]\tLoss: 212076416.000000\n",
      "Train Epoch: 802 [160/869 (18%)]\tLoss: 98188016.000000\n",
      "Train Epoch: 802 [320/869 (36%)]\tLoss: 198181296.000000\n",
      "Train Epoch: 802 [480/869 (55%)]\tLoss: 82241360.000000\n",
      "Train Epoch: 802 [640/869 (73%)]\tLoss: 58430720.000000\n",
      "Train Epoch: 802 [800/869 (91%)]\tLoss: 112176712.000000\n",
      "\n",
      "Test set: Avg. loss: 7351426.6098, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 803 [0/869 (0%)]\tLoss: 93829712.000000\n",
      "Train Epoch: 803 [160/869 (18%)]\tLoss: 94387008.000000\n",
      "Train Epoch: 803 [320/869 (36%)]\tLoss: 81492928.000000\n",
      "Train Epoch: 803 [480/869 (55%)]\tLoss: 311033376.000000\n",
      "Train Epoch: 803 [640/869 (73%)]\tLoss: 100204672.000000\n",
      "Train Epoch: 803 [800/869 (91%)]\tLoss: 248605760.000000\n",
      "\n",
      "Test set: Avg. loss: 7635582.7974, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 804 [0/869 (0%)]\tLoss: 42726100.000000\n",
      "Train Epoch: 804 [160/869 (18%)]\tLoss: 88777368.000000\n",
      "Train Epoch: 804 [320/869 (36%)]\tLoss: 45131876.000000\n",
      "Train Epoch: 804 [480/869 (55%)]\tLoss: 91786752.000000\n",
      "Train Epoch: 804 [640/869 (73%)]\tLoss: 85694944.000000\n",
      "Train Epoch: 804 [800/869 (91%)]\tLoss: 90431728.000000\n",
      "\n",
      "Test set: Avg. loss: 7385216.7249, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 805 [0/869 (0%)]\tLoss: 136719136.000000\n",
      "Train Epoch: 805 [160/869 (18%)]\tLoss: 150811136.000000\n",
      "Train Epoch: 805 [320/869 (36%)]\tLoss: 59423052.000000\n",
      "Train Epoch: 805 [480/869 (55%)]\tLoss: 81459648.000000\n",
      "Train Epoch: 805 [640/869 (73%)]\tLoss: 91278592.000000\n",
      "Train Epoch: 805 [800/869 (91%)]\tLoss: 143347152.000000\n",
      "\n",
      "Test set: Avg. loss: 7725169.0149, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 806 [0/869 (0%)]\tLoss: 83269816.000000\n",
      "Train Epoch: 806 [160/869 (18%)]\tLoss: 66532568.000000\n",
      "Train Epoch: 806 [320/869 (36%)]\tLoss: 204842464.000000\n",
      "Train Epoch: 806 [480/869 (55%)]\tLoss: 162929664.000000\n",
      "Train Epoch: 806 [640/869 (73%)]\tLoss: 51944632.000000\n",
      "Train Epoch: 806 [800/869 (91%)]\tLoss: 155070848.000000\n",
      "\n",
      "Test set: Avg. loss: 7303291.5394, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 807 [0/869 (0%)]\tLoss: 179035680.000000\n",
      "Train Epoch: 807 [160/869 (18%)]\tLoss: 57329952.000000\n",
      "Train Epoch: 807 [320/869 (36%)]\tLoss: 144795744.000000\n",
      "Train Epoch: 807 [480/869 (55%)]\tLoss: 141288784.000000\n",
      "Train Epoch: 807 [640/869 (73%)]\tLoss: 107508344.000000\n",
      "Train Epoch: 807 [800/869 (91%)]\tLoss: 91022960.000000\n",
      "\n",
      "Test set: Avg. loss: 7866392.9979, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 808 [0/869 (0%)]\tLoss: 56635440.000000\n",
      "Train Epoch: 808 [160/869 (18%)]\tLoss: 246965232.000000\n",
      "Train Epoch: 808 [320/869 (36%)]\tLoss: 129717952.000000\n",
      "Train Epoch: 808 [480/869 (55%)]\tLoss: 133517936.000000\n",
      "Train Epoch: 808 [640/869 (73%)]\tLoss: 95677680.000000\n",
      "Train Epoch: 808 [800/869 (91%)]\tLoss: 149494064.000000\n",
      "\n",
      "Test set: Avg. loss: 7505060.1279, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 809 [0/869 (0%)]\tLoss: 125120216.000000\n",
      "Train Epoch: 809 [160/869 (18%)]\tLoss: 146493760.000000\n",
      "Train Epoch: 809 [320/869 (36%)]\tLoss: 131616624.000000\n",
      "Train Epoch: 809 [480/869 (55%)]\tLoss: 102782488.000000\n",
      "Train Epoch: 809 [640/869 (73%)]\tLoss: 39636904.000000\n",
      "Train Epoch: 809 [800/869 (91%)]\tLoss: 107871752.000000\n",
      "\n",
      "Test set: Avg. loss: 7561424.9979, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 810 [0/869 (0%)]\tLoss: 170378848.000000\n",
      "Train Epoch: 810 [160/869 (18%)]\tLoss: 110846744.000000\n",
      "Train Epoch: 810 [320/869 (36%)]\tLoss: 88172872.000000\n",
      "Train Epoch: 810 [480/869 (55%)]\tLoss: 140841520.000000\n",
      "Train Epoch: 810 [640/869 (73%)]\tLoss: 202130928.000000\n",
      "Train Epoch: 810 [800/869 (91%)]\tLoss: 58702404.000000\n",
      "\n",
      "Test set: Avg. loss: 7310466.3795, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 811 [0/869 (0%)]\tLoss: 153894176.000000\n",
      "Train Epoch: 811 [160/869 (18%)]\tLoss: 167758016.000000\n",
      "Train Epoch: 811 [320/869 (36%)]\tLoss: 58449052.000000\n",
      "Train Epoch: 811 [480/869 (55%)]\tLoss: 89920064.000000\n",
      "Train Epoch: 811 [640/869 (73%)]\tLoss: 193217248.000000\n",
      "Train Epoch: 811 [800/869 (91%)]\tLoss: 125777248.000000\n",
      "\n",
      "Test set: Avg. loss: 7871660.9254, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 812 [0/869 (0%)]\tLoss: 47863884.000000\n",
      "Train Epoch: 812 [160/869 (18%)]\tLoss: 247704688.000000\n",
      "Train Epoch: 812 [320/869 (36%)]\tLoss: 206754896.000000\n",
      "Train Epoch: 812 [480/869 (55%)]\tLoss: 190191824.000000\n",
      "Train Epoch: 812 [640/869 (73%)]\tLoss: 63987944.000000\n",
      "Train Epoch: 812 [800/869 (91%)]\tLoss: 148859424.000000\n",
      "\n",
      "Test set: Avg. loss: 7415685.3220, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 813 [0/869 (0%)]\tLoss: 51142096.000000\n",
      "Train Epoch: 813 [160/869 (18%)]\tLoss: 82526208.000000\n",
      "Train Epoch: 813 [320/869 (36%)]\tLoss: 87107184.000000\n",
      "Train Epoch: 813 [480/869 (55%)]\tLoss: 253472000.000000\n",
      "Train Epoch: 813 [640/869 (73%)]\tLoss: 85094968.000000\n",
      "Train Epoch: 813 [800/869 (91%)]\tLoss: 78299200.000000\n",
      "\n",
      "Test set: Avg. loss: 7647273.4883, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 814 [0/869 (0%)]\tLoss: 177465424.000000\n",
      "Train Epoch: 814 [160/869 (18%)]\tLoss: 232475504.000000\n",
      "Train Epoch: 814 [320/869 (36%)]\tLoss: 141658992.000000\n",
      "Train Epoch: 814 [480/869 (55%)]\tLoss: 95561712.000000\n",
      "Train Epoch: 814 [640/869 (73%)]\tLoss: 146120160.000000\n",
      "Train Epoch: 814 [800/869 (91%)]\tLoss: 32567422.000000\n",
      "\n",
      "Test set: Avg. loss: 7602137.0917, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 815 [0/869 (0%)]\tLoss: 211919840.000000\n",
      "Train Epoch: 815 [160/869 (18%)]\tLoss: 88905488.000000\n",
      "Train Epoch: 815 [320/869 (36%)]\tLoss: 101124112.000000\n",
      "Train Epoch: 815 [480/869 (55%)]\tLoss: 128555968.000000\n",
      "Train Epoch: 815 [640/869 (73%)]\tLoss: 103844448.000000\n",
      "Train Epoch: 815 [800/869 (91%)]\tLoss: 203360656.000000\n",
      "\n",
      "Test set: Avg. loss: 7588223.8635, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 816 [0/869 (0%)]\tLoss: 64191060.000000\n",
      "Train Epoch: 816 [160/869 (18%)]\tLoss: 89301376.000000\n",
      "Train Epoch: 816 [320/869 (36%)]\tLoss: 83458056.000000\n",
      "Train Epoch: 816 [480/869 (55%)]\tLoss: 132682880.000000\n",
      "Train Epoch: 816 [640/869 (73%)]\tLoss: 60577696.000000\n",
      "Train Epoch: 816 [800/869 (91%)]\tLoss: 108720064.000000\n",
      "\n",
      "Test set: Avg. loss: 7307659.9147, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 817 [0/869 (0%)]\tLoss: 211209008.000000\n",
      "Train Epoch: 817 [160/869 (18%)]\tLoss: 79991744.000000\n",
      "Train Epoch: 817 [320/869 (36%)]\tLoss: 86782448.000000\n",
      "Train Epoch: 817 [480/869 (55%)]\tLoss: 65917620.000000\n",
      "Train Epoch: 817 [640/869 (73%)]\tLoss: 140214608.000000\n",
      "Train Epoch: 817 [800/869 (91%)]\tLoss: 56639672.000000\n",
      "\n",
      "Test set: Avg. loss: 7456353.4456, Accuracy: 0/469 (0%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 818 [0/869 (0%)]\tLoss: 120212976.000000\n",
      "Train Epoch: 818 [160/869 (18%)]\tLoss: 213493072.000000\n",
      "Train Epoch: 818 [320/869 (36%)]\tLoss: 155938448.000000\n",
      "Train Epoch: 818 [480/869 (55%)]\tLoss: 39516220.000000\n",
      "Train Epoch: 818 [640/869 (73%)]\tLoss: 179231296.000000\n",
      "Train Epoch: 818 [800/869 (91%)]\tLoss: 153626976.000000\n",
      "\n",
      "Test set: Avg. loss: 7653785.9872, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 819 [0/869 (0%)]\tLoss: 171206448.000000\n",
      "Train Epoch: 819 [160/869 (18%)]\tLoss: 160603024.000000\n",
      "Train Epoch: 819 [320/869 (36%)]\tLoss: 339205056.000000\n",
      "Train Epoch: 819 [480/869 (55%)]\tLoss: 160584176.000000\n",
      "Train Epoch: 819 [640/869 (73%)]\tLoss: 107874072.000000\n",
      "Train Epoch: 819 [800/869 (91%)]\tLoss: 82854864.000000\n",
      "\n",
      "Test set: Avg. loss: 7630180.5032, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 820 [0/869 (0%)]\tLoss: 104247784.000000\n",
      "Train Epoch: 820 [160/869 (18%)]\tLoss: 43638664.000000\n",
      "Train Epoch: 820 [320/869 (36%)]\tLoss: 209114768.000000\n",
      "Train Epoch: 820 [480/869 (55%)]\tLoss: 129564432.000000\n",
      "Train Epoch: 820 [640/869 (73%)]\tLoss: 64195232.000000\n",
      "Train Epoch: 820 [800/869 (91%)]\tLoss: 102829856.000000\n",
      "\n",
      "Test set: Avg. loss: 7867747.8294, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 821 [0/869 (0%)]\tLoss: 146002080.000000\n",
      "Train Epoch: 821 [160/869 (18%)]\tLoss: 156502944.000000\n",
      "Train Epoch: 821 [320/869 (36%)]\tLoss: 89704928.000000\n",
      "Train Epoch: 821 [480/869 (55%)]\tLoss: 19132200.000000\n",
      "Train Epoch: 821 [640/869 (73%)]\tLoss: 35574096.000000\n",
      "Train Epoch: 821 [800/869 (91%)]\tLoss: 161073392.000000\n",
      "\n",
      "Test set: Avg. loss: 7449471.1812, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 822 [0/869 (0%)]\tLoss: 207777632.000000\n",
      "Train Epoch: 822 [160/869 (18%)]\tLoss: 152466880.000000\n",
      "Train Epoch: 822 [320/869 (36%)]\tLoss: 201607728.000000\n",
      "Train Epoch: 822 [480/869 (55%)]\tLoss: 121610944.000000\n",
      "Train Epoch: 822 [640/869 (73%)]\tLoss: 183571120.000000\n",
      "Train Epoch: 822 [800/869 (91%)]\tLoss: 69633384.000000\n",
      "\n",
      "Test set: Avg. loss: 7902368.9808, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 823 [0/869 (0%)]\tLoss: 145979664.000000\n",
      "Train Epoch: 823 [160/869 (18%)]\tLoss: 139253152.000000\n",
      "Train Epoch: 823 [320/869 (36%)]\tLoss: 88133144.000000\n",
      "Train Epoch: 823 [480/869 (55%)]\tLoss: 85469096.000000\n",
      "Train Epoch: 823 [640/869 (73%)]\tLoss: 89691408.000000\n",
      "Train Epoch: 823 [800/869 (91%)]\tLoss: 188591760.000000\n",
      "\n",
      "Test set: Avg. loss: 7546525.6290, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 824 [0/869 (0%)]\tLoss: 133794568.000000\n",
      "Train Epoch: 824 [160/869 (18%)]\tLoss: 69947408.000000\n",
      "Train Epoch: 824 [320/869 (36%)]\tLoss: 89842536.000000\n",
      "Train Epoch: 824 [480/869 (55%)]\tLoss: 153594560.000000\n",
      "Train Epoch: 824 [640/869 (73%)]\tLoss: 237096144.000000\n",
      "Train Epoch: 824 [800/869 (91%)]\tLoss: 35771088.000000\n",
      "\n",
      "Test set: Avg. loss: 7884726.0469, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 825 [0/869 (0%)]\tLoss: 282510464.000000\n",
      "Train Epoch: 825 [160/869 (18%)]\tLoss: 106701768.000000\n",
      "Train Epoch: 825 [320/869 (36%)]\tLoss: 192361600.000000\n",
      "Train Epoch: 825 [480/869 (55%)]\tLoss: 184589872.000000\n",
      "Train Epoch: 825 [640/869 (73%)]\tLoss: 132987600.000000\n",
      "Train Epoch: 825 [800/869 (91%)]\tLoss: 54360280.000000\n",
      "\n",
      "Test set: Avg. loss: 7466209.9616, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 826 [0/869 (0%)]\tLoss: 116752544.000000\n",
      "Train Epoch: 826 [160/869 (18%)]\tLoss: 135959376.000000\n",
      "Train Epoch: 826 [320/869 (36%)]\tLoss: 35270660.000000\n",
      "Train Epoch: 826 [480/869 (55%)]\tLoss: 157942496.000000\n",
      "Train Epoch: 826 [640/869 (73%)]\tLoss: 111060984.000000\n",
      "Train Epoch: 826 [800/869 (91%)]\tLoss: 25342680.000000\n",
      "\n",
      "Test set: Avg. loss: 7321243.6759, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 827 [0/869 (0%)]\tLoss: 134349552.000000\n",
      "Train Epoch: 827 [160/869 (18%)]\tLoss: 45668888.000000\n",
      "Train Epoch: 827 [320/869 (36%)]\tLoss: 275442112.000000\n",
      "Train Epoch: 827 [480/869 (55%)]\tLoss: 67838640.000000\n",
      "Train Epoch: 827 [640/869 (73%)]\tLoss: 134604400.000000\n",
      "Train Epoch: 827 [800/869 (91%)]\tLoss: 36519480.000000\n",
      "\n",
      "Test set: Avg. loss: 7385162.1237, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 828 [0/869 (0%)]\tLoss: 91043576.000000\n",
      "Train Epoch: 828 [160/869 (18%)]\tLoss: 174096608.000000\n",
      "Train Epoch: 828 [320/869 (36%)]\tLoss: 134211984.000000\n",
      "Train Epoch: 828 [480/869 (55%)]\tLoss: 93551160.000000\n",
      "Train Epoch: 828 [640/869 (73%)]\tLoss: 66589936.000000\n",
      "Train Epoch: 828 [800/869 (91%)]\tLoss: 152861248.000000\n",
      "\n",
      "Test set: Avg. loss: 7544748.4179, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 829 [0/869 (0%)]\tLoss: 110929576.000000\n",
      "Train Epoch: 829 [160/869 (18%)]\tLoss: 47233764.000000\n",
      "Train Epoch: 829 [320/869 (36%)]\tLoss: 135513472.000000\n",
      "Train Epoch: 829 [480/869 (55%)]\tLoss: 58215936.000000\n",
      "Train Epoch: 829 [640/869 (73%)]\tLoss: 173159696.000000\n",
      "Train Epoch: 829 [800/869 (91%)]\tLoss: 136894576.000000\n",
      "\n",
      "Test set: Avg. loss: 7474222.9595, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 830 [0/869 (0%)]\tLoss: 100789216.000000\n",
      "Train Epoch: 830 [160/869 (18%)]\tLoss: 69442488.000000\n",
      "Train Epoch: 830 [320/869 (36%)]\tLoss: 77026352.000000\n",
      "Train Epoch: 830 [480/869 (55%)]\tLoss: 209521920.000000\n",
      "Train Epoch: 830 [640/869 (73%)]\tLoss: 113852736.000000\n",
      "Train Epoch: 830 [800/869 (91%)]\tLoss: 97724336.000000\n",
      "\n",
      "Test set: Avg. loss: 7579812.5458, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 831 [0/869 (0%)]\tLoss: 141187232.000000\n",
      "Train Epoch: 831 [160/869 (18%)]\tLoss: 95299776.000000\n",
      "Train Epoch: 831 [320/869 (36%)]\tLoss: 78568968.000000\n",
      "Train Epoch: 831 [480/869 (55%)]\tLoss: 78884136.000000\n",
      "Train Epoch: 831 [640/869 (73%)]\tLoss: 179731216.000000\n",
      "Train Epoch: 831 [800/869 (91%)]\tLoss: 233892320.000000\n",
      "\n",
      "Test set: Avg. loss: 7346902.6098, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 832 [0/869 (0%)]\tLoss: 337673248.000000\n",
      "Train Epoch: 832 [160/869 (18%)]\tLoss: 218843088.000000\n",
      "Train Epoch: 832 [320/869 (36%)]\tLoss: 114086480.000000\n",
      "Train Epoch: 832 [480/869 (55%)]\tLoss: 288486464.000000\n",
      "Train Epoch: 832 [640/869 (73%)]\tLoss: 138247696.000000\n",
      "Train Epoch: 832 [800/869 (91%)]\tLoss: 81842416.000000\n",
      "\n",
      "Test set: Avg. loss: 7550229.7740, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 833 [0/869 (0%)]\tLoss: 165580672.000000\n",
      "Train Epoch: 833 [160/869 (18%)]\tLoss: 104900272.000000\n",
      "Train Epoch: 833 [320/869 (36%)]\tLoss: 150003632.000000\n",
      "Train Epoch: 833 [480/869 (55%)]\tLoss: 93173952.000000\n",
      "Train Epoch: 833 [640/869 (73%)]\tLoss: 207036544.000000\n",
      "Train Epoch: 833 [800/869 (91%)]\tLoss: 68601016.000000\n",
      "\n",
      "Test set: Avg. loss: 7533369.3049, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 834 [0/869 (0%)]\tLoss: 72802736.000000\n",
      "Train Epoch: 834 [160/869 (18%)]\tLoss: 183031536.000000\n",
      "Train Epoch: 834 [320/869 (36%)]\tLoss: 120547152.000000\n",
      "Train Epoch: 834 [480/869 (55%)]\tLoss: 72832288.000000\n",
      "Train Epoch: 834 [640/869 (73%)]\tLoss: 106733560.000000\n",
      "Train Epoch: 834 [800/869 (91%)]\tLoss: 85548304.000000\n",
      "\n",
      "Test set: Avg. loss: 7564634.3795, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 835 [0/869 (0%)]\tLoss: 241123456.000000\n",
      "Train Epoch: 835 [160/869 (18%)]\tLoss: 140486784.000000\n",
      "Train Epoch: 835 [320/869 (36%)]\tLoss: 178330208.000000\n",
      "Train Epoch: 835 [480/869 (55%)]\tLoss: 150264080.000000\n",
      "Train Epoch: 835 [640/869 (73%)]\tLoss: 85162952.000000\n",
      "Train Epoch: 835 [800/869 (91%)]\tLoss: 285026656.000000\n",
      "\n",
      "Test set: Avg. loss: 7500947.8635, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 836 [0/869 (0%)]\tLoss: 79271648.000000\n",
      "Train Epoch: 836 [160/869 (18%)]\tLoss: 212476192.000000\n",
      "Train Epoch: 836 [320/869 (36%)]\tLoss: 208772256.000000\n",
      "Train Epoch: 836 [480/869 (55%)]\tLoss: 174855728.000000\n",
      "Train Epoch: 836 [640/869 (73%)]\tLoss: 146885296.000000\n",
      "Train Epoch: 836 [800/869 (91%)]\tLoss: 230757344.000000\n",
      "\n",
      "Test set: Avg. loss: 7369316.5373, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 837 [0/869 (0%)]\tLoss: 136774224.000000\n",
      "Train Epoch: 837 [160/869 (18%)]\tLoss: 43279064.000000\n",
      "Train Epoch: 837 [320/869 (36%)]\tLoss: 232532352.000000\n",
      "Train Epoch: 837 [480/869 (55%)]\tLoss: 230837920.000000\n",
      "Train Epoch: 837 [640/869 (73%)]\tLoss: 40384976.000000\n",
      "Train Epoch: 837 [800/869 (91%)]\tLoss: 175475104.000000\n",
      "\n",
      "Test set: Avg. loss: 7443778.9851, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 838 [0/869 (0%)]\tLoss: 47602688.000000\n",
      "Train Epoch: 838 [160/869 (18%)]\tLoss: 86311320.000000\n",
      "Train Epoch: 838 [320/869 (36%)]\tLoss: 131623072.000000\n",
      "Train Epoch: 838 [480/869 (55%)]\tLoss: 32277316.000000\n",
      "Train Epoch: 838 [640/869 (73%)]\tLoss: 107410416.000000\n",
      "Train Epoch: 838 [800/869 (91%)]\tLoss: 57281136.000000\n",
      "\n",
      "Test set: Avg. loss: 7694116.5032, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 839 [0/869 (0%)]\tLoss: 38106132.000000\n",
      "Train Epoch: 839 [160/869 (18%)]\tLoss: 164573904.000000\n",
      "Train Epoch: 839 [320/869 (36%)]\tLoss: 282967232.000000\n",
      "Train Epoch: 839 [480/869 (55%)]\tLoss: 134530480.000000\n",
      "Train Epoch: 839 [640/869 (73%)]\tLoss: 203800816.000000\n",
      "Train Epoch: 839 [800/869 (91%)]\tLoss: 189902528.000000\n",
      "\n",
      "Test set: Avg. loss: 7490959.6759, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 840 [0/869 (0%)]\tLoss: 134624208.000000\n",
      "Train Epoch: 840 [160/869 (18%)]\tLoss: 77551208.000000\n",
      "Train Epoch: 840 [320/869 (36%)]\tLoss: 94273056.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 840 [480/869 (55%)]\tLoss: 88826544.000000\n",
      "Train Epoch: 840 [640/869 (73%)]\tLoss: 30155476.000000\n",
      "Train Epoch: 840 [800/869 (91%)]\tLoss: 63721080.000000\n",
      "\n",
      "Test set: Avg. loss: 7404143.7271, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 841 [0/869 (0%)]\tLoss: 154608160.000000\n",
      "Train Epoch: 841 [160/869 (18%)]\tLoss: 121691528.000000\n",
      "Train Epoch: 841 [320/869 (36%)]\tLoss: 114005576.000000\n",
      "Train Epoch: 841 [480/869 (55%)]\tLoss: 187378336.000000\n",
      "Train Epoch: 841 [640/869 (73%)]\tLoss: 88864752.000000\n",
      "Train Epoch: 841 [800/869 (91%)]\tLoss: 54795588.000000\n",
      "\n",
      "Test set: Avg. loss: 7462564.1962, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 842 [0/869 (0%)]\tLoss: 37224612.000000\n",
      "Train Epoch: 842 [160/869 (18%)]\tLoss: 172206720.000000\n",
      "Train Epoch: 842 [320/869 (36%)]\tLoss: 149707456.000000\n",
      "Train Epoch: 842 [480/869 (55%)]\tLoss: 221013088.000000\n",
      "Train Epoch: 842 [640/869 (73%)]\tLoss: 168658944.000000\n",
      "Train Epoch: 842 [800/869 (91%)]\tLoss: 98788464.000000\n",
      "\n",
      "Test set: Avg. loss: 7401818.4819, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 843 [0/869 (0%)]\tLoss: 53508760.000000\n",
      "Train Epoch: 843 [160/869 (18%)]\tLoss: 52266484.000000\n",
      "Train Epoch: 843 [320/869 (36%)]\tLoss: 94599616.000000\n",
      "Train Epoch: 843 [480/869 (55%)]\tLoss: 123160560.000000\n",
      "Train Epoch: 843 [640/869 (73%)]\tLoss: 108243520.000000\n",
      "Train Epoch: 843 [800/869 (91%)]\tLoss: 122420944.000000\n",
      "\n",
      "Test set: Avg. loss: 7594349.1429, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 844 [0/869 (0%)]\tLoss: 165070080.000000\n",
      "Train Epoch: 844 [160/869 (18%)]\tLoss: 150977504.000000\n",
      "Train Epoch: 844 [320/869 (36%)]\tLoss: 27800352.000000\n",
      "Train Epoch: 844 [480/869 (55%)]\tLoss: 39675032.000000\n",
      "Train Epoch: 844 [640/869 (73%)]\tLoss: 185823408.000000\n",
      "Train Epoch: 844 [800/869 (91%)]\tLoss: 200436096.000000\n",
      "\n",
      "Test set: Avg. loss: 7487686.3539, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 845 [0/869 (0%)]\tLoss: 181627952.000000\n",
      "Train Epoch: 845 [160/869 (18%)]\tLoss: 78820448.000000\n",
      "Train Epoch: 845 [320/869 (36%)]\tLoss: 187686192.000000\n",
      "Train Epoch: 845 [480/869 (55%)]\tLoss: 79485008.000000\n",
      "Train Epoch: 845 [640/869 (73%)]\tLoss: 141546656.000000\n",
      "Train Epoch: 845 [800/869 (91%)]\tLoss: 129204432.000000\n",
      "\n",
      "Test set: Avg. loss: 7308399.1386, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 846 [0/869 (0%)]\tLoss: 87546824.000000\n",
      "Train Epoch: 846 [160/869 (18%)]\tLoss: 121244752.000000\n",
      "Train Epoch: 846 [320/869 (36%)]\tLoss: 339156544.000000\n",
      "Train Epoch: 846 [480/869 (55%)]\tLoss: 110209488.000000\n",
      "Train Epoch: 846 [640/869 (73%)]\tLoss: 120982664.000000\n",
      "Train Epoch: 846 [800/869 (91%)]\tLoss: 97799160.000000\n",
      "\n",
      "Test set: Avg. loss: 7422568.5800, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 847 [0/869 (0%)]\tLoss: 197370704.000000\n",
      "Train Epoch: 847 [160/869 (18%)]\tLoss: 104067328.000000\n",
      "Train Epoch: 847 [320/869 (36%)]\tLoss: 77461704.000000\n",
      "Train Epoch: 847 [480/869 (55%)]\tLoss: 45955608.000000\n",
      "Train Epoch: 847 [640/869 (73%)]\tLoss: 116293152.000000\n",
      "Train Epoch: 847 [800/869 (91%)]\tLoss: 90696104.000000\n",
      "\n",
      "Test set: Avg. loss: 7842602.2004, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 848 [0/869 (0%)]\tLoss: 135283680.000000\n",
      "Train Epoch: 848 [160/869 (18%)]\tLoss: 107316816.000000\n",
      "Train Epoch: 848 [320/869 (36%)]\tLoss: 47525120.000000\n",
      "Train Epoch: 848 [480/869 (55%)]\tLoss: 107605456.000000\n",
      "Train Epoch: 848 [640/869 (73%)]\tLoss: 91243000.000000\n",
      "Train Epoch: 848 [800/869 (91%)]\tLoss: 58118864.000000\n",
      "\n",
      "Test set: Avg. loss: 7768353.8252, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 849 [0/869 (0%)]\tLoss: 55223236.000000\n",
      "Train Epoch: 849 [160/869 (18%)]\tLoss: 217461040.000000\n",
      "Train Epoch: 849 [320/869 (36%)]\tLoss: 270303552.000000\n",
      "Train Epoch: 849 [480/869 (55%)]\tLoss: 81197904.000000\n",
      "Train Epoch: 849 [640/869 (73%)]\tLoss: 58258960.000000\n",
      "Train Epoch: 849 [800/869 (91%)]\tLoss: 84514384.000000\n",
      "\n",
      "Test set: Avg. loss: 7430998.0896, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 850 [0/869 (0%)]\tLoss: 242048160.000000\n",
      "Train Epoch: 850 [160/869 (18%)]\tLoss: 94879184.000000\n",
      "Train Epoch: 850 [320/869 (36%)]\tLoss: 138627392.000000\n",
      "Train Epoch: 850 [480/869 (55%)]\tLoss: 128670448.000000\n",
      "Train Epoch: 850 [640/869 (73%)]\tLoss: 79334352.000000\n",
      "Train Epoch: 850 [800/869 (91%)]\tLoss: 39230100.000000\n",
      "\n",
      "Test set: Avg. loss: 7649483.3603, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 851 [0/869 (0%)]\tLoss: 109230416.000000\n",
      "Train Epoch: 851 [160/869 (18%)]\tLoss: 172309376.000000\n",
      "Train Epoch: 851 [320/869 (36%)]\tLoss: 108455392.000000\n",
      "Train Epoch: 851 [480/869 (55%)]\tLoss: 83421904.000000\n",
      "Train Epoch: 851 [640/869 (73%)]\tLoss: 153201760.000000\n",
      "Train Epoch: 851 [800/869 (91%)]\tLoss: 49582216.000000\n",
      "\n",
      "Test set: Avg. loss: 7317714.5245, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 852 [0/869 (0%)]\tLoss: 174781056.000000\n",
      "Train Epoch: 852 [160/869 (18%)]\tLoss: 167265904.000000\n",
      "Train Epoch: 852 [320/869 (36%)]\tLoss: 146594960.000000\n",
      "Train Epoch: 852 [480/869 (55%)]\tLoss: 85206320.000000\n",
      "Train Epoch: 852 [640/869 (73%)]\tLoss: 224902000.000000\n",
      "Train Epoch: 852 [800/869 (91%)]\tLoss: 174357984.000000\n",
      "\n",
      "Test set: Avg. loss: 7503732.8614, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 853 [0/869 (0%)]\tLoss: 126217016.000000\n",
      "Train Epoch: 853 [160/869 (18%)]\tLoss: 84831376.000000\n",
      "Train Epoch: 853 [320/869 (36%)]\tLoss: 61389024.000000\n",
      "Train Epoch: 853 [480/869 (55%)]\tLoss: 43828804.000000\n",
      "Train Epoch: 853 [640/869 (73%)]\tLoss: 183144512.000000\n",
      "Train Epoch: 853 [800/869 (91%)]\tLoss: 147249168.000000\n",
      "\n",
      "Test set: Avg. loss: 7628124.1791, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 854 [0/869 (0%)]\tLoss: 134280064.000000\n",
      "Train Epoch: 854 [160/869 (18%)]\tLoss: 79388312.000000\n",
      "Train Epoch: 854 [320/869 (36%)]\tLoss: 59320592.000000\n",
      "Train Epoch: 854 [480/869 (55%)]\tLoss: 40677116.000000\n",
      "Train Epoch: 854 [640/869 (73%)]\tLoss: 190137328.000000\n",
      "Train Epoch: 854 [800/869 (91%)]\tLoss: 197118336.000000\n",
      "\n",
      "Test set: Avg. loss: 7651321.8081, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 855 [0/869 (0%)]\tLoss: 108333888.000000\n",
      "Train Epoch: 855 [160/869 (18%)]\tLoss: 84321616.000000\n",
      "Train Epoch: 855 [320/869 (36%)]\tLoss: 77893528.000000\n",
      "Train Epoch: 855 [480/869 (55%)]\tLoss: 78015840.000000\n",
      "Train Epoch: 855 [640/869 (73%)]\tLoss: 112462272.000000\n",
      "Train Epoch: 855 [800/869 (91%)]\tLoss: 149485904.000000\n",
      "\n",
      "Test set: Avg. loss: 7459108.9339, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 856 [0/869 (0%)]\tLoss: 31578458.000000\n",
      "Train Epoch: 856 [160/869 (18%)]\tLoss: 118580496.000000\n",
      "Train Epoch: 856 [320/869 (36%)]\tLoss: 38483580.000000\n",
      "Train Epoch: 856 [480/869 (55%)]\tLoss: 39906288.000000\n",
      "Train Epoch: 856 [640/869 (73%)]\tLoss: 154622768.000000\n",
      "Train Epoch: 856 [800/869 (91%)]\tLoss: 95741680.000000\n",
      "\n",
      "Test set: Avg. loss: 7329616.4904, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 857 [0/869 (0%)]\tLoss: 90843072.000000\n",
      "Train Epoch: 857 [160/869 (18%)]\tLoss: 233564768.000000\n",
      "Train Epoch: 857 [320/869 (36%)]\tLoss: 136113056.000000\n",
      "Train Epoch: 857 [480/869 (55%)]\tLoss: 203889008.000000\n",
      "Train Epoch: 857 [640/869 (73%)]\tLoss: 228239984.000000\n",
      "Train Epoch: 857 [800/869 (91%)]\tLoss: 194456400.000000\n",
      "\n",
      "Test set: Avg. loss: 7726538.4051, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 858 [0/869 (0%)]\tLoss: 46703036.000000\n",
      "Train Epoch: 858 [160/869 (18%)]\tLoss: 156662528.000000\n",
      "Train Epoch: 858 [320/869 (36%)]\tLoss: 105745856.000000\n",
      "Train Epoch: 858 [480/869 (55%)]\tLoss: 84487872.000000\n",
      "Train Epoch: 858 [640/869 (73%)]\tLoss: 93892664.000000\n",
      "Train Epoch: 858 [800/869 (91%)]\tLoss: 98032856.000000\n",
      "\n",
      "Test set: Avg. loss: 7387740.1791, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 859 [0/869 (0%)]\tLoss: 123842856.000000\n",
      "Train Epoch: 859 [160/869 (18%)]\tLoss: 122534016.000000\n",
      "Train Epoch: 859 [320/869 (36%)]\tLoss: 38106932.000000\n",
      "Train Epoch: 859 [480/869 (55%)]\tLoss: 123275552.000000\n",
      "Train Epoch: 859 [640/869 (73%)]\tLoss: 76955808.000000\n",
      "Train Epoch: 859 [800/869 (91%)]\tLoss: 125350016.000000\n",
      "\n",
      "Test set: Avg. loss: 7444891.0533, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 860 [0/869 (0%)]\tLoss: 212408752.000000\n",
      "Train Epoch: 860 [160/869 (18%)]\tLoss: 94501784.000000\n",
      "Train Epoch: 860 [320/869 (36%)]\tLoss: 116510896.000000\n",
      "Train Epoch: 860 [480/869 (55%)]\tLoss: 133417104.000000\n",
      "Train Epoch: 860 [640/869 (73%)]\tLoss: 100062680.000000\n",
      "Train Epoch: 860 [800/869 (91%)]\tLoss: 77357600.000000\n",
      "\n",
      "Test set: Avg. loss: 7645383.9147, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 861 [0/869 (0%)]\tLoss: 164515776.000000\n",
      "Train Epoch: 861 [160/869 (18%)]\tLoss: 190576224.000000\n",
      "Train Epoch: 861 [320/869 (36%)]\tLoss: 138596848.000000\n",
      "Train Epoch: 861 [480/869 (55%)]\tLoss: 28119036.000000\n",
      "Train Epoch: 861 [640/869 (73%)]\tLoss: 58990560.000000\n",
      "Train Epoch: 861 [800/869 (91%)]\tLoss: 271644960.000000\n",
      "\n",
      "Test set: Avg. loss: 7307325.8934, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 862 [0/869 (0%)]\tLoss: 259746256.000000\n",
      "Train Epoch: 862 [160/869 (18%)]\tLoss: 108312888.000000\n",
      "Train Epoch: 862 [320/869 (36%)]\tLoss: 172795904.000000\n",
      "Train Epoch: 862 [480/869 (55%)]\tLoss: 167192768.000000\n",
      "Train Epoch: 862 [640/869 (73%)]\tLoss: 84915040.000000\n",
      "Train Epoch: 862 [800/869 (91%)]\tLoss: 58698700.000000\n",
      "\n",
      "Test set: Avg. loss: 7555404.6397, Accuracy: 0/469 (0%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 863 [0/869 (0%)]\tLoss: 90197120.000000\n",
      "Train Epoch: 863 [160/869 (18%)]\tLoss: 41027868.000000\n",
      "Train Epoch: 863 [320/869 (36%)]\tLoss: 104662640.000000\n",
      "Train Epoch: 863 [480/869 (55%)]\tLoss: 44389420.000000\n",
      "Train Epoch: 863 [640/869 (73%)]\tLoss: 166314560.000000\n",
      "Train Epoch: 863 [800/869 (91%)]\tLoss: 257552112.000000\n",
      "\n",
      "Test set: Avg. loss: 7311042.5586, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 864 [0/869 (0%)]\tLoss: 140245680.000000\n",
      "Train Epoch: 864 [160/869 (18%)]\tLoss: 116352160.000000\n",
      "Train Epoch: 864 [320/869 (36%)]\tLoss: 92341056.000000\n",
      "Train Epoch: 864 [480/869 (55%)]\tLoss: 99141864.000000\n",
      "Train Epoch: 864 [640/869 (73%)]\tLoss: 137867424.000000\n",
      "Train Epoch: 864 [800/869 (91%)]\tLoss: 180523440.000000\n",
      "\n",
      "Test set: Avg. loss: 7572638.6951, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 865 [0/869 (0%)]\tLoss: 191390272.000000\n",
      "Train Epoch: 865 [160/869 (18%)]\tLoss: 142414256.000000\n",
      "Train Epoch: 865 [320/869 (36%)]\tLoss: 223301760.000000\n",
      "Train Epoch: 865 [480/869 (55%)]\tLoss: 50779988.000000\n",
      "Train Epoch: 865 [640/869 (73%)]\tLoss: 65947796.000000\n",
      "Train Epoch: 865 [800/869 (91%)]\tLoss: 48698696.000000\n",
      "\n",
      "Test set: Avg. loss: 7505370.3966, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 866 [0/869 (0%)]\tLoss: 164117712.000000\n",
      "Train Epoch: 866 [160/869 (18%)]\tLoss: 50736688.000000\n",
      "Train Epoch: 866 [320/869 (36%)]\tLoss: 145862592.000000\n",
      "Train Epoch: 866 [480/869 (55%)]\tLoss: 98553320.000000\n",
      "Train Epoch: 866 [640/869 (73%)]\tLoss: 70494064.000000\n",
      "Train Epoch: 866 [800/869 (91%)]\tLoss: 129985904.000000\n",
      "\n",
      "Test set: Avg. loss: 7442227.8038, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 867 [0/869 (0%)]\tLoss: 56238700.000000\n",
      "Train Epoch: 867 [160/869 (18%)]\tLoss: 198251360.000000\n",
      "Train Epoch: 867 [320/869 (36%)]\tLoss: 124775904.000000\n",
      "Train Epoch: 867 [480/869 (55%)]\tLoss: 319378304.000000\n",
      "Train Epoch: 867 [640/869 (73%)]\tLoss: 146750080.000000\n",
      "Train Epoch: 867 [800/869 (91%)]\tLoss: 119094312.000000\n",
      "\n",
      "Test set: Avg. loss: 7910170.5928, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 868 [0/869 (0%)]\tLoss: 105351464.000000\n",
      "Train Epoch: 868 [160/869 (18%)]\tLoss: 192272688.000000\n",
      "Train Epoch: 868 [320/869 (36%)]\tLoss: 222818640.000000\n",
      "Train Epoch: 868 [480/869 (55%)]\tLoss: 108234752.000000\n",
      "Train Epoch: 868 [640/869 (73%)]\tLoss: 80279552.000000\n",
      "Train Epoch: 868 [800/869 (91%)]\tLoss: 72353016.000000\n",
      "\n",
      "Test set: Avg. loss: 7611361.9616, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 869 [0/869 (0%)]\tLoss: 165179024.000000\n",
      "Train Epoch: 869 [160/869 (18%)]\tLoss: 33543032.000000\n",
      "Train Epoch: 869 [320/869 (36%)]\tLoss: 91599920.000000\n",
      "Train Epoch: 869 [480/869 (55%)]\tLoss: 59008704.000000\n",
      "Train Epoch: 869 [640/869 (73%)]\tLoss: 136685696.000000\n",
      "Train Epoch: 869 [800/869 (91%)]\tLoss: 405076992.000000\n",
      "\n",
      "Test set: Avg. loss: 7539224.3326, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 870 [0/869 (0%)]\tLoss: 76131576.000000\n",
      "Train Epoch: 870 [160/869 (18%)]\tLoss: 79245952.000000\n",
      "Train Epoch: 870 [320/869 (36%)]\tLoss: 96078720.000000\n",
      "Train Epoch: 870 [480/869 (55%)]\tLoss: 114061136.000000\n",
      "Train Epoch: 870 [640/869 (73%)]\tLoss: 87983168.000000\n",
      "Train Epoch: 870 [800/869 (91%)]\tLoss: 57771584.000000\n",
      "\n",
      "Test set: Avg. loss: 7318579.4968, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 871 [0/869 (0%)]\tLoss: 174393872.000000\n",
      "Train Epoch: 871 [160/869 (18%)]\tLoss: 48832860.000000\n",
      "Train Epoch: 871 [320/869 (36%)]\tLoss: 132753520.000000\n",
      "Train Epoch: 871 [480/869 (55%)]\tLoss: 70118768.000000\n",
      "Train Epoch: 871 [640/869 (73%)]\tLoss: 234155888.000000\n",
      "Train Epoch: 871 [800/869 (91%)]\tLoss: 94738656.000000\n",
      "\n",
      "Test set: Avg. loss: 7888425.0064, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 872 [0/869 (0%)]\tLoss: 168452128.000000\n",
      "Train Epoch: 872 [160/869 (18%)]\tLoss: 238056992.000000\n",
      "Train Epoch: 872 [320/869 (36%)]\tLoss: 59022732.000000\n",
      "Train Epoch: 872 [480/869 (55%)]\tLoss: 192093952.000000\n",
      "Train Epoch: 872 [640/869 (73%)]\tLoss: 86761696.000000\n",
      "Train Epoch: 872 [800/869 (91%)]\tLoss: 127884056.000000\n",
      "\n",
      "Test set: Avg. loss: 7611998.1663, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 873 [0/869 (0%)]\tLoss: 146152496.000000\n",
      "Train Epoch: 873 [160/869 (18%)]\tLoss: 143928512.000000\n",
      "Train Epoch: 873 [320/869 (36%)]\tLoss: 178899376.000000\n",
      "Train Epoch: 873 [480/869 (55%)]\tLoss: 18974162.000000\n",
      "Train Epoch: 873 [640/869 (73%)]\tLoss: 77527032.000000\n",
      "Train Epoch: 873 [800/869 (91%)]\tLoss: 191786336.000000\n",
      "\n",
      "Test set: Avg. loss: 7377136.6994, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 874 [0/869 (0%)]\tLoss: 61350480.000000\n",
      "Train Epoch: 874 [160/869 (18%)]\tLoss: 167833536.000000\n",
      "Train Epoch: 874 [320/869 (36%)]\tLoss: 53676492.000000\n",
      "Train Epoch: 874 [480/869 (55%)]\tLoss: 224253376.000000\n",
      "Train Epoch: 874 [640/869 (73%)]\tLoss: 193718880.000000\n",
      "Train Epoch: 874 [800/869 (91%)]\tLoss: 175534176.000000\n",
      "\n",
      "Test set: Avg. loss: 7596779.2409, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 875 [0/869 (0%)]\tLoss: 102909472.000000\n",
      "Train Epoch: 875 [160/869 (18%)]\tLoss: 147963584.000000\n",
      "Train Epoch: 875 [320/869 (36%)]\tLoss: 148583056.000000\n",
      "Train Epoch: 875 [480/869 (55%)]\tLoss: 107739176.000000\n",
      "Train Epoch: 875 [640/869 (73%)]\tLoss: 49519068.000000\n",
      "Train Epoch: 875 [800/869 (91%)]\tLoss: 46969616.000000\n",
      "\n",
      "Test set: Avg. loss: 7723870.8571, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 876 [0/869 (0%)]\tLoss: 111641608.000000\n",
      "Train Epoch: 876 [160/869 (18%)]\tLoss: 284836576.000000\n",
      "Train Epoch: 876 [320/869 (36%)]\tLoss: 78032336.000000\n",
      "Train Epoch: 876 [480/869 (55%)]\tLoss: 87762024.000000\n",
      "Train Epoch: 876 [640/869 (73%)]\tLoss: 44414304.000000\n",
      "Train Epoch: 876 [800/869 (91%)]\tLoss: 107792120.000000\n",
      "\n",
      "Test set: Avg. loss: 7406086.7463, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 877 [0/869 (0%)]\tLoss: 85242264.000000\n",
      "Train Epoch: 877 [160/869 (18%)]\tLoss: 25803220.000000\n",
      "Train Epoch: 877 [320/869 (36%)]\tLoss: 90496192.000000\n",
      "Train Epoch: 877 [480/869 (55%)]\tLoss: 52141328.000000\n",
      "Train Epoch: 877 [640/869 (73%)]\tLoss: 140408848.000000\n",
      "Train Epoch: 877 [800/869 (91%)]\tLoss: 104343224.000000\n",
      "\n",
      "Test set: Avg. loss: 7738761.0661, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 878 [0/869 (0%)]\tLoss: 233317744.000000\n",
      "Train Epoch: 878 [160/869 (18%)]\tLoss: 118653328.000000\n",
      "Train Epoch: 878 [320/869 (36%)]\tLoss: 200463136.000000\n",
      "Train Epoch: 878 [480/869 (55%)]\tLoss: 88683672.000000\n",
      "Train Epoch: 878 [640/869 (73%)]\tLoss: 224902320.000000\n",
      "Train Epoch: 878 [800/869 (91%)]\tLoss: 120378592.000000\n",
      "\n",
      "Test set: Avg. loss: 7490448.8870, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 879 [0/869 (0%)]\tLoss: 253313488.000000\n",
      "Train Epoch: 879 [160/869 (18%)]\tLoss: 95883792.000000\n",
      "Train Epoch: 879 [320/869 (36%)]\tLoss: 342441728.000000\n",
      "Train Epoch: 879 [480/869 (55%)]\tLoss: 78836176.000000\n",
      "Train Epoch: 879 [640/869 (73%)]\tLoss: 132961688.000000\n",
      "Train Epoch: 879 [800/869 (91%)]\tLoss: 146651872.000000\n",
      "\n",
      "Test set: Avg. loss: 7324796.8785, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 880 [0/869 (0%)]\tLoss: 189300672.000000\n",
      "Train Epoch: 880 [160/869 (18%)]\tLoss: 122753408.000000\n",
      "Train Epoch: 880 [320/869 (36%)]\tLoss: 183944624.000000\n",
      "Train Epoch: 880 [480/869 (55%)]\tLoss: 134235696.000000\n",
      "Train Epoch: 880 [640/869 (73%)]\tLoss: 253061328.000000\n",
      "Train Epoch: 880 [800/869 (91%)]\tLoss: 227200144.000000\n",
      "\n",
      "Test set: Avg. loss: 7303760.5288, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 881 [0/869 (0%)]\tLoss: 137177760.000000\n",
      "Train Epoch: 881 [160/869 (18%)]\tLoss: 36814348.000000\n",
      "Train Epoch: 881 [320/869 (36%)]\tLoss: 145044800.000000\n",
      "Train Epoch: 881 [480/869 (55%)]\tLoss: 230416096.000000\n",
      "Train Epoch: 881 [640/869 (73%)]\tLoss: 123523440.000000\n",
      "Train Epoch: 881 [800/869 (91%)]\tLoss: 85258032.000000\n",
      "\n",
      "Test set: Avg. loss: 7489013.2196, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 882 [0/869 (0%)]\tLoss: 58931580.000000\n",
      "Train Epoch: 882 [160/869 (18%)]\tLoss: 55618712.000000\n",
      "Train Epoch: 882 [320/869 (36%)]\tLoss: 83470040.000000\n",
      "Train Epoch: 882 [480/869 (55%)]\tLoss: 159398160.000000\n",
      "Train Epoch: 882 [640/869 (73%)]\tLoss: 115632544.000000\n",
      "Train Epoch: 882 [800/869 (91%)]\tLoss: 77216000.000000\n",
      "\n",
      "Test set: Avg. loss: 7433706.2431, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 883 [0/869 (0%)]\tLoss: 155624576.000000\n",
      "Train Epoch: 883 [160/869 (18%)]\tLoss: 103232944.000000\n",
      "Train Epoch: 883 [320/869 (36%)]\tLoss: 86284848.000000\n",
      "Train Epoch: 883 [480/869 (55%)]\tLoss: 257979392.000000\n",
      "Train Epoch: 883 [640/869 (73%)]\tLoss: 155271664.000000\n",
      "Train Epoch: 883 [800/869 (91%)]\tLoss: 136110288.000000\n",
      "\n",
      "Test set: Avg. loss: 7383568.5458, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 884 [0/869 (0%)]\tLoss: 153379200.000000\n",
      "Train Epoch: 884 [160/869 (18%)]\tLoss: 163610832.000000\n",
      "Train Epoch: 884 [320/869 (36%)]\tLoss: 82522792.000000\n",
      "Train Epoch: 884 [480/869 (55%)]\tLoss: 208428320.000000\n",
      "Train Epoch: 884 [640/869 (73%)]\tLoss: 139385568.000000\n",
      "Train Epoch: 884 [800/869 (91%)]\tLoss: 194609008.000000\n",
      "\n",
      "Test set: Avg. loss: 7733245.1130, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 885 [0/869 (0%)]\tLoss: 174743232.000000\n",
      "Train Epoch: 885 [160/869 (18%)]\tLoss: 50070100.000000\n",
      "Train Epoch: 885 [320/869 (36%)]\tLoss: 137384096.000000\n",
      "Train Epoch: 885 [480/869 (55%)]\tLoss: 167150784.000000\n",
      "Train Epoch: 885 [640/869 (73%)]\tLoss: 95421576.000000\n",
      "Train Epoch: 885 [800/869 (91%)]\tLoss: 219756896.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 7333477.5864, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 886 [0/869 (0%)]\tLoss: 261779104.000000\n",
      "Train Epoch: 886 [160/869 (18%)]\tLoss: 39282116.000000\n",
      "Train Epoch: 886 [320/869 (36%)]\tLoss: 163855504.000000\n",
      "Train Epoch: 886 [480/869 (55%)]\tLoss: 41658564.000000\n",
      "Train Epoch: 886 [640/869 (73%)]\tLoss: 37730080.000000\n",
      "Train Epoch: 886 [800/869 (91%)]\tLoss: 52237880.000000\n",
      "\n",
      "Test set: Avg. loss: 7396332.6482, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 887 [0/869 (0%)]\tLoss: 35363464.000000\n",
      "Train Epoch: 887 [160/869 (18%)]\tLoss: 50800004.000000\n",
      "Train Epoch: 887 [320/869 (36%)]\tLoss: 86769504.000000\n",
      "Train Epoch: 887 [480/869 (55%)]\tLoss: 102982896.000000\n",
      "Train Epoch: 887 [640/869 (73%)]\tLoss: 112239688.000000\n",
      "Train Epoch: 887 [800/869 (91%)]\tLoss: 114286000.000000\n",
      "\n",
      "Test set: Avg. loss: 7626862.7974, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 888 [0/869 (0%)]\tLoss: 173302480.000000\n",
      "Train Epoch: 888 [160/869 (18%)]\tLoss: 100440632.000000\n",
      "Train Epoch: 888 [320/869 (36%)]\tLoss: 147817200.000000\n",
      "Train Epoch: 888 [480/869 (55%)]\tLoss: 199385312.000000\n",
      "Train Epoch: 888 [640/869 (73%)]\tLoss: 72525128.000000\n",
      "Train Epoch: 888 [800/869 (91%)]\tLoss: 95286912.000000\n",
      "\n",
      "Test set: Avg. loss: 7666285.5949, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 889 [0/869 (0%)]\tLoss: 238140704.000000\n",
      "Train Epoch: 889 [160/869 (18%)]\tLoss: 125125576.000000\n",
      "Train Epoch: 889 [320/869 (36%)]\tLoss: 145561824.000000\n",
      "Train Epoch: 889 [480/869 (55%)]\tLoss: 193604432.000000\n",
      "Train Epoch: 889 [640/869 (73%)]\tLoss: 244810912.000000\n",
      "Train Epoch: 889 [800/869 (91%)]\tLoss: 120640888.000000\n",
      "\n",
      "Test set: Avg. loss: 7392044.5885, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 890 [0/869 (0%)]\tLoss: 104596672.000000\n",
      "Train Epoch: 890 [160/869 (18%)]\tLoss: 50956256.000000\n",
      "Train Epoch: 890 [320/869 (36%)]\tLoss: 40204168.000000\n",
      "Train Epoch: 890 [480/869 (55%)]\tLoss: 162140096.000000\n",
      "Train Epoch: 890 [640/869 (73%)]\tLoss: 261416144.000000\n",
      "Train Epoch: 890 [800/869 (91%)]\tLoss: 163307904.000000\n",
      "\n",
      "Test set: Avg. loss: 7465124.0085, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 891 [0/869 (0%)]\tLoss: 252935712.000000\n",
      "Train Epoch: 891 [160/869 (18%)]\tLoss: 57416916.000000\n",
      "Train Epoch: 891 [320/869 (36%)]\tLoss: 81899104.000000\n",
      "Train Epoch: 891 [480/869 (55%)]\tLoss: 107622152.000000\n",
      "Train Epoch: 891 [640/869 (73%)]\tLoss: 88849688.000000\n",
      "Train Epoch: 891 [800/869 (91%)]\tLoss: 142642768.000000\n",
      "\n",
      "Test set: Avg. loss: 7488421.8678, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 892 [0/869 (0%)]\tLoss: 36132536.000000\n",
      "Train Epoch: 892 [160/869 (18%)]\tLoss: 207757008.000000\n",
      "Train Epoch: 892 [320/869 (36%)]\tLoss: 93068384.000000\n",
      "Train Epoch: 892 [480/869 (55%)]\tLoss: 98296184.000000\n",
      "Train Epoch: 892 [640/869 (73%)]\tLoss: 124463368.000000\n",
      "Train Epoch: 892 [800/869 (91%)]\tLoss: 134577312.000000\n",
      "\n",
      "Test set: Avg. loss: 7646645.4627, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 893 [0/869 (0%)]\tLoss: 193016464.000000\n",
      "Train Epoch: 893 [160/869 (18%)]\tLoss: 31208818.000000\n",
      "Train Epoch: 893 [320/869 (36%)]\tLoss: 45387032.000000\n",
      "Train Epoch: 893 [480/869 (55%)]\tLoss: 77987840.000000\n",
      "Train Epoch: 893 [640/869 (73%)]\tLoss: 102195920.000000\n",
      "Train Epoch: 893 [800/869 (91%)]\tLoss: 41796768.000000\n",
      "\n",
      "Test set: Avg. loss: 7486594.8486, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 894 [0/869 (0%)]\tLoss: 102819560.000000\n",
      "Train Epoch: 894 [160/869 (18%)]\tLoss: 92008712.000000\n",
      "Train Epoch: 894 [320/869 (36%)]\tLoss: 107873376.000000\n",
      "Train Epoch: 894 [480/869 (55%)]\tLoss: 146372400.000000\n",
      "Train Epoch: 894 [640/869 (73%)]\tLoss: 91925520.000000\n",
      "Train Epoch: 894 [800/869 (91%)]\tLoss: 83017832.000000\n",
      "\n",
      "Test set: Avg. loss: 7394443.9232, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 895 [0/869 (0%)]\tLoss: 184030896.000000\n",
      "Train Epoch: 895 [160/869 (18%)]\tLoss: 150468656.000000\n",
      "Train Epoch: 895 [320/869 (36%)]\tLoss: 56164544.000000\n",
      "Train Epoch: 895 [480/869 (55%)]\tLoss: 140785984.000000\n",
      "Train Epoch: 895 [640/869 (73%)]\tLoss: 87631120.000000\n",
      "Train Epoch: 895 [800/869 (91%)]\tLoss: 187894576.000000\n",
      "\n",
      "Test set: Avg. loss: 7315544.4094, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 896 [0/869 (0%)]\tLoss: 84783880.000000\n",
      "Train Epoch: 896 [160/869 (18%)]\tLoss: 62270996.000000\n",
      "Train Epoch: 896 [320/869 (36%)]\tLoss: 147613552.000000\n",
      "Train Epoch: 896 [480/869 (55%)]\tLoss: 131517552.000000\n",
      "Train Epoch: 896 [640/869 (73%)]\tLoss: 208360416.000000\n",
      "Train Epoch: 896 [800/869 (91%)]\tLoss: 263011232.000000\n",
      "\n",
      "Test set: Avg. loss: 7305213.3987, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 897 [0/869 (0%)]\tLoss: 128522536.000000\n",
      "Train Epoch: 897 [160/869 (18%)]\tLoss: 225106208.000000\n",
      "Train Epoch: 897 [320/869 (36%)]\tLoss: 129787776.000000\n",
      "Train Epoch: 897 [480/869 (55%)]\tLoss: 73823192.000000\n",
      "Train Epoch: 897 [640/869 (73%)]\tLoss: 126660800.000000\n",
      "Train Epoch: 897 [800/869 (91%)]\tLoss: 156810944.000000\n",
      "\n",
      "Test set: Avg. loss: 7616546.7974, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 898 [0/869 (0%)]\tLoss: 298316448.000000\n",
      "Train Epoch: 898 [160/869 (18%)]\tLoss: 240158576.000000\n",
      "Train Epoch: 898 [320/869 (36%)]\tLoss: 170270672.000000\n",
      "Train Epoch: 898 [480/869 (55%)]\tLoss: 46837964.000000\n",
      "Train Epoch: 898 [640/869 (73%)]\tLoss: 240898272.000000\n",
      "Train Epoch: 898 [800/869 (91%)]\tLoss: 71914512.000000\n",
      "\n",
      "Test set: Avg. loss: 8009173.5224, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 899 [0/869 (0%)]\tLoss: 83102424.000000\n",
      "Train Epoch: 899 [160/869 (18%)]\tLoss: 117648192.000000\n",
      "Train Epoch: 899 [320/869 (36%)]\tLoss: 103265464.000000\n",
      "Train Epoch: 899 [480/869 (55%)]\tLoss: 69650880.000000\n",
      "Train Epoch: 899 [640/869 (73%)]\tLoss: 240095776.000000\n",
      "Train Epoch: 899 [800/869 (91%)]\tLoss: 100046712.000000\n",
      "\n",
      "Test set: Avg. loss: 7825832.4606, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 900 [0/869 (0%)]\tLoss: 241939296.000000\n",
      "Train Epoch: 900 [160/869 (18%)]\tLoss: 79056520.000000\n",
      "Train Epoch: 900 [320/869 (36%)]\tLoss: 181200608.000000\n",
      "Train Epoch: 900 [480/869 (55%)]\tLoss: 222747344.000000\n",
      "Train Epoch: 900 [640/869 (73%)]\tLoss: 120889248.000000\n",
      "Train Epoch: 900 [800/869 (91%)]\tLoss: 183934848.000000\n",
      "\n",
      "Test set: Avg. loss: 7712464.5288, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 901 [0/869 (0%)]\tLoss: 83669032.000000\n",
      "Train Epoch: 901 [160/869 (18%)]\tLoss: 124819040.000000\n",
      "Train Epoch: 901 [320/869 (36%)]\tLoss: 79404232.000000\n",
      "Train Epoch: 901 [480/869 (55%)]\tLoss: 225279664.000000\n",
      "Train Epoch: 901 [640/869 (73%)]\tLoss: 150479600.000000\n",
      "Train Epoch: 901 [800/869 (91%)]\tLoss: 83594880.000000\n",
      "\n",
      "Test set: Avg. loss: 7550841.3731, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 902 [0/869 (0%)]\tLoss: 124733384.000000\n",
      "Train Epoch: 902 [160/869 (18%)]\tLoss: 83264968.000000\n",
      "Train Epoch: 902 [320/869 (36%)]\tLoss: 110111416.000000\n",
      "Train Epoch: 902 [480/869 (55%)]\tLoss: 80058160.000000\n",
      "Train Epoch: 902 [640/869 (73%)]\tLoss: 130173120.000000\n",
      "Train Epoch: 902 [800/869 (91%)]\tLoss: 87532056.000000\n",
      "\n",
      "Test set: Avg. loss: 7928089.9787, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 903 [0/869 (0%)]\tLoss: 263651536.000000\n",
      "Train Epoch: 903 [160/869 (18%)]\tLoss: 104722224.000000\n",
      "Train Epoch: 903 [320/869 (36%)]\tLoss: 182779104.000000\n",
      "Train Epoch: 903 [480/869 (55%)]\tLoss: 146192160.000000\n",
      "Train Epoch: 903 [640/869 (73%)]\tLoss: 138390832.000000\n",
      "Train Epoch: 903 [800/869 (91%)]\tLoss: 43278120.000000\n",
      "\n",
      "Test set: Avg. loss: 7640905.5267, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 904 [0/869 (0%)]\tLoss: 106153840.000000\n",
      "Train Epoch: 904 [160/869 (18%)]\tLoss: 126776512.000000\n",
      "Train Epoch: 904 [320/869 (36%)]\tLoss: 171566848.000000\n",
      "Train Epoch: 904 [480/869 (55%)]\tLoss: 137890416.000000\n",
      "Train Epoch: 904 [640/869 (73%)]\tLoss: 130873640.000000\n",
      "Train Epoch: 904 [800/869 (91%)]\tLoss: 98038608.000000\n",
      "\n",
      "Test set: Avg. loss: 7498645.6205, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 905 [0/869 (0%)]\tLoss: 77454712.000000\n",
      "Train Epoch: 905 [160/869 (18%)]\tLoss: 282796928.000000\n",
      "Train Epoch: 905 [320/869 (36%)]\tLoss: 35694392.000000\n",
      "Train Epoch: 905 [480/869 (55%)]\tLoss: 87889560.000000\n",
      "Train Epoch: 905 [640/869 (73%)]\tLoss: 99008624.000000\n",
      "Train Epoch: 905 [800/869 (91%)]\tLoss: 116269264.000000\n",
      "\n",
      "Test set: Avg. loss: 8642272.2559, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 906 [0/869 (0%)]\tLoss: 90182224.000000\n",
      "Train Epoch: 906 [160/869 (18%)]\tLoss: 51576984.000000\n",
      "Train Epoch: 906 [320/869 (36%)]\tLoss: 46965496.000000\n",
      "Train Epoch: 906 [480/869 (55%)]\tLoss: 120403584.000000\n",
      "Train Epoch: 906 [640/869 (73%)]\tLoss: 186809360.000000\n",
      "Train Epoch: 906 [800/869 (91%)]\tLoss: 113109488.000000\n",
      "\n",
      "Test set: Avg. loss: 7615301.3902, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 907 [0/869 (0%)]\tLoss: 182176720.000000\n",
      "Train Epoch: 907 [160/869 (18%)]\tLoss: 44738080.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 907 [320/869 (36%)]\tLoss: 83991728.000000\n",
      "Train Epoch: 907 [480/869 (55%)]\tLoss: 112087696.000000\n",
      "Train Epoch: 907 [640/869 (73%)]\tLoss: 32317286.000000\n",
      "Train Epoch: 907 [800/869 (91%)]\tLoss: 115659048.000000\n",
      "\n",
      "Test set: Avg. loss: 7608333.4499, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 908 [0/869 (0%)]\tLoss: 96054248.000000\n",
      "Train Epoch: 908 [160/869 (18%)]\tLoss: 177378448.000000\n",
      "Train Epoch: 908 [320/869 (36%)]\tLoss: 110032912.000000\n",
      "Train Epoch: 908 [480/869 (55%)]\tLoss: 126107520.000000\n",
      "Train Epoch: 908 [640/869 (73%)]\tLoss: 68754568.000000\n",
      "Train Epoch: 908 [800/869 (91%)]\tLoss: 116193984.000000\n",
      "\n",
      "Test set: Avg. loss: 7531102.6525, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 909 [0/869 (0%)]\tLoss: 346642048.000000\n",
      "Train Epoch: 909 [160/869 (18%)]\tLoss: 233350752.000000\n",
      "Train Epoch: 909 [320/869 (36%)]\tLoss: 131184832.000000\n",
      "Train Epoch: 909 [480/869 (55%)]\tLoss: 38536056.000000\n",
      "Train Epoch: 909 [640/869 (73%)]\tLoss: 127581352.000000\n",
      "Train Epoch: 909 [800/869 (91%)]\tLoss: 109888688.000000\n",
      "\n",
      "Test set: Avg. loss: 7429484.3156, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 910 [0/869 (0%)]\tLoss: 109283256.000000\n",
      "Train Epoch: 910 [160/869 (18%)]\tLoss: 85418144.000000\n",
      "Train Epoch: 910 [320/869 (36%)]\tLoss: 214303520.000000\n",
      "Train Epoch: 910 [480/869 (55%)]\tLoss: 93085552.000000\n",
      "Train Epoch: 910 [640/869 (73%)]\tLoss: 126550400.000000\n",
      "Train Epoch: 910 [800/869 (91%)]\tLoss: 170972736.000000\n",
      "\n",
      "Test set: Avg. loss: 7519538.3284, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 911 [0/869 (0%)]\tLoss: 92396776.000000\n",
      "Train Epoch: 911 [160/869 (18%)]\tLoss: 81454728.000000\n",
      "Train Epoch: 911 [320/869 (36%)]\tLoss: 133172664.000000\n",
      "Train Epoch: 911 [480/869 (55%)]\tLoss: 246027776.000000\n",
      "Train Epoch: 911 [640/869 (73%)]\tLoss: 64569120.000000\n",
      "Train Epoch: 911 [800/869 (91%)]\tLoss: 188448224.000000\n",
      "\n",
      "Test set: Avg. loss: 7494090.0469, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 912 [0/869 (0%)]\tLoss: 140877728.000000\n",
      "Train Epoch: 912 [160/869 (18%)]\tLoss: 121134824.000000\n",
      "Train Epoch: 912 [320/869 (36%)]\tLoss: 199064848.000000\n",
      "Train Epoch: 912 [480/869 (55%)]\tLoss: 177824608.000000\n",
      "Train Epoch: 912 [640/869 (73%)]\tLoss: 54139412.000000\n",
      "Train Epoch: 912 [800/869 (91%)]\tLoss: 128305328.000000\n",
      "\n",
      "Test set: Avg. loss: 7629953.0149, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 913 [0/869 (0%)]\tLoss: 269657792.000000\n",
      "Train Epoch: 913 [160/869 (18%)]\tLoss: 77713368.000000\n",
      "Train Epoch: 913 [320/869 (36%)]\tLoss: 114681224.000000\n",
      "Train Epoch: 913 [480/869 (55%)]\tLoss: 114155984.000000\n",
      "Train Epoch: 913 [640/869 (73%)]\tLoss: 175246944.000000\n",
      "Train Epoch: 913 [800/869 (91%)]\tLoss: 179231216.000000\n",
      "\n",
      "Test set: Avg. loss: 8128808.6439, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 914 [0/869 (0%)]\tLoss: 170751152.000000\n",
      "Train Epoch: 914 [160/869 (18%)]\tLoss: 93508928.000000\n",
      "Train Epoch: 914 [320/869 (36%)]\tLoss: 191373664.000000\n",
      "Train Epoch: 914 [480/869 (55%)]\tLoss: 124199312.000000\n",
      "Train Epoch: 914 [640/869 (73%)]\tLoss: 155592944.000000\n",
      "Train Epoch: 914 [800/869 (91%)]\tLoss: 84083424.000000\n",
      "\n",
      "Test set: Avg. loss: 7435510.0384, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 915 [0/869 (0%)]\tLoss: 103839560.000000\n",
      "Train Epoch: 915 [160/869 (18%)]\tLoss: 129656432.000000\n",
      "Train Epoch: 915 [320/869 (36%)]\tLoss: 135847264.000000\n",
      "Train Epoch: 915 [480/869 (55%)]\tLoss: 189651216.000000\n",
      "Train Epoch: 915 [640/869 (73%)]\tLoss: 147032736.000000\n",
      "Train Epoch: 915 [800/869 (91%)]\tLoss: 78809344.000000\n",
      "\n",
      "Test set: Avg. loss: 7580988.8657, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 916 [0/869 (0%)]\tLoss: 87041496.000000\n",
      "Train Epoch: 916 [160/869 (18%)]\tLoss: 154862512.000000\n",
      "Train Epoch: 916 [320/869 (36%)]\tLoss: 71476800.000000\n",
      "Train Epoch: 916 [480/869 (55%)]\tLoss: 83039608.000000\n",
      "Train Epoch: 916 [640/869 (73%)]\tLoss: 247288032.000000\n",
      "Train Epoch: 916 [800/869 (91%)]\tLoss: 74061920.000000\n",
      "\n",
      "Test set: Avg. loss: 7644366.3454, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 917 [0/869 (0%)]\tLoss: 100651392.000000\n",
      "Train Epoch: 917 [160/869 (18%)]\tLoss: 155625168.000000\n",
      "Train Epoch: 917 [320/869 (36%)]\tLoss: 85796720.000000\n",
      "Train Epoch: 917 [480/869 (55%)]\tLoss: 91316304.000000\n",
      "Train Epoch: 917 [640/869 (73%)]\tLoss: 48946188.000000\n",
      "Train Epoch: 917 [800/869 (91%)]\tLoss: 467032608.000000\n",
      "\n",
      "Test set: Avg. loss: 7480129.2026, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 918 [0/869 (0%)]\tLoss: 154754128.000000\n",
      "Train Epoch: 918 [160/869 (18%)]\tLoss: 195167824.000000\n",
      "Train Epoch: 918 [320/869 (36%)]\tLoss: 223427824.000000\n",
      "Train Epoch: 918 [480/869 (55%)]\tLoss: 66998200.000000\n",
      "Train Epoch: 918 [640/869 (73%)]\tLoss: 87896024.000000\n",
      "Train Epoch: 918 [800/869 (91%)]\tLoss: 72616640.000000\n",
      "\n",
      "Test set: Avg. loss: 7423609.6802, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 919 [0/869 (0%)]\tLoss: 81368800.000000\n",
      "Train Epoch: 919 [160/869 (18%)]\tLoss: 132499112.000000\n",
      "Train Epoch: 919 [320/869 (36%)]\tLoss: 99925432.000000\n",
      "Train Epoch: 919 [480/869 (55%)]\tLoss: 113311696.000000\n",
      "Train Epoch: 919 [640/869 (73%)]\tLoss: 45141644.000000\n",
      "Train Epoch: 919 [800/869 (91%)]\tLoss: 83146568.000000\n",
      "\n",
      "Test set: Avg. loss: 7454169.7910, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 920 [0/869 (0%)]\tLoss: 107407704.000000\n",
      "Train Epoch: 920 [160/869 (18%)]\tLoss: 181589952.000000\n",
      "Train Epoch: 920 [320/869 (36%)]\tLoss: 126906440.000000\n",
      "Train Epoch: 920 [480/869 (55%)]\tLoss: 63961040.000000\n",
      "Train Epoch: 920 [640/869 (73%)]\tLoss: 128092264.000000\n",
      "Train Epoch: 920 [800/869 (91%)]\tLoss: 126190320.000000\n",
      "\n",
      "Test set: Avg. loss: 7566656.4691, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 921 [0/869 (0%)]\tLoss: 139287328.000000\n",
      "Train Epoch: 921 [160/869 (18%)]\tLoss: 92873144.000000\n",
      "Train Epoch: 921 [320/869 (36%)]\tLoss: 111437256.000000\n",
      "Train Epoch: 921 [480/869 (55%)]\tLoss: 87897280.000000\n",
      "Train Epoch: 921 [640/869 (73%)]\tLoss: 91478400.000000\n",
      "Train Epoch: 921 [800/869 (91%)]\tLoss: 128638264.000000\n",
      "\n",
      "Test set: Avg. loss: 7336072.9552, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 922 [0/869 (0%)]\tLoss: 136692896.000000\n",
      "Train Epoch: 922 [160/869 (18%)]\tLoss: 108551656.000000\n",
      "Train Epoch: 922 [320/869 (36%)]\tLoss: 102598832.000000\n",
      "Train Epoch: 922 [480/869 (55%)]\tLoss: 51599984.000000\n",
      "Train Epoch: 922 [640/869 (73%)]\tLoss: 93199840.000000\n",
      "Train Epoch: 922 [800/869 (91%)]\tLoss: 53344168.000000\n",
      "\n",
      "Test set: Avg. loss: 7408922.0896, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 923 [0/869 (0%)]\tLoss: 228001600.000000\n",
      "Train Epoch: 923 [160/869 (18%)]\tLoss: 360642752.000000\n",
      "Train Epoch: 923 [320/869 (36%)]\tLoss: 65671764.000000\n",
      "Train Epoch: 923 [480/869 (55%)]\tLoss: 85167208.000000\n",
      "Train Epoch: 923 [640/869 (73%)]\tLoss: 117421400.000000\n",
      "Train Epoch: 923 [800/869 (91%)]\tLoss: 176362448.000000\n",
      "\n",
      "Test set: Avg. loss: 7306413.7313, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 924 [0/869 (0%)]\tLoss: 157858816.000000\n",
      "Train Epoch: 924 [160/869 (18%)]\tLoss: 89531056.000000\n",
      "Train Epoch: 924 [320/869 (36%)]\tLoss: 96600352.000000\n",
      "Train Epoch: 924 [480/869 (55%)]\tLoss: 31354542.000000\n",
      "Train Epoch: 924 [640/869 (73%)]\tLoss: 134588192.000000\n",
      "Train Epoch: 924 [800/869 (91%)]\tLoss: 134896912.000000\n",
      "\n",
      "Test set: Avg. loss: 7485909.1940, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 925 [0/869 (0%)]\tLoss: 144610112.000000\n",
      "Train Epoch: 925 [160/869 (18%)]\tLoss: 93929880.000000\n",
      "Train Epoch: 925 [320/869 (36%)]\tLoss: 52119796.000000\n",
      "Train Epoch: 925 [480/869 (55%)]\tLoss: 54086748.000000\n",
      "Train Epoch: 925 [640/869 (73%)]\tLoss: 81352432.000000\n",
      "Train Epoch: 925 [800/869 (91%)]\tLoss: 149746768.000000\n",
      "\n",
      "Test set: Avg. loss: 7501134.4606, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 926 [0/869 (0%)]\tLoss: 146698544.000000\n",
      "Train Epoch: 926 [160/869 (18%)]\tLoss: 48423348.000000\n",
      "Train Epoch: 926 [320/869 (36%)]\tLoss: 165044720.000000\n",
      "Train Epoch: 926 [480/869 (55%)]\tLoss: 76182488.000000\n",
      "Train Epoch: 926 [640/869 (73%)]\tLoss: 158102480.000000\n",
      "Train Epoch: 926 [800/869 (91%)]\tLoss: 83055216.000000\n",
      "\n",
      "Test set: Avg. loss: 7595810.1578, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 927 [0/869 (0%)]\tLoss: 49296492.000000\n",
      "Train Epoch: 927 [160/869 (18%)]\tLoss: 107246432.000000\n",
      "Train Epoch: 927 [320/869 (36%)]\tLoss: 59767268.000000\n",
      "Train Epoch: 927 [480/869 (55%)]\tLoss: 133351944.000000\n",
      "Train Epoch: 927 [640/869 (73%)]\tLoss: 228684064.000000\n",
      "Train Epoch: 927 [800/869 (91%)]\tLoss: 110497832.000000\n",
      "\n",
      "Test set: Avg. loss: 7780519.8209, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 928 [0/869 (0%)]\tLoss: 131269536.000000\n",
      "Train Epoch: 928 [160/869 (18%)]\tLoss: 84369976.000000\n",
      "Train Epoch: 928 [320/869 (36%)]\tLoss: 290898272.000000\n",
      "Train Epoch: 928 [480/869 (55%)]\tLoss: 66513492.000000\n",
      "Train Epoch: 928 [640/869 (73%)]\tLoss: 89480896.000000\n",
      "Train Epoch: 928 [800/869 (91%)]\tLoss: 138058144.000000\n",
      "\n",
      "Test set: Avg. loss: 7526800.8785, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 929 [0/869 (0%)]\tLoss: 103526856.000000\n",
      "Train Epoch: 929 [160/869 (18%)]\tLoss: 110215784.000000\n",
      "Train Epoch: 929 [320/869 (36%)]\tLoss: 41750496.000000\n",
      "Train Epoch: 929 [480/869 (55%)]\tLoss: 144583888.000000\n",
      "Train Epoch: 929 [640/869 (73%)]\tLoss: 152541504.000000\n",
      "Train Epoch: 929 [800/869 (91%)]\tLoss: 48188492.000000\n",
      "\n",
      "Test set: Avg. loss: 8301010.9041, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 930 [0/869 (0%)]\tLoss: 98562648.000000\n",
      "Train Epoch: 930 [160/869 (18%)]\tLoss: 38755780.000000\n",
      "Train Epoch: 930 [320/869 (36%)]\tLoss: 143489456.000000\n",
      "Train Epoch: 930 [480/869 (55%)]\tLoss: 188701728.000000\n",
      "Train Epoch: 930 [640/869 (73%)]\tLoss: 120286928.000000\n",
      "Train Epoch: 930 [800/869 (91%)]\tLoss: 46826424.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 7709430.0384, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 931 [0/869 (0%)]\tLoss: 99702464.000000\n",
      "Train Epoch: 931 [160/869 (18%)]\tLoss: 98796464.000000\n",
      "Train Epoch: 931 [320/869 (36%)]\tLoss: 113248624.000000\n",
      "Train Epoch: 931 [480/869 (55%)]\tLoss: 104756160.000000\n",
      "Train Epoch: 931 [640/869 (73%)]\tLoss: 88124888.000000\n",
      "Train Epoch: 931 [800/869 (91%)]\tLoss: 25026864.000000\n",
      "\n",
      "Test set: Avg. loss: 7448290.2516, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 932 [0/869 (0%)]\tLoss: 78192264.000000\n",
      "Train Epoch: 932 [160/869 (18%)]\tLoss: 132909512.000000\n",
      "Train Epoch: 932 [320/869 (36%)]\tLoss: 115435056.000000\n",
      "Train Epoch: 932 [480/869 (55%)]\tLoss: 159953424.000000\n",
      "Train Epoch: 932 [640/869 (73%)]\tLoss: 71716328.000000\n",
      "Train Epoch: 932 [800/869 (91%)]\tLoss: 344782560.000000\n",
      "\n",
      "Test set: Avg. loss: 7791947.0533, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 933 [0/869 (0%)]\tLoss: 133427576.000000\n",
      "Train Epoch: 933 [160/869 (18%)]\tLoss: 134501440.000000\n",
      "Train Epoch: 933 [320/869 (36%)]\tLoss: 63868120.000000\n",
      "Train Epoch: 933 [480/869 (55%)]\tLoss: 40718192.000000\n",
      "Train Epoch: 933 [640/869 (73%)]\tLoss: 172750320.000000\n",
      "Train Epoch: 933 [800/869 (91%)]\tLoss: 181537568.000000\n",
      "\n",
      "Test set: Avg. loss: 7489556.5288, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 934 [0/869 (0%)]\tLoss: 103612424.000000\n",
      "Train Epoch: 934 [160/869 (18%)]\tLoss: 264156960.000000\n",
      "Train Epoch: 934 [320/869 (36%)]\tLoss: 154746240.000000\n",
      "Train Epoch: 934 [480/869 (55%)]\tLoss: 91391184.000000\n",
      "Train Epoch: 934 [640/869 (73%)]\tLoss: 113157192.000000\n",
      "Train Epoch: 934 [800/869 (91%)]\tLoss: 170956656.000000\n",
      "\n",
      "Test set: Avg. loss: 7567398.9765, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 935 [0/869 (0%)]\tLoss: 133878688.000000\n",
      "Train Epoch: 935 [160/869 (18%)]\tLoss: 112147696.000000\n",
      "Train Epoch: 935 [320/869 (36%)]\tLoss: 115688800.000000\n",
      "Train Epoch: 935 [480/869 (55%)]\tLoss: 91767400.000000\n",
      "Train Epoch: 935 [640/869 (73%)]\tLoss: 43386220.000000\n",
      "Train Epoch: 935 [800/869 (91%)]\tLoss: 176955712.000000\n",
      "\n",
      "Test set: Avg. loss: 7338553.3731, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 936 [0/869 (0%)]\tLoss: 171070288.000000\n",
      "Train Epoch: 936 [160/869 (18%)]\tLoss: 193857984.000000\n",
      "Train Epoch: 936 [320/869 (36%)]\tLoss: 126133616.000000\n",
      "Train Epoch: 936 [480/869 (55%)]\tLoss: 85056176.000000\n",
      "Train Epoch: 936 [640/869 (73%)]\tLoss: 84211040.000000\n",
      "Train Epoch: 936 [800/869 (91%)]\tLoss: 80678872.000000\n",
      "\n",
      "Test set: Avg. loss: 8112156.7932, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 937 [0/869 (0%)]\tLoss: 139019840.000000\n",
      "Train Epoch: 937 [160/869 (18%)]\tLoss: 95495744.000000\n",
      "Train Epoch: 937 [320/869 (36%)]\tLoss: 164155120.000000\n",
      "Train Epoch: 937 [480/869 (55%)]\tLoss: 182687456.000000\n",
      "Train Epoch: 937 [640/869 (73%)]\tLoss: 143114320.000000\n",
      "Train Epoch: 937 [800/869 (91%)]\tLoss: 91098792.000000\n",
      "\n",
      "Test set: Avg. loss: 7874355.3006, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 938 [0/869 (0%)]\tLoss: 84545504.000000\n",
      "Train Epoch: 938 [160/869 (18%)]\tLoss: 54357236.000000\n",
      "Train Epoch: 938 [320/869 (36%)]\tLoss: 54785644.000000\n",
      "Train Epoch: 938 [480/869 (55%)]\tLoss: 260601072.000000\n",
      "Train Epoch: 938 [640/869 (73%)]\tLoss: 145762128.000000\n",
      "Train Epoch: 938 [800/869 (91%)]\tLoss: 85817248.000000\n",
      "\n",
      "Test set: Avg. loss: 7611048.3241, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 939 [0/869 (0%)]\tLoss: 90740296.000000\n",
      "Train Epoch: 939 [160/869 (18%)]\tLoss: 38821340.000000\n",
      "Train Epoch: 939 [320/869 (36%)]\tLoss: 136823456.000000\n",
      "Train Epoch: 939 [480/869 (55%)]\tLoss: 135683904.000000\n",
      "Train Epoch: 939 [640/869 (73%)]\tLoss: 68572112.000000\n",
      "Train Epoch: 939 [800/869 (91%)]\tLoss: 64508320.000000\n",
      "\n",
      "Test set: Avg. loss: 7745001.4499, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 940 [0/869 (0%)]\tLoss: 141392720.000000\n",
      "Train Epoch: 940 [160/869 (18%)]\tLoss: 142697120.000000\n",
      "Train Epoch: 940 [320/869 (36%)]\tLoss: 120629632.000000\n",
      "Train Epoch: 940 [480/869 (55%)]\tLoss: 24958708.000000\n",
      "Train Epoch: 940 [640/869 (73%)]\tLoss: 116482016.000000\n",
      "Train Epoch: 940 [800/869 (91%)]\tLoss: 93528560.000000\n",
      "\n",
      "Test set: Avg. loss: 7360492.3497, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 941 [0/869 (0%)]\tLoss: 115779624.000000\n",
      "Train Epoch: 941 [160/869 (18%)]\tLoss: 58151952.000000\n",
      "Train Epoch: 941 [320/869 (36%)]\tLoss: 102829488.000000\n",
      "Train Epoch: 941 [480/869 (55%)]\tLoss: 78594864.000000\n",
      "Train Epoch: 941 [640/869 (73%)]\tLoss: 90630912.000000\n",
      "Train Epoch: 941 [800/869 (91%)]\tLoss: 42289616.000000\n",
      "\n",
      "Test set: Avg. loss: 7305381.8252, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 942 [0/869 (0%)]\tLoss: 141351088.000000\n",
      "Train Epoch: 942 [160/869 (18%)]\tLoss: 38499624.000000\n",
      "Train Epoch: 942 [320/869 (36%)]\tLoss: 352812768.000000\n",
      "Train Epoch: 942 [480/869 (55%)]\tLoss: 99813664.000000\n",
      "Train Epoch: 942 [640/869 (73%)]\tLoss: 82423184.000000\n",
      "Train Epoch: 942 [800/869 (91%)]\tLoss: 136398448.000000\n",
      "\n",
      "Test set: Avg. loss: 7757090.5160, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 943 [0/869 (0%)]\tLoss: 84072040.000000\n",
      "Train Epoch: 943 [160/869 (18%)]\tLoss: 51498712.000000\n",
      "Train Epoch: 943 [320/869 (36%)]\tLoss: 83088416.000000\n",
      "Train Epoch: 943 [480/869 (55%)]\tLoss: 179644560.000000\n",
      "Train Epoch: 943 [640/869 (73%)]\tLoss: 98112592.000000\n",
      "Train Epoch: 943 [800/869 (91%)]\tLoss: 77830088.000000\n",
      "\n",
      "Test set: Avg. loss: 7610332.4520, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 944 [0/869 (0%)]\tLoss: 138107328.000000\n",
      "Train Epoch: 944 [160/869 (18%)]\tLoss: 79212960.000000\n",
      "Train Epoch: 944 [320/869 (36%)]\tLoss: 100062200.000000\n",
      "Train Epoch: 944 [480/869 (55%)]\tLoss: 157376592.000000\n",
      "Train Epoch: 944 [640/869 (73%)]\tLoss: 40889592.000000\n",
      "Train Epoch: 944 [800/869 (91%)]\tLoss: 114433448.000000\n",
      "\n",
      "Test set: Avg. loss: 7404643.4712, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 945 [0/869 (0%)]\tLoss: 175192064.000000\n",
      "Train Epoch: 945 [160/869 (18%)]\tLoss: 253734496.000000\n",
      "Train Epoch: 945 [320/869 (36%)]\tLoss: 150847632.000000\n",
      "Train Epoch: 945 [480/869 (55%)]\tLoss: 93737520.000000\n",
      "Train Epoch: 945 [640/869 (73%)]\tLoss: 78650576.000000\n",
      "Train Epoch: 945 [800/869 (91%)]\tLoss: 107825136.000000\n",
      "\n",
      "Test set: Avg. loss: 7315417.3902, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 946 [0/869 (0%)]\tLoss: 82290232.000000\n",
      "Train Epoch: 946 [160/869 (18%)]\tLoss: 172411424.000000\n",
      "Train Epoch: 946 [320/869 (36%)]\tLoss: 221432048.000000\n",
      "Train Epoch: 946 [480/869 (55%)]\tLoss: 37432608.000000\n",
      "Train Epoch: 946 [640/869 (73%)]\tLoss: 136325600.000000\n",
      "Train Epoch: 946 [800/869 (91%)]\tLoss: 77961848.000000\n",
      "\n",
      "Test set: Avg. loss: 7697397.8763, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 947 [0/869 (0%)]\tLoss: 93517816.000000\n",
      "Train Epoch: 947 [160/869 (18%)]\tLoss: 124697424.000000\n",
      "Train Epoch: 947 [320/869 (36%)]\tLoss: 129520720.000000\n",
      "Train Epoch: 947 [480/869 (55%)]\tLoss: 106214560.000000\n",
      "Train Epoch: 947 [640/869 (73%)]\tLoss: 218495904.000000\n",
      "Train Epoch: 947 [800/869 (91%)]\tLoss: 281505312.000000\n",
      "\n",
      "Test set: Avg. loss: 7566397.5437, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 948 [0/869 (0%)]\tLoss: 154524160.000000\n",
      "Train Epoch: 948 [160/869 (18%)]\tLoss: 140622048.000000\n",
      "Train Epoch: 948 [320/869 (36%)]\tLoss: 79307256.000000\n",
      "Train Epoch: 948 [480/869 (55%)]\tLoss: 137909264.000000\n",
      "Train Epoch: 948 [640/869 (73%)]\tLoss: 238024800.000000\n",
      "Train Epoch: 948 [800/869 (91%)]\tLoss: 121039904.000000\n",
      "\n",
      "Test set: Avg. loss: 7715465.7399, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 949 [0/869 (0%)]\tLoss: 199685376.000000\n",
      "Train Epoch: 949 [160/869 (18%)]\tLoss: 55382528.000000\n",
      "Train Epoch: 949 [320/869 (36%)]\tLoss: 104740488.000000\n",
      "Train Epoch: 949 [480/869 (55%)]\tLoss: 173108304.000000\n",
      "Train Epoch: 949 [640/869 (73%)]\tLoss: 43897712.000000\n",
      "Train Epoch: 949 [800/869 (91%)]\tLoss: 89899488.000000\n",
      "\n",
      "Test set: Avg. loss: 7663729.6972, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 950 [0/869 (0%)]\tLoss: 324964256.000000\n",
      "Train Epoch: 950 [160/869 (18%)]\tLoss: 115377992.000000\n",
      "Train Epoch: 950 [320/869 (36%)]\tLoss: 143425808.000000\n",
      "Train Epoch: 950 [480/869 (55%)]\tLoss: 133914904.000000\n",
      "Train Epoch: 950 [640/869 (73%)]\tLoss: 39121596.000000\n",
      "Train Epoch: 950 [800/869 (91%)]\tLoss: 71823264.000000\n",
      "\n",
      "Test set: Avg. loss: 7408748.1535, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 951 [0/869 (0%)]\tLoss: 131695536.000000\n",
      "Train Epoch: 951 [160/869 (18%)]\tLoss: 170847504.000000\n",
      "Train Epoch: 951 [320/869 (36%)]\tLoss: 61004760.000000\n",
      "Train Epoch: 951 [480/869 (55%)]\tLoss: 89188304.000000\n",
      "Train Epoch: 951 [640/869 (73%)]\tLoss: 58935000.000000\n",
      "Train Epoch: 951 [800/869 (91%)]\tLoss: 140313984.000000\n",
      "\n",
      "Test set: Avg. loss: 7355483.3092, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 952 [0/869 (0%)]\tLoss: 128193440.000000\n",
      "Train Epoch: 952 [160/869 (18%)]\tLoss: 138494496.000000\n",
      "Train Epoch: 952 [320/869 (36%)]\tLoss: 154996464.000000\n",
      "Train Epoch: 952 [480/869 (55%)]\tLoss: 128231080.000000\n",
      "Train Epoch: 952 [640/869 (73%)]\tLoss: 145323328.000000\n",
      "Train Epoch: 952 [800/869 (91%)]\tLoss: 87451184.000000\n",
      "\n",
      "Test set: Avg. loss: 7392249.6119, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 953 [0/869 (0%)]\tLoss: 139863120.000000\n",
      "Train Epoch: 953 [160/869 (18%)]\tLoss: 95040120.000000\n",
      "Train Epoch: 953 [320/869 (36%)]\tLoss: 139193280.000000\n",
      "Train Epoch: 953 [480/869 (55%)]\tLoss: 129199920.000000\n",
      "Train Epoch: 953 [640/869 (73%)]\tLoss: 96379816.000000\n",
      "Train Epoch: 953 [800/869 (91%)]\tLoss: 173821424.000000\n",
      "\n",
      "Test set: Avg. loss: 7683851.8081, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 954 [0/869 (0%)]\tLoss: 143214752.000000\n",
      "Train Epoch: 954 [160/869 (18%)]\tLoss: 93477800.000000\n",
      "Train Epoch: 954 [320/869 (36%)]\tLoss: 167683696.000000\n",
      "Train Epoch: 954 [480/869 (55%)]\tLoss: 225460112.000000\n",
      "Train Epoch: 954 [640/869 (73%)]\tLoss: 85056032.000000\n",
      "Train Epoch: 954 [800/869 (91%)]\tLoss: 84209152.000000\n",
      "\n",
      "Test set: Avg. loss: 7590302.8529, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 955 [0/869 (0%)]\tLoss: 107449936.000000\n",
      "Train Epoch: 955 [160/869 (18%)]\tLoss: 50592736.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 955 [320/869 (36%)]\tLoss: 128902976.000000\n",
      "Train Epoch: 955 [480/869 (55%)]\tLoss: 35425924.000000\n",
      "Train Epoch: 955 [640/869 (73%)]\tLoss: 223056688.000000\n",
      "Train Epoch: 955 [800/869 (91%)]\tLoss: 123431656.000000\n",
      "\n",
      "Test set: Avg. loss: 7298852.5288, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 956 [0/869 (0%)]\tLoss: 129253464.000000\n",
      "Train Epoch: 956 [160/869 (18%)]\tLoss: 101780216.000000\n",
      "Train Epoch: 956 [320/869 (36%)]\tLoss: 97495712.000000\n",
      "Train Epoch: 956 [480/869 (55%)]\tLoss: 39412652.000000\n",
      "Train Epoch: 956 [640/869 (73%)]\tLoss: 245407136.000000\n",
      "Train Epoch: 956 [800/869 (91%)]\tLoss: 77406912.000000\n",
      "\n",
      "Test set: Avg. loss: 7696400.4435, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 957 [0/869 (0%)]\tLoss: 155813056.000000\n",
      "Train Epoch: 957 [160/869 (18%)]\tLoss: 270063296.000000\n",
      "Train Epoch: 957 [320/869 (36%)]\tLoss: 130265744.000000\n",
      "Train Epoch: 957 [480/869 (55%)]\tLoss: 86963832.000000\n",
      "Train Epoch: 957 [640/869 (73%)]\tLoss: 128687864.000000\n",
      "Train Epoch: 957 [800/869 (91%)]\tLoss: 104336552.000000\n",
      "\n",
      "Test set: Avg. loss: 7402443.5821, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 958 [0/869 (0%)]\tLoss: 74859352.000000\n",
      "Train Epoch: 958 [160/869 (18%)]\tLoss: 71076160.000000\n",
      "Train Epoch: 958 [320/869 (36%)]\tLoss: 200458160.000000\n",
      "Train Epoch: 958 [480/869 (55%)]\tLoss: 172262032.000000\n",
      "Train Epoch: 958 [640/869 (73%)]\tLoss: 178334416.000000\n",
      "Train Epoch: 958 [800/869 (91%)]\tLoss: 181599200.000000\n",
      "\n",
      "Test set: Avg. loss: 7608957.1002, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 959 [0/869 (0%)]\tLoss: 132609944.000000\n",
      "Train Epoch: 959 [160/869 (18%)]\tLoss: 87075152.000000\n",
      "Train Epoch: 959 [320/869 (36%)]\tLoss: 113796368.000000\n",
      "Train Epoch: 959 [480/869 (55%)]\tLoss: 287827968.000000\n",
      "Train Epoch: 959 [640/869 (73%)]\tLoss: 86384144.000000\n",
      "Train Epoch: 959 [800/869 (91%)]\tLoss: 195330304.000000\n",
      "\n",
      "Test set: Avg. loss: 7311046.1407, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 960 [0/869 (0%)]\tLoss: 68423104.000000\n",
      "Train Epoch: 960 [160/869 (18%)]\tLoss: 79723296.000000\n",
      "Train Epoch: 960 [320/869 (36%)]\tLoss: 86676768.000000\n",
      "Train Epoch: 960 [480/869 (55%)]\tLoss: 141769280.000000\n",
      "Train Epoch: 960 [640/869 (73%)]\tLoss: 275767072.000000\n",
      "Train Epoch: 960 [800/869 (91%)]\tLoss: 139165840.000000\n",
      "\n",
      "Test set: Avg. loss: 7442035.7953, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 961 [0/869 (0%)]\tLoss: 170241024.000000\n",
      "Train Epoch: 961 [160/869 (18%)]\tLoss: 140381088.000000\n",
      "Train Epoch: 961 [320/869 (36%)]\tLoss: 72053200.000000\n",
      "Train Epoch: 961 [480/869 (55%)]\tLoss: 97772112.000000\n",
      "Train Epoch: 961 [640/869 (73%)]\tLoss: 99018264.000000\n",
      "Train Epoch: 961 [800/869 (91%)]\tLoss: 130110912.000000\n",
      "\n",
      "Test set: Avg. loss: 7658312.2814, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 962 [0/869 (0%)]\tLoss: 159135024.000000\n",
      "Train Epoch: 962 [160/869 (18%)]\tLoss: 290872832.000000\n",
      "Train Epoch: 962 [320/869 (36%)]\tLoss: 91631416.000000\n",
      "Train Epoch: 962 [480/869 (55%)]\tLoss: 199922496.000000\n",
      "Train Epoch: 962 [640/869 (73%)]\tLoss: 225287520.000000\n",
      "Train Epoch: 962 [800/869 (91%)]\tLoss: 92803832.000000\n",
      "\n",
      "Test set: Avg. loss: 7663029.5267, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 963 [0/869 (0%)]\tLoss: 135680144.000000\n",
      "Train Epoch: 963 [160/869 (18%)]\tLoss: 109493432.000000\n",
      "Train Epoch: 963 [320/869 (36%)]\tLoss: 60142636.000000\n",
      "Train Epoch: 963 [480/869 (55%)]\tLoss: 143689584.000000\n",
      "Train Epoch: 963 [640/869 (73%)]\tLoss: 139930752.000000\n",
      "Train Epoch: 963 [800/869 (91%)]\tLoss: 122705520.000000\n",
      "\n",
      "Test set: Avg. loss: 7739175.7484, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 964 [0/869 (0%)]\tLoss: 84143448.000000\n",
      "Train Epoch: 964 [160/869 (18%)]\tLoss: 112632864.000000\n",
      "Train Epoch: 964 [320/869 (36%)]\tLoss: 36258860.000000\n",
      "Train Epoch: 964 [480/869 (55%)]\tLoss: 167707232.000000\n",
      "Train Epoch: 964 [640/869 (73%)]\tLoss: 143392336.000000\n",
      "Train Epoch: 964 [800/869 (91%)]\tLoss: 46080656.000000\n",
      "\n",
      "Test set: Avg. loss: 7479401.5949, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 965 [0/869 (0%)]\tLoss: 203615104.000000\n",
      "Train Epoch: 965 [160/869 (18%)]\tLoss: 81109344.000000\n",
      "Train Epoch: 965 [320/869 (36%)]\tLoss: 81746848.000000\n",
      "Train Epoch: 965 [480/869 (55%)]\tLoss: 103808072.000000\n",
      "Train Epoch: 965 [640/869 (73%)]\tLoss: 163313616.000000\n",
      "Train Epoch: 965 [800/869 (91%)]\tLoss: 97410632.000000\n",
      "\n",
      "Test set: Avg. loss: 7478188.1151, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 966 [0/869 (0%)]\tLoss: 81220432.000000\n",
      "Train Epoch: 966 [160/869 (18%)]\tLoss: 144299712.000000\n",
      "Train Epoch: 966 [320/869 (36%)]\tLoss: 131503200.000000\n",
      "Train Epoch: 966 [480/869 (55%)]\tLoss: 51554060.000000\n",
      "Train Epoch: 966 [640/869 (73%)]\tLoss: 186617232.000000\n",
      "Train Epoch: 966 [800/869 (91%)]\tLoss: 60808192.000000\n",
      "\n",
      "Test set: Avg. loss: 7412488.0128, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 967 [0/869 (0%)]\tLoss: 104667024.000000\n",
      "Train Epoch: 967 [160/869 (18%)]\tLoss: 85903088.000000\n",
      "Train Epoch: 967 [320/869 (36%)]\tLoss: 84613336.000000\n",
      "Train Epoch: 967 [480/869 (55%)]\tLoss: 139725760.000000\n",
      "Train Epoch: 967 [640/869 (73%)]\tLoss: 103031504.000000\n",
      "Train Epoch: 967 [800/869 (91%)]\tLoss: 162601360.000000\n",
      "\n",
      "Test set: Avg. loss: 7492478.3454, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 968 [0/869 (0%)]\tLoss: 171595904.000000\n",
      "Train Epoch: 968 [160/869 (18%)]\tLoss: 123025624.000000\n",
      "Train Epoch: 968 [320/869 (36%)]\tLoss: 106934688.000000\n",
      "Train Epoch: 968 [480/869 (55%)]\tLoss: 112766720.000000\n",
      "Train Epoch: 968 [640/869 (73%)]\tLoss: 106833056.000000\n",
      "Train Epoch: 968 [800/869 (91%)]\tLoss: 231264368.000000\n",
      "\n",
      "Test set: Avg. loss: 7326226.9254, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 969 [0/869 (0%)]\tLoss: 187843648.000000\n",
      "Train Epoch: 969 [160/869 (18%)]\tLoss: 154056080.000000\n",
      "Train Epoch: 969 [320/869 (36%)]\tLoss: 245005584.000000\n",
      "Train Epoch: 969 [480/869 (55%)]\tLoss: 48061596.000000\n",
      "Train Epoch: 969 [640/869 (73%)]\tLoss: 82554864.000000\n",
      "Train Epoch: 969 [800/869 (91%)]\tLoss: 88600352.000000\n",
      "\n",
      "Test set: Avg. loss: 7737945.0576, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 970 [0/869 (0%)]\tLoss: 94903944.000000\n",
      "Train Epoch: 970 [160/869 (18%)]\tLoss: 149566432.000000\n",
      "Train Epoch: 970 [320/869 (36%)]\tLoss: 178581648.000000\n",
      "Train Epoch: 970 [480/869 (55%)]\tLoss: 151675232.000000\n",
      "Train Epoch: 970 [640/869 (73%)]\tLoss: 31720820.000000\n",
      "Train Epoch: 970 [800/869 (91%)]\tLoss: 148848864.000000\n",
      "\n",
      "Test set: Avg. loss: 7487304.2303, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 971 [0/869 (0%)]\tLoss: 49973744.000000\n",
      "Train Epoch: 971 [160/869 (18%)]\tLoss: 140973328.000000\n",
      "Train Epoch: 971 [320/869 (36%)]\tLoss: 199128096.000000\n",
      "Train Epoch: 971 [480/869 (55%)]\tLoss: 85413768.000000\n",
      "Train Epoch: 971 [640/869 (73%)]\tLoss: 199054560.000000\n",
      "Train Epoch: 971 [800/869 (91%)]\tLoss: 39151372.000000\n",
      "\n",
      "Test set: Avg. loss: 7998543.1386, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 972 [0/869 (0%)]\tLoss: 217664080.000000\n",
      "Train Epoch: 972 [160/869 (18%)]\tLoss: 67245536.000000\n",
      "Train Epoch: 972 [320/869 (36%)]\tLoss: 185241888.000000\n",
      "Train Epoch: 972 [480/869 (55%)]\tLoss: 92222472.000000\n",
      "Train Epoch: 972 [640/869 (73%)]\tLoss: 109301208.000000\n",
      "Train Epoch: 972 [800/869 (91%)]\tLoss: 83902160.000000\n",
      "\n",
      "Test set: Avg. loss: 7327988.0000, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 973 [0/869 (0%)]\tLoss: 108574496.000000\n",
      "Train Epoch: 973 [160/869 (18%)]\tLoss: 101735296.000000\n",
      "Train Epoch: 973 [320/869 (36%)]\tLoss: 63135104.000000\n",
      "Train Epoch: 973 [480/869 (55%)]\tLoss: 81843536.000000\n",
      "Train Epoch: 973 [640/869 (73%)]\tLoss: 174714144.000000\n",
      "Train Epoch: 973 [800/869 (91%)]\tLoss: 45454064.000000\n",
      "\n",
      "Test set: Avg. loss: 8246095.6759, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 974 [0/869 (0%)]\tLoss: 173937792.000000\n",
      "Train Epoch: 974 [160/869 (18%)]\tLoss: 84237896.000000\n",
      "Train Epoch: 974 [320/869 (36%)]\tLoss: 171782080.000000\n",
      "Train Epoch: 974 [480/869 (55%)]\tLoss: 101722000.000000\n",
      "Train Epoch: 974 [640/869 (73%)]\tLoss: 137585280.000000\n",
      "Train Epoch: 974 [800/869 (91%)]\tLoss: 352319648.000000\n",
      "\n",
      "Test set: Avg. loss: 7525800.4861, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 975 [0/869 (0%)]\tLoss: 132099640.000000\n",
      "Train Epoch: 975 [160/869 (18%)]\tLoss: 186107344.000000\n",
      "Train Epoch: 975 [320/869 (36%)]\tLoss: 172418656.000000\n",
      "Train Epoch: 975 [480/869 (55%)]\tLoss: 93246592.000000\n",
      "Train Epoch: 975 [640/869 (73%)]\tLoss: 83184832.000000\n",
      "Train Epoch: 975 [800/869 (91%)]\tLoss: 183852304.000000\n",
      "\n",
      "Test set: Avg. loss: 7473345.6631, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 976 [0/869 (0%)]\tLoss: 34011524.000000\n",
      "Train Epoch: 976 [160/869 (18%)]\tLoss: 33604452.000000\n",
      "Train Epoch: 976 [320/869 (36%)]\tLoss: 190463248.000000\n",
      "Train Epoch: 976 [480/869 (55%)]\tLoss: 254531808.000000\n",
      "Train Epoch: 976 [640/869 (73%)]\tLoss: 87149424.000000\n",
      "Train Epoch: 976 [800/869 (91%)]\tLoss: 170363968.000000\n",
      "\n",
      "Test set: Avg. loss: 7634765.6034, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 977 [0/869 (0%)]\tLoss: 95176080.000000\n",
      "Train Epoch: 977 [160/869 (18%)]\tLoss: 169185392.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 977 [320/869 (36%)]\tLoss: 205798320.000000\n",
      "Train Epoch: 977 [480/869 (55%)]\tLoss: 117873552.000000\n",
      "Train Epoch: 977 [640/869 (73%)]\tLoss: 248583680.000000\n",
      "Train Epoch: 977 [800/869 (91%)]\tLoss: 88367624.000000\n",
      "\n",
      "Test set: Avg. loss: 7331904.5032, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 978 [0/869 (0%)]\tLoss: 44292472.000000\n",
      "Train Epoch: 978 [160/869 (18%)]\tLoss: 120287600.000000\n",
      "Train Epoch: 978 [320/869 (36%)]\tLoss: 39145536.000000\n",
      "Train Epoch: 978 [480/869 (55%)]\tLoss: 151588128.000000\n",
      "Train Epoch: 978 [640/869 (73%)]\tLoss: 100873888.000000\n",
      "Train Epoch: 978 [800/869 (91%)]\tLoss: 165704304.000000\n",
      "\n",
      "Test set: Avg. loss: 7539204.3582, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 979 [0/869 (0%)]\tLoss: 143381008.000000\n",
      "Train Epoch: 979 [160/869 (18%)]\tLoss: 160783808.000000\n",
      "Train Epoch: 979 [320/869 (36%)]\tLoss: 174055904.000000\n",
      "Train Epoch: 979 [480/869 (55%)]\tLoss: 181722912.000000\n",
      "Train Epoch: 979 [640/869 (73%)]\tLoss: 102441368.000000\n",
      "Train Epoch: 979 [800/869 (91%)]\tLoss: 137632640.000000\n",
      "\n",
      "Test set: Avg. loss: 7300373.9104, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 980 [0/869 (0%)]\tLoss: 152623488.000000\n",
      "Train Epoch: 980 [160/869 (18%)]\tLoss: 139909856.000000\n",
      "Train Epoch: 980 [320/869 (36%)]\tLoss: 241300512.000000\n",
      "Train Epoch: 980 [480/869 (55%)]\tLoss: 88549560.000000\n",
      "Train Epoch: 980 [640/869 (73%)]\tLoss: 223732768.000000\n",
      "Train Epoch: 980 [800/869 (91%)]\tLoss: 57372892.000000\n",
      "\n",
      "Test set: Avg. loss: 7519035.6418, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 981 [0/869 (0%)]\tLoss: 207835440.000000\n",
      "Train Epoch: 981 [160/869 (18%)]\tLoss: 79842912.000000\n",
      "Train Epoch: 981 [320/869 (36%)]\tLoss: 39311804.000000\n",
      "Train Epoch: 981 [480/869 (55%)]\tLoss: 217425968.000000\n",
      "Train Epoch: 981 [640/869 (73%)]\tLoss: 107466984.000000\n",
      "Train Epoch: 981 [800/869 (91%)]\tLoss: 186736720.000000\n",
      "\n",
      "Test set: Avg. loss: 7608862.1834, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 982 [0/869 (0%)]\tLoss: 170318848.000000\n",
      "Train Epoch: 982 [160/869 (18%)]\tLoss: 139623696.000000\n",
      "Train Epoch: 982 [320/869 (36%)]\tLoss: 147540592.000000\n",
      "Train Epoch: 982 [480/869 (55%)]\tLoss: 184110992.000000\n",
      "Train Epoch: 982 [640/869 (73%)]\tLoss: 133564648.000000\n",
      "Train Epoch: 982 [800/869 (91%)]\tLoss: 99521560.000000\n",
      "\n",
      "Test set: Avg. loss: 7579416.3241, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 983 [0/869 (0%)]\tLoss: 87226320.000000\n",
      "Train Epoch: 983 [160/869 (18%)]\tLoss: 173489536.000000\n",
      "Train Epoch: 983 [320/869 (36%)]\tLoss: 71467768.000000\n",
      "Train Epoch: 983 [480/869 (55%)]\tLoss: 198473504.000000\n",
      "Train Epoch: 983 [640/869 (73%)]\tLoss: 178448528.000000\n",
      "Train Epoch: 983 [800/869 (91%)]\tLoss: 90258912.000000\n",
      "\n",
      "Test set: Avg. loss: 7406682.9851, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 984 [0/869 (0%)]\tLoss: 123035040.000000\n",
      "Train Epoch: 984 [160/869 (18%)]\tLoss: 8636441.000000\n",
      "Train Epoch: 984 [320/869 (36%)]\tLoss: 191007776.000000\n",
      "Train Epoch: 984 [480/869 (55%)]\tLoss: 204287744.000000\n",
      "Train Epoch: 984 [640/869 (73%)]\tLoss: 90083040.000000\n",
      "Train Epoch: 984 [800/869 (91%)]\tLoss: 89984776.000000\n",
      "\n",
      "Test set: Avg. loss: 7323735.7697, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 985 [0/869 (0%)]\tLoss: 127437928.000000\n",
      "Train Epoch: 985 [160/869 (18%)]\tLoss: 262338912.000000\n",
      "Train Epoch: 985 [320/869 (36%)]\tLoss: 81035176.000000\n",
      "Train Epoch: 985 [480/869 (55%)]\tLoss: 38281544.000000\n",
      "Train Epoch: 985 [640/869 (73%)]\tLoss: 82389328.000000\n",
      "Train Epoch: 985 [800/869 (91%)]\tLoss: 49392548.000000\n",
      "\n",
      "Test set: Avg. loss: 7615819.2580, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 986 [0/869 (0%)]\tLoss: 136612640.000000\n",
      "Train Epoch: 986 [160/869 (18%)]\tLoss: 192884624.000000\n",
      "Train Epoch: 986 [320/869 (36%)]\tLoss: 84072416.000000\n",
      "Train Epoch: 986 [480/869 (55%)]\tLoss: 167073888.000000\n",
      "Train Epoch: 986 [640/869 (73%)]\tLoss: 200205264.000000\n",
      "Train Epoch: 986 [800/869 (91%)]\tLoss: 89319520.000000\n",
      "\n",
      "Test set: Avg. loss: 7836353.2708, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 987 [0/869 (0%)]\tLoss: 116137336.000000\n",
      "Train Epoch: 987 [160/869 (18%)]\tLoss: 82420840.000000\n",
      "Train Epoch: 987 [320/869 (36%)]\tLoss: 96424056.000000\n",
      "Train Epoch: 987 [480/869 (55%)]\tLoss: 94013744.000000\n",
      "Train Epoch: 987 [640/869 (73%)]\tLoss: 159177792.000000\n",
      "Train Epoch: 987 [800/869 (91%)]\tLoss: 264143024.000000\n",
      "\n",
      "Test set: Avg. loss: 7394103.3859, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 988 [0/869 (0%)]\tLoss: 95337232.000000\n",
      "Train Epoch: 988 [160/869 (18%)]\tLoss: 35272868.000000\n",
      "Train Epoch: 988 [320/869 (36%)]\tLoss: 104775920.000000\n",
      "Train Epoch: 988 [480/869 (55%)]\tLoss: 150272320.000000\n",
      "Train Epoch: 988 [640/869 (73%)]\tLoss: 86786480.000000\n",
      "Train Epoch: 988 [800/869 (91%)]\tLoss: 140280800.000000\n",
      "\n",
      "Test set: Avg. loss: 7741845.7484, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 989 [0/869 (0%)]\tLoss: 90410144.000000\n",
      "Train Epoch: 989 [160/869 (18%)]\tLoss: 103693696.000000\n",
      "Train Epoch: 989 [320/869 (36%)]\tLoss: 156246144.000000\n",
      "Train Epoch: 989 [480/869 (55%)]\tLoss: 109082672.000000\n",
      "Train Epoch: 989 [640/869 (73%)]\tLoss: 148925392.000000\n",
      "Train Epoch: 989 [800/869 (91%)]\tLoss: 202034224.000000\n",
      "\n",
      "Test set: Avg. loss: 7831561.9957, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 990 [0/869 (0%)]\tLoss: 138998560.000000\n",
      "Train Epoch: 990 [160/869 (18%)]\tLoss: 91257360.000000\n",
      "Train Epoch: 990 [320/869 (36%)]\tLoss: 101898272.000000\n",
      "Train Epoch: 990 [480/869 (55%)]\tLoss: 173749232.000000\n",
      "Train Epoch: 990 [640/869 (73%)]\tLoss: 170262816.000000\n",
      "Train Epoch: 990 [800/869 (91%)]\tLoss: 35227688.000000\n",
      "\n",
      "Test set: Avg. loss: 7309170.8401, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 991 [0/869 (0%)]\tLoss: 322846496.000000\n",
      "Train Epoch: 991 [160/869 (18%)]\tLoss: 150365824.000000\n",
      "Train Epoch: 991 [320/869 (36%)]\tLoss: 100271992.000000\n",
      "Train Epoch: 991 [480/869 (55%)]\tLoss: 186055904.000000\n",
      "Train Epoch: 991 [640/869 (73%)]\tLoss: 214907552.000000\n",
      "Train Epoch: 991 [800/869 (91%)]\tLoss: 139729104.000000\n",
      "\n",
      "Test set: Avg. loss: 7304007.5480, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 992 [0/869 (0%)]\tLoss: 228548368.000000\n",
      "Train Epoch: 992 [160/869 (18%)]\tLoss: 126659448.000000\n",
      "Train Epoch: 992 [320/869 (36%)]\tLoss: 42578444.000000\n",
      "Train Epoch: 992 [480/869 (55%)]\tLoss: 104621400.000000\n",
      "Train Epoch: 992 [640/869 (73%)]\tLoss: 121310112.000000\n",
      "Train Epoch: 992 [800/869 (91%)]\tLoss: 109961744.000000\n",
      "\n",
      "Test set: Avg. loss: 7519963.4712, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 993 [0/869 (0%)]\tLoss: 59953888.000000\n",
      "Train Epoch: 993 [160/869 (18%)]\tLoss: 187631104.000000\n",
      "Train Epoch: 993 [320/869 (36%)]\tLoss: 58811892.000000\n",
      "Train Epoch: 993 [480/869 (55%)]\tLoss: 225864576.000000\n",
      "Train Epoch: 993 [640/869 (73%)]\tLoss: 122184464.000000\n",
      "Train Epoch: 993 [800/869 (91%)]\tLoss: 142511568.000000\n",
      "\n",
      "Test set: Avg. loss: 8078557.2111, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 994 [0/869 (0%)]\tLoss: 113087792.000000\n",
      "Train Epoch: 994 [160/869 (18%)]\tLoss: 36847620.000000\n",
      "Train Epoch: 994 [320/869 (36%)]\tLoss: 104724280.000000\n",
      "Train Epoch: 994 [480/869 (55%)]\tLoss: 136352320.000000\n",
      "Train Epoch: 994 [640/869 (73%)]\tLoss: 142360032.000000\n",
      "Train Epoch: 994 [800/869 (91%)]\tLoss: 223163120.000000\n",
      "\n",
      "Test set: Avg. loss: 7465902.6780, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 995 [0/869 (0%)]\tLoss: 182011312.000000\n",
      "Train Epoch: 995 [160/869 (18%)]\tLoss: 84681696.000000\n",
      "Train Epoch: 995 [320/869 (36%)]\tLoss: 134614496.000000\n",
      "Train Epoch: 995 [480/869 (55%)]\tLoss: 223893552.000000\n",
      "Train Epoch: 995 [640/869 (73%)]\tLoss: 153747760.000000\n",
      "Train Epoch: 995 [800/869 (91%)]\tLoss: 45784576.000000\n",
      "\n",
      "Test set: Avg. loss: 7507676.1279, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 996 [0/869 (0%)]\tLoss: 103197136.000000\n",
      "Train Epoch: 996 [160/869 (18%)]\tLoss: 106786232.000000\n",
      "Train Epoch: 996 [320/869 (36%)]\tLoss: 188649664.000000\n",
      "Train Epoch: 996 [480/869 (55%)]\tLoss: 283309312.000000\n",
      "Train Epoch: 996 [640/869 (73%)]\tLoss: 91424560.000000\n",
      "Train Epoch: 996 [800/869 (91%)]\tLoss: 25152592.000000\n",
      "\n",
      "Test set: Avg. loss: 7312753.3475, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 997 [0/869 (0%)]\tLoss: 92754656.000000\n",
      "Train Epoch: 997 [160/869 (18%)]\tLoss: 107979584.000000\n",
      "Train Epoch: 997 [320/869 (36%)]\tLoss: 173842960.000000\n",
      "Train Epoch: 997 [480/869 (55%)]\tLoss: 89820160.000000\n",
      "Train Epoch: 997 [640/869 (73%)]\tLoss: 175558592.000000\n",
      "Train Epoch: 997 [800/869 (91%)]\tLoss: 142119168.000000\n",
      "\n",
      "Test set: Avg. loss: 7585087.2495, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 998 [0/869 (0%)]\tLoss: 170548528.000000\n",
      "Train Epoch: 998 [160/869 (18%)]\tLoss: 63576992.000000\n",
      "Train Epoch: 998 [320/869 (36%)]\tLoss: 194573712.000000\n",
      "Train Epoch: 998 [480/869 (55%)]\tLoss: 193999888.000000\n",
      "Train Epoch: 998 [640/869 (73%)]\tLoss: 27708840.000000\n",
      "Train Epoch: 998 [800/869 (91%)]\tLoss: 210785568.000000\n",
      "\n",
      "Test set: Avg. loss: 7460455.3262, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 999 [0/869 (0%)]\tLoss: 209224560.000000\n",
      "Train Epoch: 999 [160/869 (18%)]\tLoss: 54289232.000000\n",
      "Train Epoch: 999 [320/869 (36%)]\tLoss: 159473328.000000\n",
      "Train Epoch: 999 [480/869 (55%)]\tLoss: 24849868.000000\n",
      "Train Epoch: 999 [640/869 (73%)]\tLoss: 147496704.000000\n",
      "Train Epoch: 999 [800/869 (91%)]\tLoss: 130680440.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 7596746.6610, Accuracy: 0/469 (0%)\n",
      "\n",
      "Train Epoch: 1000 [0/869 (0%)]\tLoss: 117934624.000000\n",
      "Train Epoch: 1000 [160/869 (18%)]\tLoss: 246893280.000000\n",
      "Train Epoch: 1000 [320/869 (36%)]\tLoss: 113297072.000000\n",
      "Train Epoch: 1000 [480/869 (55%)]\tLoss: 135398048.000000\n",
      "Train Epoch: 1000 [640/869 (73%)]\tLoss: 88361696.000000\n",
      "Train Epoch: 1000 [800/869 (91%)]\tLoss: 69779504.000000\n",
      "\n",
      "Test set: Avg. loss: 7758906.1151, Accuracy: 0/469 (0%)\n",
      "\n",
      "Time elapsed: 52.5741 seconds\n"
     ]
    }
   ],
   "source": [
    "#Check accuaracy before training\n",
    "test()\n",
    "tic = time.perf_counter()\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test()\n",
    "    \n",
    "toc = time.perf_counter()\n",
    "\n",
    "print(f\"Time elapsed: {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "romance-summit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1650.5818], dtype=float32), array([[387251.56]], dtype=float32))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta1, beta0 = network.parameters()\n",
    "\n",
    "beta0 = beta0.data.numpy()\n",
    "beta1 = beta1.data.numpy()\n",
    "\n",
    "beta0, beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-cover",
   "metadata": {},
   "source": [
    "### Plot training result with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "supported-shark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B0: [1650.5818], B1: [[387251.56]]\n"
     ]
    }
   ],
   "source": [
    "beta1, beta0 = network.parameters()\n",
    "\n",
    "beta0 = beta0.data.numpy()\n",
    "beta1 = beta1.data.numpy()\n",
    "\n",
    "yplot = X_train * beta1 + beta0\n",
    "plt.plot(X_train, y_train, 'x');\n",
    "plt.plot(X_train, yplot.reshape(-1, 1), 'g')\n",
    "plt.show()\n",
    "\n",
    "print(f'B0: {beta0}, B1: {beta1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-vocabulary",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-theta",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/44429199/how-to-load-a-list-of-numpy-arrays-to-pytorch-dataset-loader\n",
    "https://towardsdatascience.com/linear-regression-with-pytorch-eb6dedead817"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-arbor",
   "metadata": {},
   "source": [
    "# ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-butler",
   "metadata": {},
   "source": [
    "- Loss is high, check if normalizing output helps.\n",
    "- Model is converging now, but is still little bit different from sklearn\n",
    "Options to do:\n",
    "- Normalize outputs and train model again. Might no needed.\n",
    "- What is the difference between a model and nn.Linear?\n",
    "- Repair log output to display meaningful data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-showcase",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
